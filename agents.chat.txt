This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-12T16:45:20.312Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
chat/
  history.py
clients/
  manager.py
services/
  chat_service.py
websockets/
  handlers/
    agent_handler.py
    callback_handler.py
    message_handler.py
  services/
    chat_service.py
  __init__.py
  base.py
  chat_consumer.py
  consumers.py
  crew_consumer.py
consumers.py
routing.py

================================================================
Repository Files
================================================================

================
File: chat/history.py
================
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import BaseMessage, messages_from_dict, messages_to_dict
from django.core.cache import cache
from typing import List
import hashlib

class DjangoCacheMessageHistory(BaseChatMessageHistory):
    """Message history that uses Django's cache backend"""
    
    def __init__(self, session_id: str, ttl: int = 3600):
        self.session_id = session_id
        self.ttl = ttl
        self.key = f"chat_history_{session_id}"
        self._message_hashes = set()

    def _get_message_hash(self, message: BaseMessage) -> str:
        """Generate a hash for a message to detect duplicates"""
        content = str(message.content)
        return hashlib.md5(content.encode()).hexdigest()

    def add_message(self, message: BaseMessage) -> None:
        """Append the message to the history in cache if not duplicate"""
        message_hash = self._get_message_hash(message)
        
        if message_hash not in self._message_hashes:
            self._message_hashes.add(message_hash)
            messages = self.messages
            messages.append(message)
            cache.set(
                self.key,
                messages_to_dict(messages),
                timeout=self.ttl
            )

    @property
    def messages(self) -> List[BaseMessage]:
        """Retrieve the messages from cache"""
        messages_dict = cache.get(self.key, [])
        messages = messages_from_dict(messages_dict) if messages_dict else []
        
        # Rebuild message hashes set
        self._message_hashes = {self._get_message_hash(msg) for msg in messages}
        return messages

    def clear(self) -> None:
        """Clear message history from cache"""
        cache.delete(self.key)

================
File: clients/manager.py
================
import logging
from django.utils import timezone
from apps.seo_manager.models import Client
from channels.db import database_sync_to_async

logger = logging.getLogger(__name__)

class ClientDataManager:
    def __init__(self):
        pass

    @database_sync_to_async
    def get_client_data(self, client_id):
        """Get and format client data"""
        if not client_id:
            return {
                'client_id': None,
                'current_date': timezone.now().date().isoformat(),
            }
            
        try:
            client = Client.objects.get(id=client_id)
            current_date = timezone.now().date()
            
            return {
                'client_id': client.id,
                'current_date': current_date.isoformat(),
            }
        except Client.DoesNotExist:
            logger.info(f"No client found with ID {client_id}, returning default data")
            return {
                'client_id': None,
                'current_date': timezone.now().date().isoformat(),
            }
        except Exception as e:
            logger.error(f"Error getting client data: {str(e)}", exc_info=True)
            return {
                'client_id': None,
                'current_date': timezone.now().date().isoformat(),
            }

================
File: services/chat_service.py
================
from apps.common.utils import get_llm, format_message
from apps.agents.utils import get_tool_classes
from langchain.agents import AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import Tool, StructuredTool
from apps.agents.chat.history import DjangoCacheMessageHistory
from channels.db import database_sync_to_async
from langchain_core.messages import SystemMessage, HumanMessage
import json
import logging

logger = logging.getLogger(__name__)

class ChatService:
    def __init__(self, agent, model_name, client_data, callback_handler, session_id=None):
        self.agent = agent
        self.model_name = model_name
        self.client_data = client_data
        self.callback_handler = callback_handler
        self.llm = None
        self.token_counter = None
        self.agent_executor = None
        self.processing = False
        self.tool_cache = {}  # Cache for tool results
        self.session_id = session_id or f"{agent.id}_{client_data['client_id'] if client_data else 'no_client'}"

    async def initialize(self):
        """Initialize the chat service with LLM and agent"""
        try:
            # Get LLM and token counter from utils
            self.llm, self.token_counter = get_llm(
                model_name=self.model_name,
                temperature=0.7,
                streaming=True
            )

            # Load tools
            tools = await self._load_tools()
            
            # Initialize memory
            message_history = DjangoCacheMessageHistory(
                session_id=self.session_id,
                ttl=3600
            )

            memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                chat_memory=message_history,
                output_key="output",
                input_key="input"
            )

            # Get tool names and descriptions
            tool_names = [tool.name for tool in tools]
            tool_descriptions = [f"{tool.name}: {tool.description}" for tool in tools]

            # Create prompt with required variables
            prompt = ChatPromptTemplate.from_messages([
                ("system", """
{system_prompt}

You have access to the following tools:
{tools}

Tool Names: {tool_names}

IMPORTANT INSTRUCTIONS:
1. If a tool call fails, examine the error message and try to fix the parameters
2. If multiple tool calls fail, return a helpful message explaining the limitation
3. Always provide a clear response even if data is limited
4. Never give up without providing some useful information
5. Keep responses focused and concise

To use a tool, respond with:
{{"action": "tool_name", "action_input": {{"param1": "value1", "param2": "value2"}}}}

For final responses, use:
{{"action": "Final Answer", "action_input": "your response here"}}
"""),
                ("human", "{input}"),
                ("ai", "{agent_scratchpad}"),
                ("system", "Previous conversation:\n{chat_history}")
            ])

            # Create the agent
            from langchain.agents import create_structured_chat_agent
            agent = create_structured_chat_agent(
                llm=self.llm,
                tools=tools,
                prompt=prompt.partial(
                    system_prompt=await self._create_agent_prompt(),
                    tools="\n".join(tool_descriptions),
                    tool_names=", ".join(tool_names)
                )
            )
            
            # Create agent executor
            self.agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                memory=memory,
                verbose=True,
                max_iterations=5,
                max_execution_time=60,
                early_stopping_method="force",
                handle_parsing_errors=True,
                return_intermediate_steps=False,
                output_key="output",
                input_key="input"
            )
            
            return self.agent_executor

        except Exception as e:
            logger.error(f"Error initializing chat service: {str(e)}", exc_info=True)
            raise

    async def process_message(self, message: str) -> str:
        """Process a message using the agent"""
        if not self.agent_executor:
            raise ValueError("Agent executor not initialized")

        if self.processing:
            return None

        try:
            self.processing = True
            
            # Format input as expected by the agent
            input_data = {
                "input": message,
                "chat_history": self.agent_executor.memory.chat_memory.messages if self.agent_executor.memory else []
            }
            
            logger.debug(f"Processing input: {input_data}")
            
            error_count = 0
            last_error = None
            
            # Process message with streaming
            async for chunk in self.agent_executor.astream(
                input_data
            ):
                logger.debug(f"Received chunk: {chunk}")
                try:
                    # Handle different types of chunks
                    if isinstance(chunk, dict):
                        if 'steps' in chunk:
                            for step in chunk['steps']:
                                if step.action.tool == '_Exception':
                                    error_count += 1
                                    last_error = step.log
                                    if error_count >= 3:
                                        # After 3 errors, try to salvage what we can
                                        error_summary = "I encountered some issues processing the data, but here's what I found:\n\n"
                                        if 'observation' in step:
                                            error_summary += f"- {step.observation}\n"
                                        if last_error:
                                            error_summary += f"\nDetails: {last_error}"
                                        await self.callback_handler.on_llm_new_token(error_summary)
                                        return None
                                    continue
                                
                        if "output" in chunk:
                            # Send the actual output
                            await self.callback_handler.on_llm_new_token(chunk["output"])
                        elif "intermediate_steps" in chunk:
                            # Log tool usage
                            for step in chunk["intermediate_steps"]:
                                if len(step) >= 2:
                                    action, output = step
                                    if not isinstance(output, str) and hasattr(output, '__str__'):
                                        output = str(output)
                                    tool_msg = f"Using tool: {action.tool}\nInput: {action.tool_input}\nOutput: {output}"
                                    await self.callback_handler.on_llm_new_token(tool_msg)
                        else:
                            # Send any other content
                            content = str(chunk)
                            if content.strip():  # Only send non-empty content
                                await self.callback_handler.on_llm_new_token(content)
                    else:
                        # Handle direct string output
                        content = str(chunk)
                        if content.strip():  # Only send non-empty content
                            await self.callback_handler.on_llm_new_token(content)
                            
                except Exception as chunk_error:
                    logger.error(f"Error processing chunk: {str(chunk_error)}")
                    error_count += 1
                    last_error = str(chunk_error)
                    if error_count >= 3:
                        await self.callback_handler.on_llm_new_token(
                            f"Multiple errors occurred while processing the response. Last error: {last_error}"
                        )
                        return None
                    continue
                    
            return None

        except Exception as e:
            error_msg = f"Error processing message: {str(e)}"
            logger.error(error_msg)
            await self.callback_handler.on_llm_error(error_msg)
            raise
        finally:
            self.processing = False

    @database_sync_to_async
    def _load_tools(self):
        """Load and initialize agent tools asynchronously"""
        try:
            tools = []
            seen_tools = set()
            
            for tool_model in self.agent.tools.all():
                try:
                    tool_key = f"{tool_model.tool_class}_{tool_model.tool_subclass}"
                    if tool_key in seen_tools:
                        continue
                    seen_tools.add(tool_key)

                    tool_classes = get_tool_classes(tool_model.tool_class)
                    tool_class = next((cls for cls in tool_classes 
                                   if cls.__name__ == tool_model.tool_subclass), None)
                    
                    if tool_class:
                        logger.info(f"Initializing tool: {tool_class.__name__}")
                        tool_instance = tool_class()
                        
                        # Wrap tool with caching
                        original_run = tool_instance._run
                        async def cached_run(*args, **kwargs):
                            cache_key = f"{tool_instance.__class__.__name__}:{json.dumps(kwargs, sort_keys=True)}"
                            if cache_key in self.tool_cache:
                                logger.info(f"Cache hit for tool: {cache_key}")
                                return self.tool_cache[cache_key]
                            
                            result = await original_run(*args, **kwargs)
                            self.tool_cache[cache_key] = result
                            return result
                            
                        tool_instance._run = cached_run
                        
                        # Create structured or basic tool
                        if hasattr(tool_instance, 'args_schema'):
                            tool = StructuredTool.from_function(
                                func=tool_instance._run,
                                name=tool_model.name.lower().replace(" ", "_"),
                                description=self._create_tool_description(tool_instance, tool_model),
                                args_schema=tool_instance.args_schema,
                                coroutine=tool_instance.arun if hasattr(tool_instance, 'arun') else None,
                                return_direct=False
                            )
                        else:
                            tool = Tool(
                                name=tool_model.name.lower().replace(" ", "_"),
                                description=self._create_tool_description(tool_instance, tool_model),
                                func=tool_instance._run,
                                coroutine=tool_instance.arun if hasattr(tool_instance, 'arun') else None
                            )
                        
                        tools.append(tool)
                        logger.info(f"Successfully loaded tool: {tool_model.name}")
                        
                except Exception as e:
                    logger.error(f"Error loading tool {tool_model.name}: {str(e)}")
                    
            return tools
            
        except Exception as e:
            logger.error(f"Error loading tools: {str(e)}")
            return []

    def _create_tool_description(self, tool_instance, tool_model):
        """Create a detailed description for the tool"""
        try:
            base_description = tool_instance.description or tool_model.description
            schema = tool_instance.args_schema

            if schema:
                field_descriptions = []
                for field_name, field in schema.model_fields.items():
                    field_type = str(field.annotation).replace('typing.', '')
                    if hasattr(field.annotation, '__name__'):
                        field_type = field.annotation.__name__
                    
                    field_desc = field.description or ''
                    default = field.default
                    if default is Ellipsis:
                        default = "Required"
                    elif default is None:
                        default = "Optional"
                    
                    field_descriptions.append(
                        f"- {field_name} ({field_type}): {field_desc} Default: {default}"
                    )

                tool_description = f"""{base_description}

Parameters:
{chr(10).join(field_descriptions)}

Example:
{{"action": "{tool_model.name.lower().replace(' ', '_')}", 
  "action_input": {{
    "client_id": 123,
    "start_date": "2024-01-01",
    "end_date": "2024-01-31",
    "metrics": "newUsers",
    "dimensions": "date"
  }}
}}"""
                
                return tool_description
            
            return base_description

        except Exception as e:
            logger.error(f"Error creating tool description: {str(e)}")
            return base_description or "Tool description unavailable"

    @database_sync_to_async
    def _create_agent_prompt(self):
        """Create the system prompt for the agent"""
        client_context = ""
        if self.client_data:
            client_context = f"""Current Context:
- Client ID: {self.client_data.get('client_id', 'N/A')}
- Current Date: {self.client_data.get('current_date', 'N/A')}"""

        return f"""You are {self.agent.name}, an AI assistant.
Role: {self.agent.role}

{client_context}

{self.agent.use_system_prompt if self.agent.use_system_prompt else ''}

CRITICAL INSTRUCTIONS:
1. You MUST respond with ONLY a single JSON object in one of these two formats:

For using a tool:
{{"action": "tool_name", "action_input": {{"param1": "value1", "param2": "value2"}}}}

For final answers:
{{"action": "Final Answer", "action_input": "your response here"}}

2. When handling tool outputs:
   - If you receive a large amount of data, focus on relevant parts
   - Break down analysis into smaller steps if needed
   - If a tool returns HTML or complex data, extract key information first
   - Always provide a clear, structured response even with partial data

3. Error Handling:
   - If a tool call fails, try a different approach
   - If you can't parse all the data, focus on what you can understand
   - Never give up without providing some useful insights
   - If you hit multiple errors, summarize what you learned and suggest next steps

4. Response Format:
   - Keep responses focused and concise
   - Format data in tables when presenting multiple metrics
   - Use bullet points for lists
   - Include specific numbers and metrics when available

5. NEVER:
   - Return raw HTML or script tags in your response
   - Include multiple JSON objects in one response
   - Leave analysis incomplete without explanation
   - Exceed response length limits

Example Tool Usage:
{{"action": "scrape_website_content", "action_input": {{"website": "https://example.com", "output_type": "text"}}}}

Example Final Answer:
{{"action": "Final Answer", "action_input": "Based on the analysis:\\n- Website has proper meta tags\\n- Mobile responsiveness score: 95/100\\n- Found 3 broken links\\n\\nRecommendations:\\n1. Fix broken links\\n2. Add alt text to images\\n3. Improve page load speed"}}"""

================
File: websockets/handlers/agent_handler.py
================
import logging
from apps.common.utils import format_message
from apps.agents.models import Agent

logger = logging.getLogger(__name__)

class AgentHandler:
    def __init__(self, consumer):
        self.consumer = consumer
        self.chat_service = None

    async def process_response(self, message, agent_id, model_name, client_id):
        """Process and send agent response"""
        try:
            # Get agent data
            agent = await self.get_agent(agent_id)
            if not agent:
                raise ValueError("Agent not found")

            # Get client data
            client_data = await self.consumer.client_manager.get_client_data(client_id)

            # Initialize chat service if needed
            if not self.chat_service:
                from ..services.chat_service import ChatService
                from ..handlers.callback_handler import WebSocketCallbackHandler
                
                callback_handler = WebSocketCallbackHandler(self.consumer)
                self.chat_service = ChatService(
                    agent=agent,
                    model_name=model_name,
                    client_data=client_data,
                    callback_handler=callback_handler,
                    session_id=self.consumer.session_id
                )
                await self.chat_service.initialize()

            # Process message
            response = await self.chat_service.process_message(message)
            
            # Generic error handling for any tool response
            if isinstance(response, dict) and not response.get('success', True):
                error_msg = response.get('error', 'Unknown error occurred')
                logger.error(f"Tool Error: {error_msg}")
                return f"Error: {error_msg}. Please check your input and try again."

            return response

        except Exception as e:
            logger.error(f"Error in agent handler: {str(e)}")
            raise

    async def get_agent(self, agent_id):
        """Get agent from database"""
        try:
            from django.db import models
            from channels.db import database_sync_to_async

            @database_sync_to_async
            def get_agent_from_db(agent_id):
                return Agent.objects.get(id=agent_id)

            return await get_agent_from_db(agent_id)
        except Exception as e:
            logger.error(f"Error getting agent: {str(e)}")
            raise

================
File: websockets/handlers/callback_handler.py
================
from langchain_core.callbacks import BaseCallbackHandler
import logging
import time
import json
from typing import Any, Dict, List
from datetime import datetime

class WebSocketCallbackHandler(BaseCallbackHandler):
    """Enhanced callback handler with timing and comprehensive event tracking"""
    
    def __init__(self, consumer):
        self.consumer = consumer
        self.logger = logging.getLogger(__name__)
        self._last_time = None
        self._records = []
        self._current_chain_id = None
        self._current_tool_id = None

    def _record_timing(self) -> float:
        """Record time delta between events"""
        time_now = time.time()
        time_delta = time_now - self._last_time if self._last_time is not None else 0
        self._last_time = time_now
        return time_delta

    async def _append_record(self, event_type: str, content: Any, metadata: Dict = None):
        """Record an event with timing and metadata"""
        time_delta = self._record_timing()
        record = {
            "event_type": event_type,
            "content": content,
            "metadata": metadata or {},
            "time_delta": time_delta,
            "timestamp": datetime.now().isoformat(),
            "chain_id": self._current_chain_id,
            "tool_id": self._current_tool_id
        }
        self._records.append(record)
        return record

    async def _send_message(self, content: str, message_type: str = None, is_error: bool = False):
        """Send formatted message through websocket"""
        await self.consumer.message_handler.handle_message(
            content,
            is_agent=True,
            error=is_error,
            message_type=message_type
        )

    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any):
        """Handle LLM start event"""
        self.logger.debug("LLM Start callback triggered")
        record = await self._append_record("llm_start", {
            "serialized": serialized,
            "prompts": prompts,
            **kwargs
        })
        await self._send_message(
            "Processing your request...",
            message_type="llm_start"
        )

    async def on_llm_new_token(self, token: str, **kwargs: Any):
        """Handle streaming tokens"""
        self.logger.debug(f"New token received: {token[:50]}...")
        record = await self._append_record("llm_token", {
            "token": token,
            **kwargs
        })
        if token and token.strip():
            await self._send_message(
                token,
                message_type="llm_token",
                is_error=False
            )

    async def on_llm_end(self, response, **kwargs: Any):
        """Handle LLM completion"""
        self.logger.debug("LLM End callback triggered")
        record = await self._append_record("llm_end", {
            "response": response,
            **kwargs
        })
        try:
            output = response.generations[0][0].text if response.generations else ""
            if output.strip():
                await self._send_message(
                    output,
                    message_type="llm_end"
                )
        except Exception as e:
            self.logger.error(f"Error in on_llm_end: {str(e)}", exc_info=True)

    async def on_llm_error(self, error: str, **kwargs: Any):
        """Handle LLM errors"""
        self.logger.error(f"LLM Error: {error}")
        record = await self._append_record("llm_error", {
            "error": error,
            **kwargs
        })
        await self._send_message(
            f"Error: {error}",
            message_type="llm_error",
            is_error=True
        )

    async def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any):
        """Handle chain start"""
        self._current_chain_id = kwargs.get("run_id", None)
        record = await self._append_record("chain_start", {
            "serialized": serialized,
            "inputs": inputs,
            **kwargs
        })

    async def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any):
        """Handle chain completion"""
        record = await self._append_record("chain_end", {
            "outputs": outputs,
            **kwargs
        })
        self._current_chain_id = None

    async def on_chain_error(self, error: str, **kwargs: Any):
        """Handle chain errors"""
        self.logger.error(f"Chain error: {error}")
        record = await self._append_record("chain_error", {
            "error": error,
            **kwargs
        })
        await self._send_message(
            f"Error: {error}",
            message_type="chain_error",
            is_error=True
        )
        self._current_chain_id = None

    async def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any):
        """Handle tool start"""
        self._current_tool_id = kwargs.get("run_id", None)
        record = await self._append_record("tool_start", {
            "serialized": serialized,
            "input": input_str,
            **kwargs
        })
        await self._send_message(
            f"Using tool: {serialized.get('name', 'unknown')}\nInput: {input_str}",
            message_type="tool_start"
        )

    async def on_tool_end(self, output: str, **kwargs: Any):
        """Handle tool completion"""
        record = await self._append_record("tool_end", {
            "output": output,
            **kwargs
        })
        await self._send_message(
            output,
            message_type="tool_output"
        )
        self._current_tool_id = None

    async def on_tool_error(self, error: str, **kwargs: Any):
        """Handle tool errors"""
        self.logger.error(f"Tool error: {error}")
        record = await self._append_record("tool_error", {
            "error": error,
            **kwargs
        })
        await self._send_message(
            f"Tool error: {error}",
            message_type="tool_error",
            is_error=True
        )
        self._current_tool_id = None

    async def on_text(self, text: str, **kwargs: Any):
        """Handle text events"""
        record = await self._append_record("text", {
            "text": text,
            **kwargs
        })
        await self._send_message(
            text,
            message_type="text"
        )

    async def on_agent_action(self, action, **kwargs: Any):
        """Handle agent actions"""
        record = await self._append_record("agent_action", {
            "action": action,
            **kwargs
        })
        await self._send_message(
            f"Agent action: {action.tool}\nInput: {action.tool_input}",
            message_type="agent_action"
        )

    async def on_agent_finish(self, finish, **kwargs: Any):
        """Handle agent completion"""
        record = await self._append_record("agent_finish", {
            "finish": finish,
            **kwargs
        })
        if hasattr(finish, 'return_values'):
            await self._send_message(
                str(finish.return_values.get('output', '')),
                message_type="agent_finish"
            )

    def get_records(self) -> List[Dict]:
        """Get all recorded events"""
        return self._records

    async def save_records(self, session_id: str):
        """Save records to Django cache"""
        try:
            from django.core.cache import cache
            cache_key = f"callback_records_{session_id}"
            cache.set(cache_key, self._records, timeout=3600)  # 1 hour timeout
        except Exception as e:
            self.logger.error(f"Error saving callback records: {str(e)}", exc_info=True)

================
File: websockets/handlers/message_handler.py
================
import json
import logging
from datetime import datetime
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

class MessageHandler:
    def __init__(self, consumer):
        self.consumer = consumer

    def format_table(self, content):
        """Format content as an HTML table if it contains tabular data"""
        try:
            # Check if content has table-like format (e.g., "Date | Users")
            if '|' in content and '-|-' in content:
                lines = [line.strip() for line in content.strip().split('\n')]
                
                # Find the header line
                header_line = None
                separator_line = None
                data_lines = []
                
                for i, line in enumerate(lines):
                    if '|' in line:
                        if header_line is None:
                            header_line = line
                        elif separator_line is None and '-|-' in line:
                            separator_line = line
                        else:
                            data_lines.append(line)
                
                if not header_line or not separator_line:
                    return content
                    
                # Process headers
                headers = [h.strip() for h in header_line.split('|') if h.strip()]
                
                # Create HTML table
                html = ['<table class="table"><thead><tr>']
                html.extend([f'<th>{h}</th>' for h in headers])
                html.append('</tr></thead><tbody>')
                
                # Process data rows
                for line in data_lines:
                    if '|' in line:
                        cells = [cell.strip() for cell in line.split('|') if cell.strip()]
                        if cells:
                            html.append('<tr>')
                            html.extend([f'<td>{cell}</td>' for cell in cells])
                            html.append('</tr>')
                
                html.append('</tbody></table>')
                return '\n'.join(html)
                
            return content
            
        except Exception as e:
            logger.error(f"Error formatting table: {str(e)}")
            return content

    def format_tool_output(self, content):
        """Format tool output with proper styling"""
        try:
            if isinstance(content, dict):
                return f'<div class="json-output">{json.dumps(content, indent=2)}</div>'
            elif isinstance(content, str):
                # Try to parse as JSON first
                try:
                    json_content = json.loads(content)
                    return f'<div class="json-output">{json.dumps(json_content, indent=2)}</div>'
                except json.JSONDecodeError:
                    pass
                
                # Format as table if possible
                content = self.format_table(content)
            
            return f'<div class="tool-output">{content}</div>'
        except Exception as e:
            logger.error(f"Error formatting tool output: {str(e)}")
            return content

    def format_tool_usage(self, content, message_type=None):
        """Format tool usage messages"""
        if message_type == "tool_start" and content.startswith('Using tool:'):
            tool_info = content.split('\n')
            formatted = f'''
            <div class="tool-usage">
                <i class="fas fa-tools"></i>
                <div>
                    <strong>{tool_info[0]}</strong>
                    <div class="tool-input">{tool_info[1] if len(tool_info) > 1 else ''}</div>
                </div>
            </div>
            '''
            return formatted
        elif message_type == "tool_error":
            return f'''
            <div class="tool-error">
                <i class="fas fa-exclamation-triangle"></i>
                <div>{content}</div>
            </div>
            '''
        return content

    def format_final_answer(self, content):
        """Format the final agent response"""
        try:
            # Format as table if possible
            content = self.format_table(content)
            return f'<div class="agent-response">{content}</div>'
        except Exception as e:
            logger.error(f"Error formatting final answer: {str(e)}")
            return content

    async def handle_message(self, message, is_agent=True, error=False, is_stream=False, message_type=None):
        """Format and send a message"""
        try:
            content = str(message)
            
            if is_agent:
                # Handle invalid/incomplete response errors
                if isinstance(message, dict) and 'steps' in message:
                    step = message['steps'][0]
                    if step.action.tool == '_Exception' and 'Could not parse LLM output' in step.log:
                        logger.warning(f"LLM parsing error: {step.log}")
                        error = True
                        content = "I encountered an error processing your request. Let me try again with a simpler query."

                # Apply formatting based on message type
                if message_type == "tool_output":
                    content = self.format_tool_output(content)
                elif message_type in ["tool_start", "tool_error"]:
                    content = self.format_tool_usage(content, message_type)
                elif message_type == "final_answer":
                    content = self.format_final_answer(content)
                else:
                    # Default formatting for other types
                    content = self.format_table(content)
            
            response_data = {
                'type': 'agent_message' if is_agent else 'user_message',
                'message': content,
                'is_agent': bool(is_agent),
                'error': bool(error),
                'is_stream': bool(is_stream),
                'message_type': message_type,
                'timestamp': datetime.now().isoformat()
            }
            
            logger.debug(f"ðŸ“¤ Sending {'agent' if is_agent else 'user'} message type: {message_type}")
            
            await self.consumer.send_json(response_data)
            
        except Exception as e:
            logger.error(f"Error in message handler: {str(e)}")
            await self.consumer.send_json({
                'type': 'error',
                'error': True,
                'message': 'Error processing message'
            })

    async def handle_keep_alive(self):
        """Handle keep-alive messages"""
        await self.consumer.send_json({
            'type': 'keep_alive_response',
            'timestamp': datetime.now().isoformat()
        })

================
File: websockets/services/chat_service.py
================
from apps.common.utils import get_llm, format_message
from apps.agents.utils import get_tool_classes
from langchain.agents import AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import Tool, StructuredTool
from apps.agents.chat.history import DjangoCacheMessageHistory
from channels.db import database_sync_to_async
from langchain_core.messages import SystemMessage, HumanMessage
import json
import logging

logger = logging.getLogger(__name__)

class ChatService:
    def __init__(self, agent, model_name, client_data, callback_handler, session_id=None):
        self.agent = agent
        self.model_name = model_name
        self.client_data = client_data
        self.callback_handler = callback_handler
        self.llm = None
        self.token_counter = None
        self.agent_executor = None
        self.processing = False
        self.session_id = session_id or f"{agent.id}_{client_data['client_id'] if client_data else 'no_client'}"

    async def initialize(self):
        """Initialize the chat service with LLM and agent"""
        try:
            # Get LLM and token counter from utils
            self.llm, self.token_counter = get_llm(
                model_name=self.model_name,
                temperature=0.7,
                streaming=True
            )

            # Load tools
            tools = await self._load_tools()
            
            # Initialize memory
            message_history = DjangoCacheMessageHistory(
                session_id=self.session_id,
                ttl=3600
            )

            memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                chat_memory=message_history,
                output_key="output",
                input_key="input"
            )

            # Get tool names and descriptions
            tool_names = [tool.name for tool in tools]
            tool_descriptions = [f"{tool.name}: {tool.description}" for tool in tools]

            # Create prompt with required variables
            prompt = ChatPromptTemplate.from_messages([
                ("system", """
{system_prompt}

You have access to the following tools:
{tools}

Tool Names: {tool_names}

IMPORTANT INSTRUCTIONS:
1. If a tool call fails, examine the error message and try to fix the parameters
2. If multiple tool calls fail, return a helpful message explaining the limitation
3. Always provide a clear response even if data is limited
4. Never give up without providing some useful information
5. Keep responses focused and concise

To use a tool, respond with:
{{"action": "tool_name", "action_input": {{"param1": "value1", "param2": "value2"}}}}

For final responses, use:
{{"action": "Final Answer", "action_input": "your response here"}}
"""),
                ("human", "{input}"),
                ("ai", "{agent_scratchpad}"),
                ("system", "Previous conversation:\n{chat_history}")
            ])

            # Create the agent
            from langchain.agents import create_structured_chat_agent
            agent = create_structured_chat_agent(
                llm=self.llm,
                tools=tools,
                prompt=prompt.partial(
                    system_prompt=await self._create_agent_prompt(),
                    tools="\n".join(tool_descriptions),
                    tool_names=", ".join(tool_names)
                )
            )
            
            # Create agent executor
            self.agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                memory=memory,
                verbose=True,
                max_iterations=5,
                max_execution_time=60,
                early_stopping_method="force",
                handle_parsing_errors=True,
                return_intermediate_steps=False,
                output_key="output",
                input_key="input"
            )
            
            return self.agent_executor

        except Exception as e:
            logger.error(f"Error initializing chat service: {str(e)}", exc_info=True)
            raise

    async def process_message(self, message: str) -> str:
        """Process a message using the agent"""
        if not self.agent_executor:
            raise ValueError("Agent executor not initialized")

        if self.processing:
            return None

        try:
            self.processing = True
            
            # Format input as expected by the agent
            input_data = {
                "input": message,
                "chat_history": self.agent_executor.memory.chat_memory.messages if self.agent_executor.memory else []
            }
            
            logger.debug(f"Processing input: {input_data}")
            
            # Process message with streaming
            async for chunk in self.agent_executor.astream(
                input_data
            ):
                logger.debug(f"Received chunk: {chunk}")
                try:
                    # Handle different types of chunks
                    if isinstance(chunk, dict):
                        if "output" in chunk:
                            # Send the actual output
                            await self.callback_handler.on_llm_new_token(chunk["output"])
                        elif "intermediate_steps" in chunk:
                            # Log tool usage
                            for step in chunk["intermediate_steps"]:
                                if len(step) >= 2:
                                    action, output = step
                                    tool_msg = f"Using tool: {action.tool}\nInput: {action.tool_input}\nOutput: {output}"
                                    await self.callback_handler.on_llm_new_token(tool_msg)
                        else:
                            # Send any other content
                            content = str(chunk)
                            if content.strip():  # Only send non-empty content
                                await self.callback_handler.on_llm_new_token(content)
                    else:
                        # Handle direct string output
                        content = str(chunk)
                        if content.strip():  # Only send non-empty content
                            await self.callback_handler.on_llm_new_token(content)
                            
                except Exception as chunk_error:
                    logger.error(f"Error processing chunk: {str(chunk_error)}")
                    await self.callback_handler.on_llm_new_token(f"Error processing response: {str(chunk_error)}")
                    continue
                    
            return None  # Return None instead of "Response complete"

        except Exception as e:
            error_msg = f"Error processing message: {str(e)}"
            logger.error(error_msg)
            await self.callback_handler.on_llm_error(error_msg)
            raise
        finally:
            self.processing = False

    @database_sync_to_async
    def _load_tools(self):
        """Load and initialize agent tools asynchronously"""
        try:
            tools = []
            seen_tools = set()
            
            for tool_model in self.agent.tools.all():
                try:
                    tool_key = f"{tool_model.tool_class}_{tool_model.tool_subclass}"
                    if tool_key in seen_tools:
                        continue
                    seen_tools.add(tool_key)

                    tool_classes = get_tool_classes(tool_model.tool_class)
                    tool_class = next((cls for cls in tool_classes 
                                   if cls.__name__ == tool_model.tool_subclass), None)
                    
                    if tool_class:
                        logger.info(f"Initializing tool: {tool_class.__name__}")
                        tool_instance = tool_class()
                        
                        # Wrap tool output formatting
                        def format_tool_output(func):
                            def wrapper(*args, **kwargs):
                                result = func(*args, **kwargs)
                                if isinstance(result, dict):
                                    return json.dumps(result, indent=2)
                                return str(result)
                            return wrapper
                        
                        # Create structured or basic tool
                        if hasattr(tool_instance, 'args_schema'):
                            wrapped_run = format_tool_output(tool_instance._run)
                            tool = StructuredTool.from_function(
                                func=wrapped_run,
                                name=tool_model.name.lower().replace(" ", "_"),
                                description=self._create_tool_description(tool_instance, tool_model),
                                args_schema=tool_instance.args_schema,
                                coroutine=tool_instance.arun if hasattr(tool_instance, 'arun') else None,
                                return_direct=False
                            )
                        else:
                            wrapped_run = format_tool_output(tool_instance._run)
                            tool = Tool(
                                name=tool_model.name.lower().replace(" ", "_"),
                                description=self._create_tool_description(tool_instance, tool_model),
                                func=wrapped_run,
                                coroutine=tool_instance.arun if hasattr(tool_instance, 'arun') else None
                            )
                        
                        tools.append(tool)
                        logger.info(f"Successfully loaded tool: {tool_model.name}")
                        
                except Exception as e:
                    logger.error(f"Error loading tool {tool_model.name}: {str(e)}")
                    
            return tools
            
        except Exception as e:
            logger.error(f"Error loading tools: {str(e)}")
            return []

    def _create_tool_description(self, tool_instance, tool_model):
        """Create a detailed description for the tool"""
        try:
            base_description = tool_instance.description or tool_model.description
            schema = tool_instance.args_schema

            if schema:
                field_descriptions = []
                for field_name, field in schema.model_fields.items():
                    field_type = str(field.annotation).replace('typing.', '')
                    if hasattr(field.annotation, '__name__'):
                        field_type = field.annotation.__name__
                    
                    field_desc = field.description or ''
                    default = field.default
                    if default is Ellipsis:
                        default = "Required"
                    elif default is None:
                        default = "Optional"
                    
                    field_descriptions.append(
                        f"- {field_name} ({field_type}): {field_desc} Default: {default}"
                    )

                tool_description = f"""{base_description}

Parameters:
{chr(10).join(field_descriptions)}

Example:
{{"action": "{tool_model.name.lower().replace(' ', '_')}", 
  "action_input": {{
    "client_id": 123,
    "start_date": "2024-01-01",
    "end_date": "2024-01-31",
    "metrics": "newUsers",
    "dimensions": "date"
  }}
}}"""
                
                return tool_description
            
            return base_description

        except Exception as e:
            logger.error(f"Error creating tool description: {str(e)}")
            return base_description or "Tool description unavailable"

    @database_sync_to_async
    def _create_agent_prompt(self):
        """Create the system prompt for the agent"""
        client_context = ""
        if self.client_data:
            client_context = f"""Current Context:
- Client ID: {self.client_data.get('client_id', 'N/A')}
- Current Date: {self.client_data.get('current_date', 'N/A')}"""

        return f"""You are {self.agent.name}, an AI assistant.
Role: {self.agent.role}

{client_context}

{self.agent.use_system_prompt if self.agent.use_system_prompt else ''}

RESPONSE FORMAT INSTRUCTIONS:
-1. You MUST respond with ONLY a single JSON object in one of these two formats:
+1. You MUST respond with ONLY a single JSON object in one of these two formats:

For using a tool:
{{"action": "tool_name", "action_input": {{"param1": "value1", "param2": "value2"}}}}

For final answers:
{{"action": "Final Answer", "action_input": "your response here"}}

-2. Never respond with plain text or any other format
-3. Always ensure your response is valid JSON
-4. If you want to have a conversation, use the Final Answer format
-5. Make sure to escape any quotes or special characters in your responses
+2. CRITICAL RULES:
+   - Send ONLY the JSON object - no explanations before or after
+   - No plain text responses
+   - No markdown formatting
+   - For conversations or explanations, use Final Answer format
+   - If a tool call fails, retry with corrected parameters
+   - Ensure all JSON is properly formatted with double quotes

-Example conversational response:
-{{"action": "Final Answer", "action_input": "Hello! How can I help you today?"}}
+Examples:
+# Tool usage:
+{{"action": "generic_google_analytics_data_fetcher", "action_input": {{"client_id": 1, "start_date": "30daysAgo", "end_date": "today", "metrics": "totalUsers,sessions", "dimensions": "date"}}}}
+
+# Conversation:
+{{"action": "Final Answer", "action_input": "Based on the data, your website had 1,000 users last month. Would you like more detailed metrics?"}}"""

================
File: websockets/__init__.py
================
from .chat_consumer import ChatConsumer
from .base import BaseWebSocketConsumer

__all__ = ['ChatConsumer', 'BaseWebSocketConsumer']

================
File: websockets/base.py
================
from channels.generic.websocket import AsyncWebsocketConsumer
import json
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class BaseWebSocketConsumer(AsyncWebsocketConsumer):
    async def send_json(self, data):
        """Send JSON data as text"""
        try:
            await self.send(text_data=json.dumps(data))
        except Exception as e:
            logger.error(f"Error sending JSON: {str(e)}")
            await self.send(text_data=json.dumps({
                'error': True,
                'message': 'Error sending message'
            }))

    async def handle_binary_message(self, message):
        """Handle binary message data"""
        try:
            if isinstance(message, bytes):
                message = message.decode('utf-8')
            return json.loads(message)
        except Exception as e:
            logger.error(f"Error handling binary message: {str(e)}")
            return None

================
File: websockets/chat_consumer.py
================
from .base import BaseWebSocketConsumer
from .handlers.message_handler import MessageHandler
from .handlers.agent_handler import AgentHandler
from ..tools.manager import AgentToolManager
from ..clients.manager import ClientDataManager
from ..chat.history import DjangoCacheMessageHistory
from ..models import Conversation
import logging
import uuid
import json
from datetime import datetime
from urllib.parse import parse_qs

logger = logging.getLogger(__name__)

class ChatConsumer(BaseWebSocketConsumer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.tool_manager = AgentToolManager()
        self.client_manager = ClientDataManager()
        self.session_id = None
        self.group_name = None
        self.message_handler = MessageHandler(self)
        self.agent_handler = AgentHandler(self)
        self.is_connected = False
        self.message_history = None

    async def connect(self):
        if self.is_connected:
            return

        try:
            # Get session ID from query parameters
            query_string = self.scope.get('query_string', b'').decode()
            params = dict(param.split('=') for param in query_string.split('&') if param)
            self.session_id = params.get('session')
            
            if not self.session_id:
                logger.error("No session ID provided")
                await self.close()
                return
                
            self.user = self.scope.get("user")
            if not self.user or not self.user.is_authenticated:
                logger.error("User not authenticated")
                await self.close()
                return
                
            self.group_name = f"chat_{self.session_id}"
            self.message_history = DjangoCacheMessageHistory(session_id=self.session_id)
            
            # Get or create conversation
            conversation = await self.get_or_create_conversation()
            if not conversation:
                logger.error("Failed to get/create conversation")
                await self.close()
                return
                
            await self.channel_layer.group_add(self.group_name, self.channel_name)
            await self.accept()
            self.is_connected = True
            
            # Send historical messages
            messages = self.message_history.messages
            for msg in messages:
                message_type = 'agent_message' if msg.type == 'ai' else 'user_message'
                message_content = msg.content
                
                # If it's an agent message, check if it's a tool usage
                if message_type == 'agent_message' and (
                    'AgentAction(tool=' in message_content or 
                    'AgentStep(action=' in message_content
                ):
                    # Send as is - the frontend will handle the formatting
                    pass
                
                await self.send_json({
                    'type': message_type,
                    'message': message_content,
                    'timestamp': conversation.updated_at.isoformat() if conversation else None
                })
            
            logger.info(f"WebSocket connected for session {self.session_id}")
            await self.send_json({
                'type': 'system_message',
                'message': 'Connected to chat server',
                'connection_status': 'connected',
                'session_id': self.session_id
            })
            
        except Exception as e:
            logger.error(f"Error in connect: {str(e)}")
            await self.close()
            return

    async def get_or_create_conversation(self):
        try:
            # Get existing conversation
            conversation = await Conversation.objects.filter(
                session_id=self.session_id,
                user=self.user
            ).afirst()
            
            if not conversation:
                # Create new conversation with placeholder title
                conversation = await Conversation.objects.acreate(
                    session_id=self.session_id,
                    user=self.user,
                    title="..."  # Will be updated with first message
                )
                logger.info(f"Created new conversation: {conversation.id}")
            else:
                logger.info(f"Found existing conversation: {conversation.id}")
            
            return conversation
            
        except Exception as e:
            logger.error(f"Error getting/creating conversation: {str(e)}")
            return None

    async def update_conversation(self, message, agent_id=None, client_id=None):
        try:
            conversation = await Conversation.objects.filter(
                session_id=self.session_id
            ).afirst()
            
            if conversation:
                # Update title if it's still the default
                if conversation.title == "...":
                    # Clean and truncate the message for the title
                    title = message.strip().replace('\n', ' ')[:50]
                    # Add ellipsis if truncated
                    if len(message) > 50:
                        title += "..."
                    conversation.title = title
                
                # Update agent and client if provided
                if agent_id:
                    conversation.agent_id = agent_id
                if client_id:
                    conversation.client_id = client_id
                    
                await conversation.asave()
                logger.info(f"Updated conversation: {conversation.id} with title: {conversation.title}")
                
        except Exception as e:
            logger.error(f"Error updating conversation: {str(e)}")

    async def receive(self, text_data=None, bytes_data=None):
        try:
            # Handle binary data if present
            if bytes_data:
                data = await self.handle_binary_message(bytes_data)
            else:
                data = json.loads(text_data)
                if data.get('type') != 'keep_alive':
                    logger.debug(f"ðŸ“¥ Received: {text_data}")

            if data.get('type') == 'keep_alive':
                await self.message_handler.handle_keep_alive()
                return

            await self.process_message(data)

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON decode error: {str(e)}")
            await self.message_handler.handle_message(
                'Invalid message format', is_agent=True, error=True
            )
        except Exception as e:
            logger.error(f"âŒ Error: {str(e)}")
            await self.message_handler.handle_message(
                'Internal server error', is_agent=True, error=True) 

    async def process_message(self, data):
        """Process incoming message data"""
        try:
            message = data.get('message', '').strip()
            agent_id = data.get('agent_id')
            model_name = data.get('model')
            client_id = data.get('client_id')

            if not message or not agent_id:
                await self.message_handler.handle_message(
                    'Missing required fields (message or agent_id)',
                    is_agent=True,
                    error=True
                )
                return

            # Update conversation details before processing
            await self.update_conversation(message, agent_id, client_id if client_id else None)

            # Echo user's message back with proper type
            logger.debug("ðŸ“¤ Sending user message")
            await self.send_json({
                'type': 'user_message',
                'message': message,
                'timestamp': datetime.now().isoformat()
            })

            # Process with agent
            logger.debug("ðŸ¤– Processing with agent")
            response = await self.agent_handler.process_response(
                message,
                agent_id,
                model_name,
                client_id if client_id else None
            )

            # Handle error responses
            if isinstance(response, str) and response.startswith('Error:'):
                await self.send_json({
                    'type': 'error',
                    'message': response,
                    'timestamp': datetime.now().isoformat()
                })
                return

            logger.debug("ðŸ“¤ Sending agent response")
            await self.send_json({
                'type': 'agent_message',
                'message': response,
                'timestamp': datetime.now().isoformat()
            })

        except Exception as e:
            logger.error(f"âŒ Error: {str(e)}")
            await self.send_json({
                'type': 'error',
                'message': f"Error processing message: {str(e)}",
                'error': True
            })

================
File: websockets/consumers.py
================
async def handle_tool_call(self, tool_name, tool_args):
    """Handle tool calls from the agent"""
    try:
        # Implement your tool handling logic here
        if tool_name == "search":
            result = await self.search_tool(tool_args)
        elif tool_name == "calculator":
            result = await self.calculator_tool(tool_args)
        # Add more tool handlers as needed
        
        return result
    except Exception as e:
        logger.error(f"Error handling tool call: {str(e)}")
        return f"Error executing tool {tool_name}: {str(e)}"

================
File: websockets/crew_consumer.py
================
from channels.generic.websocket import AsyncWebsocketConsumer
# ... other imports ...

class CrewExecutionConsumer(AsyncWebsocketConsumer):
    # ... existing implementation ...
    pass

================
File: consumers.py
================
import json
from channels.generic.websocket import AsyncWebsocketConsumer
from channels.db import database_sync_to_async
from django.contrib.auth import get_user_model
from .models import CrewExecution, CrewMessage, ChatMessage, Agent
from django.core.cache import cache
from apps.common.utils import format_message, get_llm
from .utils import get_tool_classes
import logging
import uuid
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, FunctionMessage, BaseMessage, messages_from_dict, messages_to_dict
import asyncio
import tiktoken
from langchain_community.chat_models import ChatLiteLLM
from langchain_core.tools import Tool
from django.utils import timezone
from apps.seo_manager.models import Client
from langchain.prompts import ChatPromptTemplate
from langchain.agents import initialize_agent, AgentType, AgentExecutor, create_structured_chat_agent
from langchain.agents.format_scratchpad import format_to_openai_function_messages
from langchain.agents.output_parsers import JSONAgentOutputParser
from langchain_core.callbacks import BaseCallbackHandler
import datetime
from langchain.tools import StructuredTool
from typing import Dict, Any, List
from pydantic import create_model
from langchain.memory import ConversationBufferMemory
from langchain_core.chat_history import BaseChatMessageHistory
import re
import time

logger = logging.getLogger(__name__)

def count_tokens(text):
    """Count tokens in text using tiktoken"""
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))

class ConnectionTestConsumer(AsyncWebsocketConsumer):
    async def connect(self):
        await self.accept()
        await self.send(text_data=json.dumps({
            'message': 'Connected to server'
        }))

    async def disconnect(self, close_code):
        pass

    async def receive(self, text_data):
        try:
            text_data_json = json.loads(text_data)
            message = text_data_json['message']

            # Echo the received message back to the client
            await self.send(text_data=json.dumps({
                'message': f'Server received: {message}'
            }))
        except json.JSONDecodeError:
            await self.send(text_data=json.dumps({
                'error': 'Invalid JSON format'
            }))
        except KeyError:
            await self.send(text_data=json.dumps({
                'error': 'Missing "message" key in JSON'
            }))

class CrewExecutionConsumer(AsyncWebsocketConsumer):
    async def connect(self):
        self.execution_id = self.scope['url_route']['kwargs']['execution_id']
        self.execution_group_name = f'crew_execution_{self.execution_id}'

        # Join room group
        await self.channel_layer.group_add(
            self.execution_group_name,
            self.channel_name
        )

        await self.accept()

        # Send initial status
        await self.send_execution_status()

    async def disconnect(self, close_code):
        # Leave room group
        await self.channel_layer.group_discard(
            self.execution_group_name,
            self.channel_name
        )

    async def receive(self, text_data):
        text_data_json = json.loads(text_data)
        message_type = text_data_json.get('type')

        if message_type == 'human_input':
            input_key = text_data_json.get('input_key')
            user_input = text_data_json.get('input')
            await self.handle_human_input(input_key, user_input)

    async def crew_execution_update(self, event):
        status = event.get('status', '')  # No formatting applied
        formatted_messages = [
            {
                'agent': msg.get('agent', 'System'),
                'content': format_message(msg.get('content', ''))
            } for msg in event.get('messages', []) if msg.get('content')
        ]
        # logger.info(f"Sending status: {status}")
        # logger.info(f"Sending formatted messages: {formatted_messages}")
        await self.send(text_data=json.dumps({
            'status': status,
            'messages': formatted_messages,
            'human_input_request': event.get('human_input_request')
        }))

    @database_sync_to_async
    def handle_human_input(self, input_key, user_input):
        cache.set(f"{input_key}_response", user_input, timeout=3600)
        execution = CrewExecution.objects.get(id=self.execution_id)
        CrewMessage.objects.create(
            execution=execution,
            agent='Human',
            content=f"Human input received: {user_input}"
        )

    @database_sync_to_async
    def get_execution_status(self):
        execution = CrewExecution.objects.get(id=self.execution_id)
        messages = CrewMessage.objects.filter(execution=execution).order_by('-timestamp')[:10]
        return {
            'status': execution.status,
            'messages': [{'agent': msg.agent, 'content': msg.content} for msg in messages],
        }

    async def send_execution_status(self):
        status_data = await self.get_execution_status()
        status = status_data['status']  # No formatting applied
        formatted_messages = [
            {
                'agent': msg['agent'],
                'content': format_message(msg['content'])
            } for msg in status_data['messages'] if msg.get('content')
        ]
        
        # logger.info(f"Sending status: {status}")
        # logger.info(f"Sending formatted messages: {formatted_messages}")
        
        await self.send(text_data=json.dumps({
            'status': status,
            'messages': formatted_messages,
        }))

================
File: routing.py
================
from django.urls import re_path
from .consumers import ConnectionTestConsumer, ChatConsumer, CrewExecutionConsumer
from .kanban_consumers import CrewKanbanConsumer

websocket_urlpatterns = [
    re_path(r'ws/chat/$', ChatConsumer.as_asgi()),
    re_path(r'ws/crew_execution/(?P<execution_id>\w+)/$', CrewExecutionConsumer.as_asgi()),
    re_path(r'ws/test-connection/$', ConnectionTestConsumer.as_asgi()),
]
