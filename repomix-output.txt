This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-21T13:42:56.161Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
common/
  management/
    commands/
      copy_agents_db.py
      copy_clients_db copy 2.py
      copy_clients_db copy.py
      copy_clients_db.py
      copy_credentials_db.py
      copy_db.py
  tools/
    async_crawl_website_tool.py
    browser_tools.py
    crawl_website_search_tool.py
    crawl_website_tool.py
    ExaSearchTool.py
    google_suggestions_tool.py
    keyword_tools.py
    keywords_for_site_tool.py
    rag_tool.py
    report_generator.py
    screenshot_tool.py
    searxng_tool.py
    summarizer.py
    user_activity_tool.py
    website_search_tool.py
  __init__.py
  admin.py
  apps.py
  browser_tool.py
  chat_model_handler.py
  compression_manager.py
  content_loader.py
  models.py
  summarization_manager.py
  summarizer.py
  tests.py
  utils.py
  views.py
file_manager/
  templatetags/
    file_extension.py
    file_manager_extras.py
    info_value.py
  admin.py
  apps.py
  models.py
  tests.py
  urls.py
  views.py
seo_manager/
  management/
    commands/
      backfill_rankings.py
      generate_monthly_report.py
  templatetags/
    __init__.py
    custom_filters.py
    form_tags.py
    seo_manager_filters.py
    seo_tags.py
  utils/
    meta_tags_analyzer.py
  views/
    __init__.py
    activity_views.py
    analytics_views.py
    auth_views.py
    business_objective_views.py
    client_views.py
    keyword_views.py
    meta_tags_views.py
    project_views.py
    ranking_views.py
    report_views.py
    search_console_views.py
  admin.py
  apps.py
  exceptions.py
  forms.py
  google_auth.py
  middleware.py
  models.py
  services.py
  sitemap_extractor.py
  urls.py
  utils.py
  views_analytics.py
  views_summarizer.py
  views.py
users/
  admin.py
  apps.py
  forms.py
  models.py
  signals.py
  tests.py
  urls.py
  views.py

================================================================
Repository Files
================================================================

================
File: common/management/commands/copy_agents_db.py
================
from django.core.management.base import BaseCommand
from django.apps import apps
from django.db import connections
from django.db.migrations.executor import MigrationExecutor
from apps.agents import models as agent_models
from django.db.models import Max

class Command(BaseCommand):
    help = 'Copy data from agents app models (excluding runtime results) from source database to target database'

    def add_arguments(self, parser):
        parser.add_argument('--source', type=str, default='default', help='Source database')
        parser.add_argument('--target', type=str, required=True, help='Target database')
        parser.add_argument('--skip-migrations', action='store_true', help='Skip running migrations on target database')

    def handle(self, *args, **options):
        source_db = options['source']
        target_db = options['target']
        skip_migrations = options['skip_migrations']

        # Check databases
        if source_db not in connections:
            self.stderr.write(self.style.ERROR(f"Source database '{source_db}' is not configured"))
            return
        if target_db not in connections:
            self.stderr.write(self.style.ERROR(f"Target database '{target_db}' is not configured"))
            return

        # Run migrations if needed
        if not skip_migrations:
            executor = MigrationExecutor(connections[target_db])
            plan = executor.migration_plan(executor.loader.graph.leaf_nodes())
            if plan:
                self.stdout.write(self.style.NOTICE('Target database needs migrations. Running migrations first...'))
                executor.migrate(targets=executor.loader.graph.leaf_nodes(), plan=plan)

        # Models to exclude (runtime results)
        excluded_models = {'CrewExecution', 'CrewMessage', 'CrewOutput'}

        # Get only models from agents app, excluding runtime results
        all_models = [
            m for m in apps.get_models() 
            if m.__module__ == 'apps.agents.models' 
            and m.__name__ not in excluded_models
        ]
        m2m_data = {}  # Store all M2M relationships

        # First pass: Copy all models without M2M relationships
        for model in all_models:
            try:
                self.stdout.write(f"Copying {model.__name__}...")
                
                # Get all objects from source
                objects = list(model.objects.using(source_db).all())
                
                if not objects:
                    self.stdout.write(self.style.NOTICE(f"No {model.__name__} objects to copy"))
                    continue

                # Store M2M relationships
                m2m_data[model] = {}
                for obj in objects:
                    m2m_fields = [f for f in obj._meta.get_fields() if f.many_to_many and not f.auto_created]
                    if m2m_fields:
                        m2m_data[model][obj.pk] = {
                            field.name: list(getattr(obj, field.name).all().values_list('pk', flat=True))
                            for field in m2m_fields
                        }

                # Clear target
                try:
                    model.objects.using(target_db).all().delete()
                except Exception as e:
                    self.stderr.write(self.style.WARNING(f"Could not clear {model.__name__}: {str(e)}"))

                # Copy objects
                new_objects = model.objects.using(target_db).bulk_create(
                    objects,
                    batch_size=1000,
                    ignore_conflicts=True
                )

                self.stdout.write(self.style.SUCCESS(f"Copied {len(objects)} {model.__name__} objects"))

            except Exception as e:
                self.stderr.write(self.style.ERROR(f"Error copying {model.__name__}: {str(e)}"))
                continue

        # Second pass: Restore M2M relationships
        self.stdout.write(self.style.NOTICE("Restoring many-to-many relationships..."))
        
        for model, relationships in m2m_data.items():
            try:
                for obj_id, fields in relationships.items():
                    try:
                        obj = model.objects.using(target_db).get(pk=obj_id)
                        for field_name, related_ids in fields.items():
                            m2m_field = getattr(obj, field_name)
                            m2m_field.clear()
                            m2m_field.add(*related_ids)
                    except model.DoesNotExist:
                        self.stderr.write(self.style.WARNING(f"Could not find {model.__name__} with pk {obj_id}"))
                        continue
            except Exception as e:
                self.stderr.write(self.style.ERROR(f"Error restoring M2M relationships for {model.__name__}: {str(e)}"))
                continue

        self.stdout.write(self.style.SUCCESS('Successfully completed agents database copy operation (excluding runtime results)'))

    def copy_model(self, model, source_using='default', target_using='target'):
        objects = model.objects.using(source_using).all()
        if not objects:
            self.stdout.write(f'No {model.__name__} objects to copy')
            return
        
        self.stdout.write(f'Copying {model.__name__}...')
        
        # Get the max ID from the target database
        max_id_target = model.objects.using(target_using).aggregate(Max('id'))['id__max'] or 0
        
        # Create a mapping of old IDs to new IDs
        id_mapping = {}
        
        for obj in objects:
            old_id = obj.id
            # Assign a new ID that's guaranteed to be unique
            obj.id = max_id_target + old_id + 1
            id_mapping[old_id] = obj.id
        
        model.objects.using(target_using).bulk_create(objects)
        self.stdout.write(f'Copied {len(objects)} {model.__name__} objects')
        
        return id_mapping

    def restore_m2m_relationships(self, model, id_mapping, source_using='default', target_using='target'):
        try:
            for obj in model.objects.using(source_using).all():
                new_obj = model.objects.using(target_using).get(id=id_mapping[obj.id])
                
                # Get all M2M fields
                for field in model._meta.many_to_many:
                    # Get the related objects from source
                    related_objects = getattr(obj, field.name).all()
                    
                    # Clear existing relationships in target
                    getattr(new_obj, field.name).clear()
                    
                    # Add the mapped related objects
                    for related_obj in related_objects:
                        if hasattr(related_obj, 'id'):
                            mapped_id = id_mapping.get(related_obj.id)
                            if mapped_id:
                                getattr(new_obj, field.name).add(mapped_id)
        
        except Exception as e:
            self.stdout.write(f'Error restoring M2M relationships for {model.__name__}: {str(e)}')

================
File: common/management/commands/copy_clients_db copy 2.py
================
from django.core.management.base import BaseCommand
from django.apps import apps
from django.db import connections, transaction
from django.db.models import Q
from apps.seo_manager.models import (
    Client,
    GoogleAnalyticsCredentials,
    SearchConsoleCredentials,
    TargetedKeyword,
    SEOData,
    ClientGroup,
    SEOProject,
)

class Command(BaseCommand):
    help = 'Copy Client and related data from source database to target database'

    def add_arguments(self, parser):
        parser.add_argument('--source', type=str, default='default', help='Source database')
        parser.add_argument('--target', type=str, required=True, help='Target database')

    def handle(self, *args, **options):
        source_db = options['source']
        target_db = options['target']

        # Check databases
        if source_db not in connections:
            self.stderr.write(self.style.ERROR(f"Source database '{source_db}' is not configured"))
            return
        if target_db not in connections:
            self.stderr.write(self.style.ERROR(f"Target database '{target_db}' is not configured"))
            return

        with transaction.atomic(using=target_db):
            # Copy Client Groups first (maintaining hierarchy)
            self.copy_client_groups(source_db, target_db)
            # Copy Clients and related data
            self.copy_clients(source_db, target_db)

        self.stdout.write(self.style.SUCCESS('Successfully completed clients database copy operation'))

    def copy_client_groups(self, source_db, target_db):
        """Copy client groups while maintaining parent-child relationships"""
        client_groups = list(ClientGroup.objects.using(source_db).all())
        if not client_groups:
            self.stdout.write(self.style.NOTICE("No ClientGroup objects to copy"))
            return

        # Create mapping of old PKs to new objects
        group_mapping = {}
        
        for group in client_groups:
            old_pk = group.pk
            existing_group = ClientGroup.objects.using(target_db).filter(name=group.name).first()
            
            if existing_group:
                # Update existing group
                existing_group.name = group.name
                existing_group.save(using=target_db)
                group_mapping[old_pk] = existing_group
            else:
                # Create new group
                group.pk = None
                group.parent = None  # Temporarily remove parent reference
                group.save(using=target_db)
                group_mapping[old_pk] = group

        # Update parent relationships
        for original_group in client_groups:
            if original_group.parent_id:
                new_group = group_mapping[original_group.pk]
                new_group.parent = group_mapping.get(original_group.parent_id)
                new_group.save(using=target_db)

        self.stdout.write(self.style.SUCCESS(f"Processed {len(client_groups)} ClientGroup objects"))

    def copy_clients(self, source_db, target_db):
        clients = list(Client.objects.using(source_db).all())
        if not clients:
            self.stdout.write(self.style.NOTICE("No Client objects to copy"))
            return

        for client in clients:
            # Try to find existing client by name
            existing_client = Client.objects.using(target_db).filter(
                Q(name=client.name) | Q(website_url=client.website_url)
            ).first()

            if existing_client:
                # Update existing client
                for field in client._meta.fields:
                    if field.name not in ['id', 'created_at']:
                        setattr(existing_client, field.name, getattr(client, field.name))
                existing_client.save(using=target_db)
                self.copy_related_data(client, existing_client, source_db, target_db)
            else:
                # Create new client
                old_pk = client.pk
                client.pk = None
                client.save(using=target_db)
                self.copy_related_data(Client.objects.using(source_db).get(pk=old_pk), client, source_db, target_db)

        self.stdout.write(self.style.SUCCESS(f"Processed {len(clients)} Client objects"))

    def copy_related_data(self, source_client, target_client, source_db, target_db):
        # Copy Google Analytics Credentials
        ga_creds = GoogleAnalyticsCredentials.objects.using(source_db).filter(client=source_client).first()
        if ga_creds:
            existing_ga = GoogleAnalyticsCredentials.objects.using(target_db).filter(client=target_client).first()
            if existing_ga:
                # Update existing credentials
                for field in ga_creds._meta.fields:
                    if field.name not in ['id', 'client']:
                        setattr(existing_ga, field.name, getattr(ga_creds, field.name))
                existing_ga.save(using=target_db)
            else:
                # Create new credentials
                ga_creds.pk = None
                ga_creds.client = target_client
                ga_creds.save(using=target_db)

        # Copy Search Console Credentials
        sc_creds = SearchConsoleCredentials.objects.using(source_db).filter(client=source_client).first()
        if sc_creds:
            existing_sc = SearchConsoleCredentials.objects.using(target_db).filter(client=target_client).first()
            if existing_sc:
                # Update existing credentials
                for field in sc_creds._meta.fields:
                    if field.name not in ['id', 'client']:
                        setattr(existing_sc, field.name, getattr(sc_creds, field.name))
                existing_sc.save(using=target_db)
            else:
                # Create new credentials
                sc_creds.pk = None
                sc_creds.client = target_client
                sc_creds.save(using=target_db)

        # Copy Targeted Keywords
        keywords = TargetedKeyword.objects.using(source_db).filter(client=source_client)
        for keyword in keywords:
            existing_keyword = TargetedKeyword.objects.using(target_db).filter(
                client=target_client,
                keyword=keyword.keyword
            ).first()
            
            if existing_keyword:
                # Update existing keyword
                for field in keyword._meta.fields:
                    if field.name not in ['id', 'client', 'created_at']:
                        setattr(existing_keyword, field.name, getattr(keyword, field.name))
                existing_keyword.save(using=target_db)
            else:
                # Create new keyword
                keyword.pk = None
                keyword.client = target_client
                keyword.save(using=target_db)

        # Copy SEO Data
        seo_data_entries = SEOData.objects.using(source_db).filter(client=source_client)
        for data in seo_data_entries:
            existing_data = SEOData.objects.using(target_db).filter(
                client=target_client,
                date=data.date
            ).first()
            
            if existing_data:
                # Update existing SEO data
                for field in data._meta.fields:
                    if field.name not in ['id', 'client']:
                        setattr(existing_data, field.name, getattr(data, field.name))
                existing_data.save(using=target_db)
            else:
                # Create new SEO data
                data.pk = None
                data.client = target_client
                data.save(using=target_db)

        # Copy SEO Projects
        projects = SEOProject.objects.using(source_db).filter(client=source_client)
        for project in projects:
            existing_project = SEOProject.objects.using(target_db).filter(
                client=target_client,
                title=project.title
            ).first()
            
            if existing_project:
                # Update existing project
                for field in project._meta.fields:
                    if field.name not in ['id', 'client', 'created_at']:
                        setattr(existing_project, field.name, getattr(project, field.name))
                existing_project.save(using=target_db)
            else:
                # Create new project
                project.pk = None
                project.client = target_client
                project.save(using=target_db)

        self.stdout.write(self.style.SUCCESS(f"Processed related data for Client: {target_client.name}"))

================
File: common/management/commands/copy_clients_db copy.py
================
from django.core.management.base import BaseCommand
from django.apps import apps
from django.db import connections, transaction
from django.db.models import Q
from apps.seo_manager.models import (
    Client,
    GoogleAnalyticsCredentials,
    SearchConsoleCredentials,
    TargetedKeyword,
    SEOData,
    ClientGroup,
    SEOProject,
)

class Command(BaseCommand):
    help = 'Copy Client and related data from source database to target database'

    def add_arguments(self, parser):
        parser.add_argument('--source', type=str, default='default', help='Source database')
        parser.add_argument('--target', type=str, required=True, help='Target database')

    def handle(self, *args, **options):
        source_db = options['source']
        target_db = options['target']

        # Check databases
        if source_db not in connections:
            self.stderr.write(self.style.ERROR(f"Source database '{source_db}' is not configured"))
            return
        if target_db not in connections:
            self.stderr.write(self.style.ERROR(f"Target database '{target_db}' is not configured"))
            return

        with transaction.atomic(using=target_db):
            # Copy Client Groups first (maintaining hierarchy)
            self.copy_client_groups(source_db, target_db)
            # Copy Clients and related data
            self.copy_clients(source_db, target_db)

        self.stdout.write(self.style.SUCCESS('Successfully completed clients database copy operation'))

    def copy_client_groups(self, source_db, target_db):
        """Copy client groups while maintaining parent-child relationships"""
        client_groups = list(ClientGroup.objects.using(source_db).all())
        if not client_groups:
            self.stdout.write(self.style.NOTICE("No ClientGroup objects to copy"))
            return

        # Create mapping of old PKs to new objects
        group_mapping = {}
        
        for group in client_groups:
            old_pk = group.pk
            existing_group = ClientGroup.objects.using(target_db).filter(name=group.name).first()
            
            if existing_group:
                # Update existing group
                existing_group.name = group.name
                existing_group.save(using=target_db)
                group_mapping[old_pk] = existing_group
            else:
                # Create new group
                group.pk = None
                group.parent = None  # Temporarily remove parent reference
                group.save(using=target_db)
                group_mapping[old_pk] = group

        # Update parent relationships
        for original_group in client_groups:
            if original_group.parent_id:
                new_group = group_mapping[original_group.pk]
                new_group.parent = group_mapping.get(original_group.parent_id)
                new_group.save(using=target_db)

        self.stdout.write(self.style.SUCCESS(f"Processed {len(client_groups)} ClientGroup objects"))

    def copy_clients(self, source_db, target_db):
        clients = list(Client.objects.using(source_db).all())
        if not clients:
            self.stdout.write(self.style.NOTICE("No Client objects to copy"))
            return

        for client in clients:
            # Try to find existing client by name
            existing_client = Client.objects.using(target_db).filter(
                Q(name=client.name) | Q(website_url=client.website_url)
            ).first()

            if existing_client:
                # Update existing client
                for field in client._meta.fields:
                    if field.name not in ['id', 'created_at']:
                        setattr(existing_client, field.name, getattr(client, field.name))
                existing_client.save(using=target_db)
                self.copy_related_data(client, existing_client, source_db, target_db)
            else:
                # Create new client
                old_pk = client.pk
                client.pk = None
                client.save(using=target_db)
                self.copy_related_data(Client.objects.using(source_db).get(pk=old_pk), client, source_db, target_db)

        self.stdout.write(self.style.SUCCESS(f"Processed {len(clients)} Client objects"))

    def copy_related_data(self, source_client, target_client, source_db, target_db):
        # Copy Google Analytics Credentials
        ga_creds = GoogleAnalyticsCredentials.objects.using(source_db).filter(client=source_client).first()
        if ga_creds:
            existing_ga = GoogleAnalyticsCredentials.objects.using(target_db).filter(client=target_client).first()
            if existing_ga:
                # Update existing credentials
                for field in ga_creds._meta.fields:
                    if field.name not in ['id', 'client']:
                        setattr(existing_ga, field.name, getattr(ga_creds, field.name))
                existing_ga.save(using=target_db)
            else:
                # Create new credentials
                ga_creds.pk = None
                ga_creds.client = target_client
                ga_creds.save(using=target_db)

        # Copy Search Console Credentials
        sc_creds = SearchConsoleCredentials.objects.using(source_db).filter(client=source_client).first()
        if sc_creds:
            existing_sc = SearchConsoleCredentials.objects.using(target_db).filter(client=target_client).first()
            if existing_sc:
                # Update existing credentials
                for field in sc_creds._meta.fields:
                    if field.name not in ['id', 'client']:
                        setattr(existing_sc, field.name, getattr(sc_creds, field.name))
                existing_sc.save(using=target_db)
            else:
                # Create new credentials
                sc_creds.pk = None
                sc_creds.client = target_client
                sc_creds.save(using=target_db)

        # Copy Targeted Keywords
        keywords = TargetedKeyword.objects.using(source_db).filter(client=source_client)
        for keyword in keywords:
            existing_keyword = TargetedKeyword.objects.using(target_db).filter(
                client=target_client,
                keyword=keyword.keyword
            ).first()
            
            if existing_keyword:
                # Update existing keyword
                for field in keyword._meta.fields:
                    if field.name not in ['id', 'client', 'created_at']:
                        setattr(existing_keyword, field.name, getattr(keyword, field.name))
                existing_keyword.save(using=target_db)
            else:
                # Create new keyword
                keyword.pk = None
                keyword.client = target_client
                keyword.save(using=target_db)

        # Copy SEO Data
        seo_data_entries = SEOData.objects.using(source_db).filter(client=source_client)
        for data in seo_data_entries:
            existing_data = SEOData.objects.using(target_db).filter(
                client=target_client,
                date=data.date
            ).first()
            
            if existing_data:
                # Update existing SEO data
                for field in data._meta.fields:
                    if field.name not in ['id', 'client']:
                        setattr(existing_data, field.name, getattr(data, field.name))
                existing_data.save(using=target_db)
            else:
                # Create new SEO data
                data.pk = None
                data.client = target_client
                data.save(using=target_db)

        # Copy SEO Projects
        projects = SEOProject.objects.using(source_db).filter(client=source_client)
        for project in projects:
            existing_project = SEOProject.objects.using(target_db).filter(
                client=target_client,
                title=project.title
            ).first()
            
            if existing_project:
                # Update existing project
                for field in project._meta.fields:
                    if field.name not in ['id', 'client', 'created_at']:
                        setattr(existing_project, field.name, getattr(project, field.name))
                existing_project.save(using=target_db)
            else:
                # Create new project
                project.pk = None
                project.client = target_client
                project.save(using=target_db)

        self.stdout.write(self.style.SUCCESS(f"Processed related data for Client: {target_client.name}"))

================
File: common/management/commands/copy_clients_db.py
================
from django.core.management.base import BaseCommand
from django.apps import apps
from django.db import connections
from apps.seo_manager.models import (
    Client,
    GoogleAnalyticsCredentials,
    SearchConsoleCredentials,
    TargetedKeyword,
    SEOData,
    ClientGroup,
    SEOProject,
)

class Command(BaseCommand):
    help = 'Copy Client and related data from source database to target database'

    def add_arguments(self, parser):
        parser.add_argument('--source', type=str, default='default', help='Source database')
        parser.add_argument('--target', type=str, required=True, help='Target database')

    def handle(self, *args, **options):
        source_db = options['source']
        target_db = options['target']

        # Check databases
        if source_db not in connections:
            self.stderr.write(self.style.ERROR(f"Source database '{source_db}' is not configured"))
            return
        if target_db not in connections:
            self.stderr.write(self.style.ERROR(f"Target database '{target_db}' is not configured"))
            return

        # Copy Client Groups
        self.copy_client_groups(source_db, target_db)

        # Copy Clients and related data
        self.copy_clients(source_db, target_db)

        self.stdout.write(self.style.SUCCESS('Successfully completed clients database copy operation'))

    def copy_client_groups(self, source_db, target_db):
        client_groups = list(ClientGroup.objects.using(source_db).all())
        if not client_groups:
            self.stdout.write(self.style.NOTICE("No ClientGroup objects to copy"))
            return

        for group in client_groups:
            group.pk = None  # Reset primary key to create a new instance
            group.save(using=target_db)  # Save to target database

        self.stdout.write(self.style.SUCCESS(f"Copied {len(client_groups)} ClientGroup objects"))

    def copy_clients(self, source_db, target_db):
        clients = list(Client.objects.using(source_db).all())
        if not clients:
            self.stdout.write(self.style.NOTICE("No Client objects to copy"))
            return

        for client in clients:
            # Copy Client
            client.pk = None  # Reset primary key to create a new instance
            client.save(using=target_db)  # Save to target database

            # Copy related data
            self.copy_related_data(client, source_db, target_db)

        self.stdout.write(self.style.SUCCESS(f"Copied {len(clients)} Client objects"))

    def copy_related_data(self, client, source_db, target_db):
        # Copy Google Analytics Credentials
        ga_credentials = GoogleAnalyticsCredentials.objects.using(source_db).filter(client=client).first()
        if ga_credentials:
            ga_credentials.pk = None  # Reset primary key
            ga_credentials.client = client  # Associate with new client
            ga_credentials.save(using=target_db)

        # Copy Search Console Credentials
        sc_credentials = SearchConsoleCredentials.objects.using(source_db).filter(client=client).first()
        if sc_credentials:
            sc_credentials.pk = None  # Reset primary key
            sc_credentials.client = client  # Associate with new client
            sc_credentials.save(using=target_db)

        # Copy Targeted Keywords
        targeted_keywords = TargetedKeyword.objects.using(source_db).filter(client=client)
        for keyword in targeted_keywords:
            keyword.pk = None  # Reset primary key
            keyword.client = client  # Associate with new client
            keyword.save(using=target_db)

        # Copy SEO Data
        seo_data = SEOData.objects.using(source_db).filter(client=client)
        for data in seo_data:
            data.pk = None  # Reset primary key
            data.client = client  # Associate with new client
            data.save(using=target_db)

        # Copy SEO Projects
        seo_projects = SEOProject.objects.using(source_db).filter(client=client)
        for project in seo_projects:
            project.pk = None  # Reset primary key
            project.client = client  # Associate with new client
            project.save(using=target_db)

        self.stdout.write(self.style.SUCCESS(f"Copied related data for Client: {client.name}"))

================
File: common/management/commands/copy_credentials_db.py
================


================
File: common/management/commands/copy_db.py
================
from django.core.management.base import BaseCommand
from django.apps import apps
from django.db import connections
from django.db.migrations.executor import MigrationExecutor

class Command(BaseCommand):
    help = 'Copy data from source database to target database'

    def add_arguments(self, parser):
        parser.add_argument('--source', type=str, default='default', help='Source database')
        parser.add_argument('--target', type=str, required=True, help='Target database')
        parser.add_argument('--skip-migrations', action='store_true', help='Skip running migrations on target database')

    def handle(self, *args, **options):
        source_db = options['source']
        target_db = options['target']
        skip_migrations = options['skip_migrations']

        # Check databases and run migrations (existing code...)
        if source_db not in connections:
            self.stderr.write(self.style.ERROR(f"Source database '{source_db}' is not configured"))
            return
        if target_db not in connections:
            self.stderr.write(self.style.ERROR(f"Target database '{target_db}' is not configured"))
            return

        if not skip_migrations:
            executor = MigrationExecutor(connections[target_db])
            plan = executor.migration_plan(executor.loader.graph.leaf_nodes())
            if plan:
                self.stdout.write(self.style.NOTICE('Target database needs migrations. Running migrations first...'))
                executor.migrate(targets=executor.loader.graph.leaf_nodes(), plan=plan)

        # First pass: Copy all models without M2M relationships
        all_models = apps.get_models()
        m2m_data = {}  # Store all M2M relationships

        for model in all_models:
            try:
                self.stdout.write(f"Copying {model.__name__}...")
                
                # Get all objects from source
                objects = list(model.objects.using(source_db).all())
                
                if not objects:
                    self.stdout.write(self.style.NOTICE(f"No {model.__name__} objects to copy"))
                    continue

                # Store M2M relationships
                m2m_data[model] = {}
                for obj in objects:
                    m2m_fields = [f for f in obj._meta.get_fields() if f.many_to_many and not f.auto_created]
                    if m2m_fields:
                        m2m_data[model][obj.pk] = {
                            field.name: list(getattr(obj, field.name).all().values_list('pk', flat=True))
                            for field in m2m_fields
                        }

                # Clear target
                try:
                    model.objects.using(target_db).all().delete()
                except Exception as e:
                    self.stderr.write(self.style.WARNING(f"Could not clear {model.__name__}: {str(e)}"))

                # Copy objects
                new_objects = model.objects.using(target_db).bulk_create(
                    objects,
                    batch_size=1000,
                    ignore_conflicts=True
                )

                self.stdout.write(self.style.SUCCESS(f"Copied {len(objects)} {model.__name__} objects"))

            except Exception as e:
                self.stderr.write(self.style.ERROR(f"Error copying {model.__name__}: {str(e)}"))
                continue

        # Second pass: Restore M2M relationships
        self.stdout.write(self.style.NOTICE("Restoring many-to-many relationships..."))
        
        for model, relationships in m2m_data.items():
            try:
                for obj_id, fields in relationships.items():
                    try:
                        obj = model.objects.using(target_db).get(pk=obj_id)
                        for field_name, related_ids in fields.items():
                            m2m_field = getattr(obj, field_name)
                            m2m_field.clear()
                            m2m_field.add(*related_ids)
                    except model.DoesNotExist:
                        self.stderr.write(self.style.WARNING(f"Could not find {model.__name__} with pk {obj_id}"))
                        continue
            except Exception as e:
                self.stderr.write(self.style.ERROR(f"Error restoring M2M relationships for {model.__name__}: {str(e)}"))
                continue

        self.stdout.write(self.style.SUCCESS('Successfully completed database copy operation'))

================
File: common/tools/async_crawl_website_tool.py
================
import asyncio
from typing import Optional, Type, List, Dict, Any
from pydantic import BaseModel, Field
from crewai_tools import BaseTool
from urllib.parse import urlparse
from trafilatura import extract
from celery import shared_task
from celery.contrib.abortable import AbortableTask
from django.contrib.auth.models import User
from django.conf import settings
import logging
import os
import re
from apps.crawl_website.models import CrawlResult
from spider_rs import Website, Page
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

class AsyncCrawlWebsiteToolSchema(BaseModel):
    """Input for AsyncCrawlWebsiteTool."""
    website_url: str = Field(..., description="Mandatory website URL to crawl and read content")

class AsyncCrawlWebsiteTool(BaseTool):
    name: str = "Async Crawl and Read Website Content"
    description: str = "An asynchronous tool that can crawl a website and read its content, including content from internal links on the same page."
    args_schema: Type[BaseModel] = AsyncCrawlWebsiteToolSchema
    website_url: Optional[str] = None

    def __init__(self, website_url: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        if website_url:
            self.website_url = website_url
            self.description = f"An asynchronous tool to crawl {website_url} and read its content, including content from internal links."

    async def _run(self, website_url: str = None) -> dict:
        url = website_url or self.website_url
        if not url:
            raise ValueError("No website URL provided")

        logger.info(f"Starting crawl for URL: {url}")
        
        try:
            result = await self.crawl_website(url)
            logger.info(f"Crawl completed. Total links: {result['total_links']}, Links visited: {len(result['links_visited'])}")
            return result
        except Exception as e:
            logger.error(f"Error during crawl: {e}")
            raise

    async def crawl_website(self, url: str) -> Dict[str, Any]:
        website = Website(url)
        website.with_budget({"*": 1000})  # Set a high limit for comprehensive crawling
        website.with_respect_robots_txt(True)
        website.with_subdomains(False)  # Stick to the main domain
        website.with_tld(False)  # Don't crawl top-level domain
        website.with_delay(1)  # Be respectful with a 1-second delay between requests

        pages_queue = asyncio.Queue()
        total_links = 0
        links_visited = set()
        content = ""
        crawl_complete = asyncio.Event()
        semaphore = asyncio.Semaphore(5)  # Limit concurrent processing

        def on_page_event(page: Page):
            nonlocal total_links
            total_links += 1
            logger.info(f"Received page: {page.url}")
            asyncio.run_coroutine_threadsafe(pages_queue.put_nowait(page), asyncio.get_event_loop())

        logger.info("Starting website crawl")
        with ThreadPoolExecutor() as executor:
            crawl_future = executor.submit(website.crawl, on_page_event=on_page_event)

        async def process_pages():
            nonlocal content
            while True:
                try:
                    page = await asyncio.wait_for(pages_queue.get(), timeout=5.0)
                    async with semaphore:
                        logger.info(f"Processing page: {page.url}")
                        page_content = await self._extract_content(page)
                        content += page_content
                        links_visited.add(self._get_relative_path(page.url))
                        pages_queue.task_done()
                        logger.info(f"Processed page: {page.url}, Content length: {len(page_content)}")
                except asyncio.TimeoutError:
                    if crawl_complete.is_set() and pages_queue.empty():
                        logger.info("No more pages to process")
                        break
                    logger.info("Waiting for more pages...")
                except Exception as e:
                    logger.error(f"Error processing page: {e}")

        async def monitor_crawl():
            while not crawl_future.done():
                await asyncio.sleep(1)
            crawl_complete.set()
            logger.info("Crawl completed")

        processing_task = asyncio.create_task(process_pages())
        monitoring_task = asyncio.create_task(monitor_crawl())

        await asyncio.gather(processing_task, monitoring_task)

        if not crawl_future.done():
            logger.warning("Crawl future not completed, waiting for completion")
            crawl_future.result()  # This will raise any exceptions that occurred during crawling

        logger.info(f"Crawl finished. Total links: {total_links}, Links visited: {len(links_visited)}")

        return {
            "content": content,
            "links_visited": list(links_visited),
            "total_links": total_links,
            "links_to_visit": list(set(page.url for page in website.get_pages()) - links_visited)
        }

    async def _extract_content(self, page: Page) -> str:
        try:
            html_content = page.content
            extracted_content = extract(html_content)
            logger.info(f"Extracted content length for {page.url}: {len(extracted_content) if extracted_content else 0}")
            return f"---link: {page.url}\n{extracted_content}\n---page-end---\n" if extracted_content else ""
        except Exception as e:
            logger.error(f"Error extracting content from {page.url}: {e}")
            return ""

    @staticmethod
    def _get_relative_path(url: str) -> str:
        parsed_url = urlparse(url)
        return parsed_url.path or '/'

def sanitize_url(url: str) -> str:
    """Sanitize the URL to create a valid folder name."""
    url = re.sub(r'^https?://(www\.)?', '', url)
    return re.sub(r'[^a-zA-Z0-9]', '_', url)

def save_crawl_result(user_id: int, website_url: str, content: str, links_visited: List[str], total_links: int, links_to_visit: List[str]) -> str:
    """Save the crawl result to a file in the user's directory."""
    sanitized_url = sanitize_url(website_url)
    user_dir = os.path.join(settings.MEDIA_ROOT, f'{user_id}', 'Crawled Websites')
    result_dir = os.path.join(user_dir, sanitized_url)
    os.makedirs(result_dir, exist_ok=True)

    file_path = os.path.join(result_dir, f'{sanitized_url}--content.txt')
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(f"Website URL: {website_url}\n\n")
        f.write(f"Links Visited:\n{', '.join(links_visited)}\n\n")
        f.write(f"Total Links: {total_links}\n\n")
        f.write("Content:\n")
        f.write(content)

    return file_path

@shared_task(bind=True, base=AbortableTask)
def crawl_website_task(self, website_url: str, user_id: int):
    logger.info(f"Starting crawl_website_task for URL: {website_url}")
    
    try:
        user = User.objects.get(id=user_id)
    except User.DoesNotExist:
        logger.error(f"User with id {user_id} not found")
        return None

    async_tool = AsyncCrawlWebsiteTool(website_url=website_url)
    
    async def run_crawl():
        return await async_tool._run(website_url)

    try:
        result = asyncio.run(run_crawl())
        logger.info(f"Crawl completed. Total links: {result['total_links']}, Links visited: {len(result['links_visited'])}")
    except Exception as e:
        logger.error(f"Error during crawl: {e}")
        return None

    file_path = save_crawl_result(
        user_id,
        website_url,
        result["content"],
        result["links_visited"],
        result["total_links"],
        result["links_to_visit"]
    )

    crawl_result = CrawlResult.objects.create(
        user=user,
        website_url=website_url,
        content=result["content"],
        links_visited=result["links_visited"],
        total_links=result["total_links"],
        links_to_visit=result["links_to_visit"],
        result_file_path=file_path
    )
    
    logger.info(f"Crawl task completed for URL: {website_url}")
    return crawl_result.id

================
File: common/tools/browser_tools.py
================
import json
import os

import requests
from langchain.tools import tool
from unstructured.partition.html import partition_html
from trafilatura import fetch_url, extract, sitemaps, spider
from summarizer import summarize

class BrowserTools():

  @tool("Scrape and summarize website content")
  def scrape_and_summarize_website(website):
    """Useful to scrape and summarize a website content, just pass a string with
    only the full url, no need for a final slash `/`, eg: https://google.com or https://clearbit.com/about-us"""
    url = f"https://browserless.neuralami.com/content?token={os.environ['BROWSERLESS_API_KEY']}"
    payload = json.dumps({"url": website})
    headers = {'cache-control': 'no-cache', 'content-type': 'application/json'}
    print(f"Browsin': {website}")
    response = requests.request("POST", url, headers=headers, data=payload)
    content = extract(response.text)
    summary = summarize(content)
    print(f"summary: '{summary}'")
    content = summary

    return f'\nSummary of {website}: {content}\n'

  @tool("Scrape website content")
  def scrape_website(website):
    """Useful to scrape website content, just pass a string with
    only the full url, no need for a final slash `/`, eg: https://google.com or https://clearbit.com/about-us"""
    url = f"https://browserless.neuralami.com/content?token={os.environ['BROWSERLESS_API_KEY']}"
    payload = json.dumps({"url": website})
    headers = {'cache-control': 'no-cache', 'content-type': 'application/json'}
    print(f"Browsin': {website}")
    response = requests.request("POST", url, headers=headers, data=payload)
    content = extract(response.text)
    return f'\nContent of {website}: {content}\n'

================
File: common/tools/crawl_website_search_tool.py
================
from typing import Optional, Type, Any, List

from embedchain.models.data_type import DataType
from pydantic.v1 import BaseModel, Field

from crewai_tools import RagTool

from tools.custom_browserless_loader import CustomBrowserlessLoader
from trafilatura import fetch_url, extract, sitemaps, spider

import os

class FixedCrawlWebsiteSearchToolSchema(BaseModel):
    """Input for CrawlWebsiteTool."""

    search_query: str = Field(
        ...,
        description = "Mandatory search query  you want to use to search a specific website and it's internal pages."
    )
    pass

class CrawlWebsiteSearchToolSchema(FixedCrawlWebsiteSearchToolSchema):
    """Input for CrawlWebsiteTool."""
    website: str = Field(
        ..., 
        description="Mandatory website url to crawl and read content")

class CrawlWebsiteSearchTool(RagTool):
    name: str = "Crawl and search website content"
    description: str = "A tool that can be used to crawl a website and read its content, including content from internal links on the same page."
    args_schema: Type[BaseModel] = CrawlWebsiteSearchToolSchema
    # website: Optional[str] = None
    # api_token: str = ""
    # base_url: str = ""

    def __init__(self, website: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        # self.api_token = os.environ.get("BROWSERLESS_API_KEY")
        # self.base_url = os.environ.get("BROWSERLESS_BASE_URL")
        if website is not None:
            self._crawl_website(website)
            self.description = f"A tool that can be used to crawl {website} and read its content, including content from internal links on the same page."
            self.args_schema = FixedCrawlWebsiteSearchToolSchema

    def add(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        kwargs["data_type"] = DataType.WEB_PAGE
        super().add(*args, **kwargs)

    def _crawl_website(self, url: str) -> str:
        links_to_visit = self._get_links_to_visit(url)
        print(f"Reading {len(links_to_visit)} pages.")
        for link in links_to_visit:
            try:
                self.add(link) # Assuming this is a method that adds the link to some data structure
            except Exception as e:
                print(f"Failed to read page at '{link}'. Error: {e}")

        
    def _get_links_to_visit(self, url: str) -> List[str]:
        sitemap_links = sitemaps.sitemap_search(url)
        # if sitemap_links:
        if 0:
            print(f"Found {len(sitemap_links)} pages from sitemap.")
            return sitemap_links
        else:
            _, known_urls = spider.focused_crawler(url, max_seen_urls=10, max_known_urls=50)
            print(f"Found {len(known_urls)} from crawling the website.")
            return list(known_urls)

    def _before_run(
        self,
        query: str,
        **kwargs: Any,
    ) -> Any:
        if "website" in kwargs:
            self.add(kwargs["website"])

    def _run(
        self,
        search_query: str,
        **kwargs: Any,
    )-> Any:
        return super()._run(query=search_query)

================
File: common/tools/crawl_website_tool.py
================
import requests
from bs4 import BeautifulSoup
from typing import Optional, Type, Any, List
from pydantic.v1 import BaseModel, Field
from crewai_tools import BaseTool
from urllib.parse import urljoin
from trafilatura import fetch_url, extract, sitemaps, spider

class FixedCrawlWebsiteToolSchema(BaseModel):
    """Input for CrawlWebsiteTool."""
    pass

class CrawlWebsiteToolSchema(FixedCrawlWebsiteToolSchema):
    """Input for CrawlWebsiteTool."""
    website_url: str = Field(..., description="Mandatory website url to crawl and read content")

class CrawlWebsiteTool(BaseTool):
    name: str = "Crawl and read website content"
    description: str = "A tool that can be used to crawl a website and read its content, including content from internal links on the same page."
    args_schema: Type[BaseModel] = CrawlWebsiteToolSchema
    website_url: Optional[str] = None
    headers: Optional[dict] = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://www.google.com/'
    }

    def __init__(self, website_url: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        if website_url is not None:
            self.website_url = website_url
            self.description = f"A tool that can be used to crawl {website_url} and read its content, including content from internal links on the same page."
            self.args_schema = FixedCrawlWebsiteToolSchema

    def _run(self, **kwargs: Any) -> str:
        website_url = kwargs.get('website_url', self.website_url)
        print(f"Processing {website_url}")
        content = self._crawl_website(website_url)
        return content

    def _crawl_website(self, url: str) -> str:
        links_to_visit = self._get_links_to_visit(url)
        content = ""
        print(f"Reading {len(links_to_visit)} pages.")
        for link in links_to_visit:
            page_content = self._fetch_and_extract_content(link)
            content += page_content
        
        return content

    def _get_links_to_visit(self, url: str) -> List[str]:
#        sitemap_links = sitemaps.sitemap_search(url)
        sitemap_links = []
        if sitemap_links:
            print(f"Found {len(sitemap_links)} pages from sitemap.")
            return sitemap_links
        else:
            _, known_urls = spider.focused_crawler(url, max_seen_urls=10, max_known_urls=1000)
            print(f"Found {len(known_urls)} from crawling the website.")
            return list(known_urls)

    def _fetch_and_extract_content(self, url: str) -> str:
        html_content = fetch_url(url)
        if html_content:
            extracted_content = f"---link: {url}\n{extract(html_content,url=url)}\n---page-end---\n"
            return extracted_content or ""
        else:
            return ""

================
File: common/tools/ExaSearchTool.py
================
import os
from exa_py import Exa
from langchain.agents import tool

class ExaSearchTool:
	@tool
	def search(query: str):
		"""Search for a webpage based on the query."""
		return ExaSearchTool._exa().search(f"{query}", use_autoprompt=True, num_results=3)

	@tool
	def find_similar(url: str):
		"""Search for webpages similar to a given URL.
		The url passed in should be a URL returned from `search`.
		"""
		return ExaSearchTool._exa().find_similar(url, num_results=3)

	@tool
	def get_contents(ids: str):
		"""Get the contents of a webpage.
		The ids must be passed in as a list, a list of ids returned from `search`.
		"""
		ids = eval(ids)
		contents = str(ExaSearchTool._exa().get_contents(ids))
		print(contents)
		contents = contents.split("URL:")
		contents = [content[:1000] for content in contents]
		return "\n\n".join(contents)

	def tools():
		return [ExaSearchTool.search, ExaSearchTool.find_similar, ExaSearchTool.get_contents]

	def _exa():
		return Exa(api_key=os.environ["EXA_API_KEY"])

================
File: common/tools/google_suggestions_tool.py
================
from crewai_tools import tool
import requests
import xml.etree.ElementTree as ET

@tool("Google Suggestions")
def google_suggestions_tool(argument: str) -> str:
    """Retrieve Google search suggestions for a given keyword."""
    # Parse the argument to extract the keyword and other parameters
    keyword = argument.split(",")[0].strip()
    country_code = argument.split(",")[1].strip() if "," in argument else "us"

    # Build the Google Search query URL
    search_query = f"is {keyword}"
    google_search_url = f"http://google.com/complete/search?output=toolbar&gl={country_code}&q={search_query}"

    # Call the URL and read the data
    result = requests.get(google_search_url)
    tree = ET.ElementTree(ET.fromstring(result.content))
    root = tree.getroot()

    # Extract the suggestions from the XML response
    suggestions = []
    for suggestion in root.findall('CompleteSuggestion'):
        question = suggestion.find('suggestion').attrib.get('data')
        suggestions.append(question)

    # Return the suggestions as a comma-separated string
    return ", ".join(suggestions)

================
File: common/tools/keyword_tools.py
================
import os
import requests
from typing import Any, Type, List, Dict, Tuple
from pydantic.v1 import BaseModel, Field
from crewai_tools.tools.base_tool import BaseTool


class KeywordsInput(BaseModel):
    keywords: List[str] = Field(description="list of keywords")
    filters: List[Tuple[str, str, float]] = Field(description="list of filters")


class KeywordsForSiteTool(BaseTool):
    name: str = "Keywords for Site"
    description: str = "Provides a list of keywords relevant to the target domain. Each keyword is supplied with relevant categories, search volume data for the last month, cost-per-click, competition, and search volume trend values for the past 12 months"
    args_schema: Type[BaseModel] = BaseModel

    def _run(self, target: str, **kwargs: Any) -> Any:
        login, password = KeywordTools._dataforseo_credentials()
        cred = (login, password)
        url = "https://api.dataforseo.com/v3/keywords_data/google_ads/keywords_for_site/live"
        payload = [
            {
                "target": target,
                "language_code": "en",
                "location_code": 2840,
            }
        ]
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, auth=cred)
        response.raise_for_status()  # Raise an exception for non-2xx status codes
        results = response.json()
        return results["tasks"][0]["result"]

    async def _arun(self, target: str, **kwargs: Any) -> Any:
        raise NotImplementedError("KeywordsForSiteTool does not support async")


class KeywordSuggestionsTool(BaseTool):
    name: str = "Keyword Suggestions"
    description: str = "Provides a list of keywords relevant to the target domain. Each keyword is supplied with relevant categories, search volume data for the last month, cost-per-click, competition, and search volume trend values for the past 12 months"
    args_schema: Type[BaseModel] = BaseModel

    def _run(self, seed_keyword: str, filters: List = None, **kwargs: Any) -> Any:
        login, password = KeywordTools._dataforseo_credentials()
        cred = (login, password)
        url = "https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live"
        payload = [
            {
                "keyword": seed_keyword,
                "location_code": 2840,
                "language_code": "en",
                "include_serp_info": True,
                "include_seed_keyword": True,
                "limit": 50,
            }
        ]
        if filters:
            payload[0]["filters"] = filters
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, auth=cred)
        response.raise_for_status()  # Raise an exception for non-2xx status codes
        results = response.json()
        return results

    async def _arun(self, seed_keyword: str, filters: List = None, **kwargs: Any) -> Any:
        raise NotImplementedError("KeywordSuggestionsTool does not support async")


class KeywordIdeasTool(BaseTool):
    name: str = "Keyword Ideas"
    description: str = "Provides search terms that are relevant to the product or service categories of the specified keywords. The algorithm selects the keywords which fall into the same categories as the seed keywords specified"
    args_schema: Type[BaseModel] = KeywordsInput

    def _run(self, keywords: List[str], filters: List[str] = None, **kwargs: Any) -> Any:
        login, password = KeywordTools._dataforseo_credentials()
        cred = (login, password)
        url = "https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live"
        payload = [
            {
                "keywords": keywords,
                "location_code": 2840,
                "language_code": "en",
                "include_serp_info": True,
                "limit": 100,
            }
        ]
        payload[0]["order_by"] = "keyword_info.search_volume,desc"
        if filters:
            payload[0]["filters"] = filters
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, auth=cred)
        response.raise_for_status()  # Raise an exception for non-2xx status codes
        results = response.json()
        return results

    async def _arun(self, keywords: List[str], filters: List[str] = None, **kwargs: Any) -> Any:
        raise NotImplementedError("KeywordIdeasTool does not support async")


class KeywordTools:
    @staticmethod
    def tools():
        return [KeywordsForSiteTool(), KeywordSuggestionsTool(), KeywordIdeasTool()]

    @staticmethod
    def _dataforseo_credentials():
        login = os.environ["DATAFORSEO_LOGIN"]
        password = os.environ["DATAFORSEO_PASSWORD"]
        return login, password

================
File: common/tools/keywords_for_site_tool.py
================
import os
import requests
from crewai import Agent
from langchain.tools import tool

@tool("DataForSEO Keywords for Site")
def keywords_for_site_tool(target: str) -> str:
    """
    Retrieves a list of SEO keywords relevant to the specified site using the DataForSEO API.
    """
    login = os.environ["DATAFORSEO_LOGIN"]
    password = os.environ["DATAFORSEO_PASSWORD"]
    cred = (login, password)
    url = "https://api.dataforseo.com/v3/dataforseo_labs/google/keywords_for_site/live"

    payload = [
        {
            "target": target,
            "language_code": "en",
            "location_code": 2840,
        }
    ]

    headers = {"Content-Type": "application/json"}

    response = requests.post(url, json=payload, headers=headers, auth=cred)
    results = response.json()

    # Process the results and return the desired output
    # ...

    return results

================
File: common/tools/rag_tool.py
================
from abc import ABC, abstractmethod
from typing import Any

from pydantic import BaseModel, Field, model_validator

from crewai_tools.tools.base_tool import BaseTool


class Adapter(BaseModel, ABC):
    class Config:
        arbitrary_types_allowed = True

    @abstractmethod
    def query(self, question: str) -> str:
        """Query the knowledge base with a question and return the answer."""

    @abstractmethod
    def add(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        """Add content to the knowledge base."""


class RagTool(BaseTool):
    class _AdapterPlaceholder(Adapter):
        def query(self, question: str) -> str:
            raise NotImplementedError

        def add(self, *args: Any, **kwargs: Any) -> None:
            raise NotImplementedError

    name: str = "Knowledge base"
    description: str = "A knowledge base that can be used to answer questions."
    summarize: bool = False
    adapter: Adapter = Field(default_factory=_AdapterPlaceholder)
    config: dict[str, Any] | None = None

    @model_validator(mode="after")
    def _set_default_adapter(self):
        if isinstance(self.adapter, RagTool._AdapterPlaceholder):
            from embedchain import App

            from crewai_tools.adapters.embedchain_adapter import EmbedchainAdapter

            app = App.from_config(config=self.config) if self.config else App()
            self.adapter = EmbedchainAdapter(
                embedchain_app=app, summarize=self.summarize
            )

        return self

    def add(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        self.adapter.add(*args, **kwargs)

    def _run(
        self,
        query: str,
        **kwargs: Any,
    ) -> Any:
        self._before_run(query, **kwargs)

        return f"Relevant Content:\n{self.adapter.query(query)}"

    def _before_run(self, query, **kwargs):
        pass

================
File: common/tools/report_generator.py
================
import json
from typing import Dict, Any
from datetime import datetime

class ReportGenerator:
    @staticmethod
    def generate_seo_report(audit_results: Dict[str, Any], recommendations: str = None) -> str:
        """Generate a comprehensive SEO audit report."""
        report = []
        report.append("=== SEO Audit Report ===")
        report.append(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # Crawl Statistics
        if 'crawl_stats' in audit_results:
            report.append("=== Crawl Statistics ===")
            stats = audit_results['crawl_stats']
            report.append(f"Total Pages Crawled: {stats.get('total_pages', 0)}")
            report.append(f"Total Links Found: {stats.get('total_links', 0)}")
            report.append(f"Crawl Time: {stats.get('crawl_time', 0):.2f} seconds\n")

        # Basic Requirements
        report.append("=== Basic Requirements ===")
        report.append(f"SSL Certificate: {' Valid' if audit_results.get('ssl_issues', {}).get('valid_certificate', False) else ' Invalid'}")
        report.append(f"Sitemap: {' Present' if audit_results.get('sitemap_present', False) else ' Missing'}")
        report.append(f"Robots.txt: {' Present' if audit_results.get('robots_txt_present', False) else ' Missing'}\n")

        # Broken Links
        broken_links = audit_results.get('broken_links', [])
        if broken_links:
            report.append("=== Broken Links ===")
            for link in broken_links:
                status = f"(Status: {link['status_code']})" if link.get('status_code') else "(Connection Failed)"
                report.append(f"Source: {link['source_page']}")
                report.append(f"Broken Link: {link['broken_link']} {status}\n")

        # Duplicate Content
        duplicate_content = audit_results.get('duplicate_content', [])
        if duplicate_content:
            report.append("=== Duplicate Content Issues ===")
            for dup in duplicate_content:
                report.append(f"Page: {dup['page_url']}")
                report.append(f"Issue Type: {dup['issue_type']}")
                if dup.get('duplicate_with'):
                    report.append(f"Duplicate With: {dup['duplicate_with']}\n")

        # Meta Tag Issues
        meta_issues = audit_results.get('meta_tag_issues', [])
        if meta_issues:
            report.append("=== Meta Tag Issues ===")
            for issue in meta_issues:
                report.append(f"Page: {issue['page_url']}")
                if issue.get('missing_meta'):
                    report.append(f"Missing Meta Tags: {', '.join(issue['missing_meta'])}")
                if issue.get('duplicate_meta'):
                    report.append(f"Duplicate Meta Tags: {', '.join(issue['duplicate_meta'])}")
                if issue.get('meta_length_issues'):
                    for tag, length in issue['meta_length_issues'].items():
                        report.append(f"{tag.title()} Length: {length} characters")
                report.append("")

        # Page Speed Issues
        if 'page_speed_issues' in audit_results and audit_results['page_speed_issues']:
            report.append("=== Page Speed Issues ===")
            for page, issues in audit_results['page_speed_issues'].items():
                report.append(f"Page: {page}")
                for issue, details in issues.items():
                    report.append(f"{issue}: {details}")
                report.append("")

        # Mobile Friendliness
        report.append("=== Mobile Friendliness ===")
        report.append(f"Mobile Friendly: {' Yes' if audit_results.get('mobile_friendly', True) else ' No'}\n")

        # LLM Recommendations
        if recommendations:
            report.append("=== AI-Generated Recommendations ===")
            report.append(recommendations)

        return "\n".join(report)

    @staticmethod
    def generate_json_report(audit_results: Dict[str, Any], recommendations: str = None) -> str:
        """Generate a JSON format report."""
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "audit_results": audit_results,
            "recommendations": recommendations
        }
        return json.dumps(report_data, indent=2)

================
File: common/tools/screenshot_tool.py
================
import os
import requests
import json
from typing import Any, Type
from pydantic import BaseModel, Field
from crewai_tools.tools.base_tool import BaseTool
from django.conf import settings
from urllib.parse import urlparse
import re
"""
You can use the ScreenshotTool by 
 1. importing 'from apps.common.tools.screenshot_tool import screenshot_tool'' and 
 2. calling its run method with a URL as the argument: 'result = screenshot_tool.run(url=url)'
 """

class ScreenshotToolSchema(BaseModel):
    """Input schema for ScreenshotTool."""
    url: str = Field(..., description="The URL of the website to capture a screenshot.")

class ScreenshotTool(BaseTool):
    name: str = "Capture Website Screenshot"
    description: str = "Captures a screenshot of a given website URL."
    args_schema: Type[BaseModel] = ScreenshotToolSchema
    
    def _run(
        self, 
        url: str, 
        **kwargs: Any
    ) -> Any:
        browserless_url = os.getenv('BROWSERLESS_BASE_URL')
        api_key = os.getenv('BROWSERLESS_API_KEY')
        
        if not browserless_url or not api_key:
            return {'error': 'Browserless configuration is missing'}
        
        screenshot_url = f"{browserless_url}/screenshot?token={api_key}"
        
        payload = {
            "url": url,
            "options": {
                "fullPage": False,
                "type": "png"
            }
        }
        
        response = requests.post(screenshot_url, json=payload)
        
        if response.status_code == 200:
            # Generate a sanitized filename based on the URL
            parsed_url = urlparse(url)
            sanitized_name = re.sub(r'[^\w\-_\. ]', '_', parsed_url.netloc + parsed_url.path)
            filename = f"{sanitized_name[:200]}.png"  # Limit filename length
            filepath = os.path.join(settings.MEDIA_ROOT, 'crawled_screenshots', filename)
            # Ensure the directory exists
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            # Save the image
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            # Generate the URL for the saved image
            image_url = f"{settings.MEDIA_URL}crawled_screenshots/{filename}"
            
            return {'screenshot_url': image_url}
        else:
            return {'error': f'Failed to get screenshot. Status code: {response.status_code}'}

# Initialize the tool
screenshot_tool = ScreenshotTool()

================
File: common/tools/searxng_tool.py
================
import os
import requests
import json
from typing import Any, Type
from pydantic.v1 import BaseModel, Field
from crewai_tools.tools.base_tool import BaseTool

class SearxNGToolSchema(BaseModel):
    """Input schema for SearxNGSearchTool."""
    search_query: str = Field(..., description="The search query to be used.")

class SearxNGSearchTool(BaseTool):
    name: str = "Search the internet"
    description: str = "Searches the internet displaying titles, links, snippets, engines, and categories."
    args_schema: Type[BaseModel] = SearxNGToolSchema
    search_url: str = "https://search.neuralami.com"
    n_results: int = None
    
    def _run(
		self, 
		search_query: str, 
		**kwargs: Any
		) -> Any:
        payload = {        
            'q': search_query,
            'format': 'json',
            'pageno': '1',
            'language': 'en-US'
        }
        response = requests.get(self.search_url, params=payload)
        if response.ok:
            results = response.json()['results']
            formatted_results = []
            for result in results:
                try:
                    engines = ', '.join(result['engines']) if 'engines' in result else 'N/A'
                    formatted_results.append('\n'.join([
                            f"Title: {result.get('title', 'No Title')}",
                            f"Link: {result.get('url', 'No Link')}",
                            f"Score: {result.get('score', 'No Score')}",
                            f"Snippet: {result.get('content', 'No Snippet')}",
                            f"Engines: {engines}",
                            f"Category: {result.get('category', 'No Category')}",
                            "---"
                    ]))
                except KeyError as e:
                    print(f"Skipping an entry due to missing key: {e}")
                    continue

            content = '\n'.join(formatted_results)
            return f"Search results:\n{content}"
        else:
            return f"Failed to fetch search results. Status code: {response.status_code}"

================
File: common/tools/summarizer.py
================
from langchain.chat_models import ChatOllama
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.output_parsers import StrOutputParser

def summarize(query, base_url="http://192.168.30.100:11434"):
    """
    Generate a response to a user's query using the ChatOllama model.

    Args:
        query (str): The user's query.
        base_url (str, optional): The base URL for the API endpoint. Defaults to "https://api.example.com/v1/chat".

    Returns:
        str: The response to the user's query, formatted in Markdown.
    """
    
    # Initialize the ChatOllama model with the base_url parameter
    llm = ChatOllama(model="mistral", base_url=base_url)

    # Define a chat prompt template
    prompt = ChatPromptTemplate.from_messages([
    ("system", "Extract all relevant facts from the text."),
    ("human", "{query}"),
    ])

    # prompt = ChatPromptTemplate(
    #     messages=[
    #         SystemMessagePromptTemplate(
    #             prompt=("You are a helpful AI assistant for summarization. "
    #                     "Provide a concise summary of the users input, just the summary, no extra commentary.")
    #         ),
    #         HumanMessagePromptTemplate(input_variables=["query"]),
    #     ]
    # )
    chain = prompt | llm | StrOutputParser()
    # Define an output parser to handle Markdown responses
    # Generate a response to the user's query
    response = chain.invoke({'query':query})
    result = response

    return result

================
File: common/tools/user_activity_tool.py
================
from typing import Any, Optional, Type 
from typing import Any, Optional
from pydantic import BaseModel, Field
from crewai_tools.tools.base_tool import BaseTool
from apps.seo_manager.models import UserActivity
import logging

logger = logging.getLogger(__name__)

"""
You can use the UserActivityTool by 
 1. importing 'from apps.common.tools.user_activity_tool import user_activity_tool' and 
 2. calling its run method with the required arguments: 'result = user_activity_tool.run(user=user, category=category, action=action, client=client, details=details)'
"""

class UserActivityToolSchema(BaseModel):
    """Input schema for UserActivityTool."""
    user: Any = Field(..., description="The user performing the action.")
    category: str = Field(..., description="The category of the action.")
    action: str = Field(..., description="The action performed.")
    client: Optional[Any] = Field(None, description="The client associated with the action (optional).")
    details: Optional[str] = Field(None, description="Additional details about the action (optional).")

class UserActivityTool(BaseTool):
    name: str = "Log User Activity"
    description: str = "Logs user activity in the system."
    args_schema: Type[BaseModel] = UserActivityToolSchema
    
    def _run(
        self, 
        user: Any,
        category: str,
        action: str,
        client: Optional[Any] = None,
        details: Optional[str] = None,
        **kwargs: Any
    ) -> Any:
        try:
            UserActivity.objects.create(
                user=user,
                client=client,
                category=category,
                action=action,
                details=details
            )
            logger.debug(f"User activity logged: {user.username} - {category} - {action} - {client} - {details}")
            return {"success": True, "message": "User activity logged successfully."}
        except Exception as e:
            return {"success": False, "error": str(e)}

# Initialize the tool
user_activity_tool = UserActivityTool()

================
File: common/tools/website_search_tool.py
================
from typing import Any, Optional, Type

from embedchain.models.data_type import DataType
from pydantic.v1 import BaseModel, Field

from ..rag.rag_tool import RagTool


class FixedWebsiteSearchToolSchema(BaseModel):
    """Input for WebsiteSearchTool."""

    search_query: str = Field(
        ...,
        description="Mandatory search query you want to use to search a specific website",
    )


class WebsiteSearchToolSchema(FixedWebsiteSearchToolSchema):
    """Input for WebsiteSearchTool."""

    website: str = Field(
        ..., description="Mandatory valid website URL you want to search on"
    )


class WebsiteSearchTool(RagTool):
    name: str = "Search in a specific website"
    description: str = "A tool that can be used to semantic search a query from a specific URL content."
    args_schema: Type[BaseModel] = WebsiteSearchToolSchema

    def __init__(self, website: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        if website is not None:
            self.add(website)
            self.description = f"A tool that can be used to semantic search a query from {website} website content."
            self.args_schema = FixedWebsiteSearchToolSchema

    def add(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        kwargs["data_type"] = DataType.WEB_PAGE
        super().add(*args, **kwargs)

    def _before_run(
        self,
        query: str,
        **kwargs: Any,
    ) -> Any:
        if "website" in kwargs:
            self.add(kwargs["website"])

================
File: common/__init__.py
================
# Package initialization

================
File: common/admin.py
================
from django.contrib import admin

# Register your models here.

================
File: common/apps.py
================
from django.apps import AppConfig


class CommonConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'apps.common'

================
File: common/browser_tool.py
================
import json
import os

import requests
from langchain.tools import tool
from unstructured.partition.html import partition_html
from trafilatura import fetch_url, extract, sitemaps, spider

class BrowserTools():

  @tool("Scrape website content")
  def scrape_website(website):
    """Useful to scrape website content, just pass a string with
    only the full url, no need for a final slash `/`, eg: https://google.com or https://clearbit.com/about-us"""
    url = f"https://browserless.rijsinghani.us/content?token={os.environ['BROWSERLESS_API_KEY']}"
    payload = json.dumps({"url": website})
    headers = {'cache-control': 'no-cache', 'content-type': 'application/json'}
    print(f"Browsin': {website}")
    response = requests.request("POST", url, headers=headers, data=payload)
    content = extract(response.text)
    return f'\nContent of {website}: {content}\n'

================
File: common/chat_model_handler.py
================
from apps.common.utils import get_llm

from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import AIMessage, HumanMessage, SystemMessage

class ChatModelHandler:
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.chat_model, _ = get_llm(model=model_name, temperature=0.05)
        self.summarize_prompt = self._create_summarize_prompt()

    def _create_summarize_prompt(self) -> ChatPromptTemplate:
        """ Create a prompt template for summarization """
        system_message_prompt = SystemMessagePromptTemplate.from_template(
            "You are an AI assistant that summarizes text while preserving key details, using markdown formatting."
        )
        human_message_prompt = HumanMessagePromptTemplate.from_template("{text}")
        return ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

    def _generate_messages(self, query: str) -> list:
        """ Generate messages for the chat model based on the query """
        human_message = HumanMessage(content=self.summarize_prompt.format_prompt(text=query).to_messages()[1].content)
        return [human_message]

    def generate_response(self, query: str) -> str:
        """ Generate a response from the chat model based on the query """
        messages = self._generate_messages(query)
        response = self.chat_model(messages)
        return response.content

================
File: common/compression_manager.py
================
from .utils import tokenize, get_llm,  TokenCounterCallback
from langchain.text_splitter import TokenTextSplitter
from langchain.prompts.chat import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import tiktoken
from django.conf import settings
import logging
from langchain.callbacks.manager import CallbackManager
import os
import errno

class CompressionManager:
    def __init__(self, model_name:str, task_instance = None):
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.model_name = model_name
        self.task_instance = task_instance
        self.llm, self.token_counter_callback = get_llm(model_name, temperature=0.0)

 
    
    def compress_content(self, content: str, max_tokens: int) -> str:
        content_tokens = tokenize(content, self.tokenizer)
        #logging.info(f"Compressing content: {len(content)} chars and {content_tokens} tokens")
        if content_tokens < 20:
            return content,0,0
        else:        
            logging.info(f"Compressing content: {len(content)} chars and {content_tokens} tokens and {max_tokens} max tokens.")
            return self._compress_iteratively(content, max_tokens)

    def _compress_iteratively(self, content: str, max_tokens: int) -> str:
        """ Compress content iteratively until it fits within max_tokens """
        chunk_size = int(max_tokens) // 3 # max_tokens #* 3 # 3x the max_tokens -> if split with chars
        overlap = 25
        text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
        logging.info(f"Chunk size: {chunk_size} tokens")
        compress_prompt = ChatPromptTemplate.from_messages([
            ("human", """
- Carefully read through the text and take detailed notes, do not lose any information, but remove content nonrelated to the main topic like promotions, advertisements, calls to action, etc.
- Focus on including every detail 
- long form outline format
- No preambles, post ambles, summaries, just the notes.
- Write at a 16 year old level.
\n\n```{content}```\n\n""")
        ])

        compress_chain = compress_prompt| self.llm | StrOutputParser()

        compressed_chunks = []
        chunks = text_splitter.split_text(content)
        num_chunks = len(chunks)
        iteration = 1

        last_iteration_size = tokenize(content,self.tokenizer)
        path=f'{settings.DOWNLOAD_FOLDER}/summarizer/compress_content-{str(iteration)}'
        if not os.path.exists(path):
            try:
                os.makedirs(os.path.dirname(path))
                logging.info("Created directory for compressed content")
            except FileExistsError:
                pass
        while True:
        #if True: # temporary to only do 1 pass
            #logging.info(f"Compression iteration {iteration} with {len(chunks)} chunks...")

            self.task_instance.update_state(
                state=f'reading content...',
                meta={'current_chunk': 0, 'total_chunks': num_chunks}
            )
            current_chunk = 0
            for chunk in chunks:
                #logging.info(f"Compressing chunk of length {tokenize(chunk,self.tokenizer)} tokens...")
                current_chunk += 1
                compressed_chunk = compress_chain.invoke({'content':chunk})
                #logging.info(f"Compressed chunk: {current_chunk} of {num_chunks} chunks of length {tokenize(compressed_chunk, self.tokenizer)} tokens...")
                self.task_instance.update_state(
                    state='processing',
                    meta={'current_chunk': current_chunk, 'total_chunks': num_chunks}
                )
                with open(f'{settings.DOWNLOAD_FOLDER}/summarizer/chunk-{current_chunk}-{iteration}','w') as f:
                    f.write(chunk)
                with open(f'{settings.DOWNLOAD_FOLDER}/summarizer/compressed-chunk-{current_chunk}-{iteration}','w') as f:
                    f.write(compressed_chunk)
                    
                

                compressed_chunks.append(compressed_chunk)

            compressed_content = "\n".join(compressed_chunks)
            token_count = tokenize(compressed_content, self.tokenizer)
            with open(f'{settings.DOWNLOAD_FOLDER}/summarizer/compress_content-{iteration}','w') as f:
                f.write(compressed_content)
                
            if token_count <= max_tokens or token_count > .75*last_iteration_size:
                break
            else:
                chunks = text_splitter.split_text(compressed_content)
                compressed_chunks = []
                num_chunks= len(chunks)
                iteration += 1
                last_iteration_size = token_count

        input_tokens = self.token_counter_callback.input_tokens
        output_tokens = self.token_counter_callback.output_tokens
        logging.info(f"Compression Input tokens: {input_tokens}, output tokens: {output_tokens}")
        return compressed_content, input_tokens, output_tokens

================
File: common/content_loader.py
================
from .utils import is_pdf_url, is_youtube, is_stock_symbol
from .browser_tool import BrowserTools
from langchain_community.document_loaders import YoutubeLoader, PyMuPDFLoader
import logging
from sec_edgar_downloader import Downloader
import os
from bs4 import BeautifulSoup
from django.conf import settings

logger = logging.getLogger(__name__)

class ContentLoader:
    def __init__(self):
        self.browser_tool = BrowserTools()

    def load_content(self, query: str) -> str:
        """ Load and return content from a URL """
        logging.info("Loading content")
        if len(query) > 500:
            logger.info("Content too long to be anything but text")
            return query 
        if query.startswith("http"):
            url = query
            if is_youtube(url):
                logger.info(f"Loading content from YouTube: {url}")
                return self._load_from_youtube(url)
            elif is_pdf_url(url):
                logger.info(f"Loading content from PDF: {url}")
                return self._load_from_pdf(url)
            else:
                logger.info(f"Loading content from website: {url}")
                return self.browser_tool.scrape_website(url)
        elif is_stock_symbol(query):
                logger.info(f"Loading content from SEC EDGAR: {query}")
                return self._load_from_sec(query)
        else:
            logger.info("Loading as text")
            return query

    def _load_from_youtube(self, url: str) -> str:
        loader = YoutubeLoader.from_youtube_url(url)
        docs = loader.load()
        page_content = "".join(doc.page_content for doc in docs)
        metadata = docs[0].metadata
        # Create output string with metadata and page_content
        output = f"Title: {metadata.get('title')}\n\n"
        output += f"Description: {metadata.get('description')}\n\n"
        output += f"View Count: {metadata.get('view_count')}\n\n"
        output += f"Author: {metadata.get('author')}\n\n"
        output += f"Category: {metadata.get('category')}\n\n"
        output += f"Source: {metadata.get('source')}\n\n"
        output += f"Page Content:\n{page_content}"
        return output

    def _load_from_pdf(self, url: str) -> str:
        # Simulated PDF content loading method
        loader = PyMuPDFLoader(url)
        docs = loader.load()
        return "".join(doc.page_content for doc in docs)

    def _load_from_sec(self, query: str) -> str:
        """ Load and return content from SEC EDGAR """
        # Provide a company name and email address to comply with SEC EDGAR's fair access policy
        company_name = settings.COMPANY_NAME
        email_address = settings.EMAIL_ADDRESS

        # Create a Downloader instance with the specified download folder
        download_folder = settings.DOWNLOAD_FOLDER + "/sec-edgar-files"
        
        if not os.path.exists (download_folder):
            try:
                 os.makedirs (download_folder)
            except FileExistsError:
                pass
                  
        
        dl = Downloader(company_name, email_address, download_folder)

        num_filings_downloaded = dl.get("10-K", query, limit=1, download_details=True)
        logging.info(f"Downloaded {num_filings_downloaded} 10-K filing(s) for {query}.")

        print(f"Downloaded {num_filings_downloaded} 10-K filing(s) for {query}.")

        # Access the downloaded HTML filing
        if num_filings_downloaded > 0:
            logging.info("getting filings dir")
            filings_dir = os.path.join(download_folder, "sec-edgar-filings", query, "10-K")
            filing_subdirs = os.listdir(filings_dir)
            # latest_filing_subdir = sorted(filing_subdirs)[-1]
            # latest_filing_path = os.path.join(filings_dir, latest_filing_subdir, "primary-document.html")
            logging.info("getting latest_filings_subdir")
            latest_filing_subdir = sorted(filing_subdirs)[-1]
            # Convert set to string if necessary
            if not isinstance(latest_filing_subdir, str):
                latest_filing_subdir = str(latest_filing_subdir)
            logging.info("getting latest_filing_path")
            latest_filing_path = os.path.join(filings_dir, latest_filing_subdir, "primary-document.html")

            
            
            with open(latest_filing_path, "r") as f:
                html_content = f.read()
            
            # Parse the HTML content using BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extract the text content
            text_content = soup.get_text()
            
            return text_content

================
File: common/models.py
================
from django.db import models

# Create your models here.
        
# Don't remove this mark
### ### Below code is Generated ### ###

from django.db import models

class RefundedChoices(models.TextChoices):
	YES = 'YES', 'Yes'
	NO = 'NO', 'No'

class CurrencyChoices(models.TextChoices):
	USD = 'USD', 'USD'
	EUR = 'EUR', 'EUR'
	
class Sales(models.Model):
	ID = models.AutoField(primary_key=True)
	Product = models.TextField(blank=True, null=True)
	BuyerEmail = models.EmailField(blank=True, null=True)
	PurchaseDate = models.DateField(blank=True, null=True)
	Country = models.TextField(blank=True, null=True)
	Price = models.FloatField(blank=True, null=True)
	Refunded = models.CharField(max_length=20, choices=RefundedChoices.choices, default=RefundedChoices.NO)
	Currency = models.CharField(max_length=10, choices=CurrencyChoices.choices, default=CurrencyChoices.USD)
	Quantity = models.IntegerField(blank=True, null=True)

================
File: common/summarization_manager.py
================
from apps.common.utils import get_llm as utils_get_llm  # Rename the import
from .utils import tokenize
from langchain.text_splitter import TokenTextSplitter
from langchain.prompts.chat import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import tiktoken
from django.conf import settings
import logging
from langchain.callbacks.manager import CallbackManager
from apps.common.utils import TokenCounterCallback
from django.utils import timezone
import pprint

logger = logging.getLogger(__name__)

class SummarizationManager:
    def __init__(self, model_name:str, task_instance = None):
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.model_name = model_name
        self.task_instance = task_instance
        self.llm, self.token_counter_callback = utils_get_llm(model_name, temperature=0.0)  # Use the imported function
        
    def summarize_content(self, content: str) -> str:
        
        content_tokens = tokenize(content, self.tokenizer)
        #logging.info(f'summarize_content {content_tokens} tokens: {content[:100]}')

        if content_tokens < 20:
            return f"##### TITLE: {content}",0,0
        todays_date=timezone.now().strftime("%Y-%m-%d")
        summarize_prompt = ChatPromptTemplate.from_messages([
            ("human", 
            """
            You are an AI assistant designed to perform the initial extraction of key information from various types of content. Your task is to identify and select the most important elements from the given text, creating a concise yet comprehensive foundation for further refinement. Follow these guidelines:
Identify Key Information:
Extract main ideas, critical facts, and essential data points
Include relevant statistics, dates, and figures if present
Capture the core argument or thesis of the content
Maintain Original Structure:
Preserve the logical flow of ideas from the source material
Keep extracted sentences in their original order when possible
Focus on Relevance:
Prioritize information that is central to the main topic
Exclude tangential or less important details
Capture Diverse Elements:
Include a mix of introductory, supporting, and concluding information
Ensure representation of different viewpoints if present in the original text
Preserve Context:
Include enough surrounding information to maintain clarity
Ensure extracted portions can be understood without the full original text
Handle Quotations:
Include direct quotes only if they are crucial to the main points
Properly attribute any extracted quotes
Technical Content:
For specialized topics, retain key technical terms and their explanations
Include critical methodologies or processes if relevant
Avoid Bias:
Extract information objectively, without introducing personal interpretation
Maintain the tone and intent of the original content
Formatting:
form of an engaging, easy to read, well marked down long form blog post like the style of Neil Patel.
Input: {content}                   
            """),
        ])
        logging.info(f"summarize_prompt: {summarize_prompt}")
        summarize_chain = summarize_prompt| self.llm | StrOutputParser()

        self.task_instance.update_state(
            state='summarizing content',
            meta={'current_chunk': 0, 'total_chunks': 1}           
        )

        summary = summarize_chain.invoke({'content':content, 'date':todays_date})

        input_tokens = self.token_counter_callback.input_tokens
        output_tokens = self.token_counter_callback.output_tokens
        #logging.info(f"Summarization Input tokens: {input_tokens}, output tokens: {output_tokens}")
        return summary, input_tokens, output_tokens

================
File: common/summarizer.py
================
from langchain_community.chat_models import ChatOllama
from langchain.prompts.chat import (
    ChatPromptTemplate,
)
from langchain_core.output_parsers import StrOutputParser
import re
import tiktoken

from langchain_community.document_loaders import YoutubeLoader
from .browser_tool import BrowserTools
from django.utils import timezone
from apps.seo_manager.models import SummarizerUsage
from langchain.text_splitter import TokenTextSplitter

class Summarizer:
    def __init__(self):
        self.llm_tokens_sent=0
        self.llm_tokens_r=0
        self.encoder=tiktoken.get_encoding("gpt2")

def is_pdf_url(url: str) -> bool:
    """
    Returns True if the URL points to a PDF, False otherwise.
    """
    try:
        # 1. Check if the URL has a .pdf extension
        parsed_url = urllib.parse.urlparse(url)
        if parsed_url.path.endswith('.pdf'):
            return True

        # 2. Send a HEAD request to the URL to get the Content-Type header
        response = requests.head(url, allow_redirects=True, timeout=5)

        # 3. Check if the Content-Type header is application/pdf
        content_type = response.headers.get('Content-Type')
        if content_type and content_type.startswith('application/pdf'):
            return True

        # 4. Check if the URL returns a PDF MIME type
        mime_type, _ = mimetypes.guess_type(url)
        if mime_type and mime_type.startswith('application/pdf'):
            return True

        # 5. If all else fails, try to download a small chunk of the file and check its magic number
        response = requests.get(url, stream=True, timeout=5)
        chunk = response.raw.read(1024)
        if chunk.startswith(b'%PDF-'):
            return True

        # If none of the above checks pass, it's likely not a PDF
        return False

    except requests.exceptions.RequestException as e:
        # Handle requests exceptions (e.g. connection errors, timeouts)
        print(f"Error checking URL: {e}")
        return False

    except Exception as e:
        # Handle any other unexpected exceptions
        print(f"Error checking URL: {e}")
        return False



    def compress_content(self, content, llm , task_instance, max_tokens):
        """
        Compress the given content using the provided compression chain.
        The content is split into chunks with an overlap of 100 characters, and each chunk is compressed.
        The compressed chunks are then combined, and the process is repeated if the total token count is still greater than the max_tokens.

        Args:
            content (str): The content to be compressed.
            compression_chain: The chain to use for compressing the content.
            max_tokens (int): The maximum number of tokens allowed for the compressed content.

        Returns:
            str: The compressed content.
        """
        tokenizer = tiktoken.get_encoding("gpt2")
        
        chunk_size = max_tokens*3
        overlap = round(chunk_size*.1)

        compressed_content = ""

        compress_prompt = ChatPromptTemplate.from_messages([
            ("system",
            """
            <INSTRUCTION>
            Process the text in the following guidelines, no preamble or postamble:
            <GUIDELINES>
            - preserve all details
            - use markdown
            - do not summarize
            - remove redundancies, fluff, filler content, advertisements, sponsors, and other distracting elements    
            </GUIDELINES>
            """),
            ("human", "<TEXT>{query}</TEXT>"),
        ])

        text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)

        chunks = text_splitter.split_text(content)
        compress_chain = compress_prompt | llm | StrOutputParser()
        i=0
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size-overlap)]
        continue_compression = True
        last_chunk_size=len(chunks)
        while continue_compression:
            i+=1
            compressed_chunks = []
            last_token_size=0
            num_chunks = len(chunks)
            last_chunk_size=num_chunks
            print(f"Compressing iteration {i} with {num_chunks} chunks...")
            j=0
            task_instance.update_state(
                state='start compressing',
                meta={'current_chunk': j, 'total_chunks': num_chunks}
            )
            for chunk in chunks:
                j+=1
                print(f"Compressing chunk {j} of {num_chunks}...")
                compressed_chunk = compress_chain.invoke({'query': chunk})
                compressed_chunks.append(compressed_chunk)
                self.llm_tokens_sent += len(self.encoder.encode(compress_prompt.format(query=chunk), disallowed_special=()))
                self.llm_tokens_r += len(self.encoder.encode(compressed_chunk))
                print (f"LLM tokens received: {len(self.encoder.encode(compressed_chunk))}")
                # Send progress update
                task_instance.update_state(
                    state='compressing',
                    meta={'current_chunk': j, 'total_chunks': num_chunks}
                )

            compressed_content = "\n".join(compressed_chunks)
            print(compressed_content)
            compressed_tokens = tokenizer.encode(compressed_content, disallowed_special=())
            compressed_token_count = len(compressed_tokens)
            print(f"The compressed content has {compressed_token_count} tokens.")

            if compressed_token_count <= max_tokens:
                continue_compression = False
            else:
                chunks = text_splitter.split_text(compressed_content)
                print(f"Splitting into {len(chunks)} chunks. Last chunk size: {last_chunk_size}")
                if len(chunks) >= last_chunk_size: # if compression isn't getting much smaller then stop
                    continue_compression= False 

        return compressed_content

    def summarize(self, query, user, task_instance, base_url="http://192.168.30.100:11434"):
        """
        Generate a response to a user's query using the ChatOllama model.

        Args:
            query (str): The user's query (text or URL).
            base_url (str, optional): The base URL for the API endpoint. Defaults to "https://api.example.com/v1/chat".

        Returns:
            str: The response to the user's query, formatted in Markdown.
        """
        
        max_tokens=8192

        start_time = timezone.now()
        model = 'wizardlm2:7b-q8_0' # very very good
        #model = 'qwen:1.8b'        # bad
        #model = 'phi:latest'        # bad
        #model = 'openhermes:latest' # bad
        #model = 'mistral:7b-instruct-v0.2-q6_K' # ok
        #model = 'nous-hermes2-mixtral:8x7b-dpo-q4_K_M' # good
        #model = 'gemma:7b-instruct-v1.1-q8_0'  # worth investigating more, but not bad
        #model = 'adrienbrault/nous-hermes2pro:Q8_0' # not good
        #model = 'command-r:latest'
        #model = 'eramax/senku:latest' # bad
        #model = 'yi:6b-200k-fp16' # not enough vram
        #model = 'yi:6b-chat-fp16' # only outputted chinese -> didn't pursue
        #model = 'qwen:32b' # not great
        #model = 'dolphin-llama3:8b-v2.9-q8_0'
        #model = 'phi3:3.8b-mini-instruct-4k-fp16' # not that good
        #model = 'llama3:8b-instruct-fp16' # broken
        #model = 'dolphin-llama3:latest' # not that good
        #model = 'mixtral:8x7b-instruct-v0.1-q4_K_M'
        #model = 'herald/phi3-128k'


        # Initialize the ChatOllama model with the base_url parameter
        llm = ChatOllama(model=model, base_url=base_url, temperature=0.4, num_ctx=max_tokens)
        #    llm = ChatOllama(model="llama3:latest", base_url=base_url, num_ctx="8000",)

        # Define the prompt templates
        summarize_prompt = ChatPromptTemplate.from_messages([
            ("system",
            """
            <INSTRUCTION>
            Process the text in the following guidelines, no preamble or postamble:
            <GUIDELINES>
            - preserve all details
            - use markdown
            - do not summarize
            - remove redundancies, fluff, filler content, advertisements, sponsors, and other distracting elements    
            </GUIDELINES>
            </INSTRUCTION>
            """),
            ("human", """<TEXT>{query}\n</TEXT>
            <EXPECTED OUTPUT>
            "##### Title: create a pithy and insightful title\n
            ##### Date: provide the date if cited in source material, else 'not indicated'\n
            ##### Author(s): list the author(s) of the content(if available), else 'not indicated'\n
            ###### TLDR: write a concise, pithy summary of the content and it's conclusions\n\n

            """),
        ])

        summarize_chain = summarize_prompt | llm | StrOutputParser()

        # Check if the input is a URL
        url_pattern = r'^https?://\S+$'
        if re.match(url_pattern, query):
            # Scrape the website content
            youtube_regex = r"(?:https?:\/\/)?(?:www\.)?youtu(?:\.be|be\.com)\/(?:watch\?v=)?([\w-]{11})"
            match = re.match(youtube_regex, query)
            if match:
                # Use langchain YoutubeLoader to get the transcription
                loader = YoutubeLoader.from_youtube_url(query)
                docs = loader.load()

                content = ""
                for doc in docs:
                    content += doc.page_content + "\n"  # Combine page contents

            else: 
                if self.is_pdf_url(query):
                    pdfloader = PDFDocumentLoad()
                    docs = pdfloader.load_from_url(query)
                    content = ""
                    for doc in docs:
                        content += doc.page_content + "\n"  # Combine page contents
                else:
                    # assume it's text
                    browser_tools = BrowserTools()
                    content = browser_tools.scrape_website(query)

            #print("scraped content: ", content)
        else:
            # Summarize the provided text
            content = query

        tokenizer = tiktoken.get_encoding("gpt2")
        print(f"The content precompression has {len(tokenizer.encode(content,disallowed_special=()))} tokens.")
        cleaned_content=""
        cleaned_content = self.summarize_nlp(content)
        if cleaned_content:
            content = cleaned_content
        #print(f"cleaned content: {content}")
#        print(f"The content is:\n{content}")
        # Count the tokens in the content using tiktoken
        tokens = tokenizer.encode(content, disallowed_special=())
        token_count = len(tokens)

        print(f"The content now has {len(tokenizer.encode(content,disallowed_special=()))} tokens.")

        # Compress the content if it's greater than max tokens
        if token_count > max_tokens:
            compressed_content = self.compress_content(content, llm, task_instance, max_tokens)

            token_count = len(tokenizer.encode(compressed_content,disallowed_special=()))
            print(f"The compressed_content has {token_count} tokens.")
            # Update state to indicate summarization
            content=compressed_content

        task_instance.update_state(
            state='summarizing',
            meta={'current_chunk': 1,  'total_chunks': 1}
        )

        response = summarize_chain.invoke({'query': content})

        result = response

        end_time = timezone.now()
        duration = end_time - start_time
        
        #save the usage data to the database
        usage = SummarizerUsage.objects.create(
            user=user,
            query=query,
            response=result,
            duration = duration,
            content_token_size=token_count,
            content_character_count=len(content),
            total_input_tokens=self.llm_tokens_sent,
            total_output_tokens=self.llm_tokens_r,
            model_used = model
        )
        usage.save()
        return result

================
File: common/tests.py
================
from django.test import TestCase

# Create your tests here.

================
File: common/utils.py
================
import requests
import mimetypes
import urllib.parse
import re
from django.core.cache import cache
import logging
import tiktoken
from django.conf import settings
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatLiteLLM
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.manager import CallbackManager
import openai
from langchain.schema import HumanMessage
from markdown_it import MarkdownIt
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta

# Initialize markdown-it instance at module level for reuse
md = MarkdownIt('commonmark', {'html': True})

class TokenCounterCallback(BaseCallbackHandler):
    def __init__(self, tokenizer):
        self.llm = None
        self.input_tokens = 0
        self.output_tokens = 0
        self.tokenizer = tokenizer

    def on_llm_start(self, serialized, prompts, **kwargs):
        for prompt in prompts:
            self.input_tokens += len(self.tokenizer.encode(prompt, disallowed_special=()))

    def on_llm_end(self, response, **kwargs):
        for generation in response.generations:
            for result in generation:
                self.output_tokens += len(self.tokenizer.encode(result.text, disallowed_special=()))

class ExtendedChatOpenAI(ChatOpenAI):
    """Extended ChatOpenAI with CrewAI required methods"""
    
    def supports_stop_words(self) -> bool:
        """Whether the LLM supports stop words"""
        return False  # ChatOpenAI doesn't support stop words in our implementation

def get_models():
    """
    Fetches available models from the API with improved error handling and logging
    """
    try:
        # Check if we have cached models
        cached_models = cache.get('available_models')
        if cached_models:
            return cached_models

        # Construct URL and headers
        url = f'{settings.API_BASE_URL}/models'
        headers = {
            'accept': 'application/json',
            'Authorization': f'Bearer {settings.LITELLM_MASTER_KEY}'
        }
        
        # Log the request attempt
        logging.info(f"Fetching models from {url}")
        
        # Make the request
        response = requests.get(url, headers=headers, timeout=10)
        
        # Log the response status
        logging.info(f"Models API response status: {response.status_code}")
        
        if response.status_code == 200:
            try:
                data = response.json()
                if 'data' in data and isinstance(data['data'], list):
                    # Sort the models by ID
                    models = sorted([item['id'] for item in data['data']])
                    
                    # Cache the results for 5 minutes
                    cache.set('available_models', models, 300)
                    
                    logging.info(f"Successfully fetched {len(models)} models")
                    return models
                else:
                    logging.error(f"Unexpected API response structure: {data}")
                    return []
            except ValueError as e:
                logging.error(f"Failed to parse JSON response: {e}")
                return []
        else:
            logging.error(f"API request failed with status {response.status_code}: {response.text}")
            return []
            
    except requests.exceptions.RequestException as e:
        logging.error(f"Request failed: {str(e)}")
        return []
    except Exception as e:
        logging.error(f"Unexpected error in get_models: {str(e)}")
        return []

def get_llm(model_name: str, temperature: float = 0.7, streaming: bool = False):
    """Get LLM instance and token counter based on model name"""
    try:
        from langchain_openai import ChatOpenAI
        
        # Initialize ChatOpenAI with LiteLLM proxy configuration
        llm = ExtendedChatOpenAI(
            model=model_name,
            base_url=settings.API_BASE_URL,
            api_key=settings.LITELLM_MASTER_KEY,
            temperature=temperature,
            streaming=streaming,  # Enable streaming support
            callbacks=[] if not streaming else None  # Allow callbacks for streaming
        )
        
        # Use the existing token counter
        tokenizer = tiktoken.get_encoding("cl100k_base")
        token_counter = TokenCounterCallback(tokenizer)
        
        return llm, token_counter
        
    except Exception as e:
        logger.error(f"Error initializing LLM: {str(e)}")
        raise

def is_pdf_url(url: str) -> bool:
    """Determine if the given URL points to a PDF document."""
    try:
        parsed_url = urllib.parse.urlparse(url)
        if parsed_url.path.endswith('.pdf'):
            return True
        response = requests.head(url, allow_redirects=True, timeout=5)
        content_type = response.headers.get('Content-Type')
        if content_type and content_type.startswith('application/pdf'):
            return True
        mime_type, _ = mimetypes.guess_type(url)
        if mime_type and mime_type.startswith('application/pdf'):
            return True
        response = requests.get(url, stream=True, timeout=5)
        return response.raw.read(1024).startswith(b'%PDF-')
    except requests.exceptions.RequestException:
        return False

def is_youtube(url: str) -> bool:
    return "youtube.com" in url or "youtu.be" in url

def is_stock_symbol(query):
    url = f'https://www.alphavantage.co/query?function=SYMBOL_SEARCH&keywords={query}&apikey={settings.ALPHA_VANTAGE_API_KEY}'
    r=requests.get(url)
    data = r.json()
    print(data)
    if 'bestMatches' in data and len(data['bestMatches']) > 0:
        return True
    else:
        return False

def tokenize(text: str, tokenizer = "cl100k_base") -> int:
    """ Helper function to tokenize text and return token count """
    #logging.info(f'tokenize text: {text[:50]}...')
    return len(tokenizer.encode(text, disallowed_special=()))

def extract_top_level_domain(url):
  """Extracts only the top-level domain (TLD) from a URL, handling various cases.

  Args:
    url: The URL to extract the TLD from.

  Returns:
    The top-level domain (TLD) as a string (without protocol or subdomains), 
    or None if the TLD cannot be determined or if None is passed in.
  """
  if url is None:
    return None  # Handle None input explicitly

  try:
    # Remove protocol (http://, https://)
    url = url.split("//")[-1]  
    # Remove trailing slash
    url = url.rstrip("/")
    # Split into parts and extract TLD using the previous logic
    url_parts = url.split(".")
    if len(url_parts) > 1 and url_parts[-1] in {"com", "org", "net", "edu", "gov", "mil"}:
      return url_parts[-2]  # Return TLD (e.g., sld.com, sld.org)
    elif len(url_parts) > 2 and url_parts[-3] in {"co", "ac"}:
      return ".".join(url_parts[-2:])  # Handle "sld.co.uk", etc.
    else:
      return url_parts[-1]  # Default to last part 
  except IndexError:
    return None 

def normalize_url(url):
    """Normalize a single URL"""
    url = url.lower()
    parsed_url = urllib.parse.urlparse(url)
    if parsed_url.port == 80 and parsed_url.scheme == 'http':
        parsed_url = parsed_url._replace(netloc=parsed_url.netloc.split(':')[0])
    url = urllib.parse.urlunparse(parsed_url)
    url = url.rstrip('/')
    url = urllib.parse.urldefrag(url)[0]
    url = urllib.parse.unquote(url)
    return url

def compare_urls(url1, url2):
    """Compare two URLs after normalizing them"""
    url1 = normalize_url(url1)
    url2 = normalize_url(url2)
    return url1 == url2

def format_message(content):
    if not content:
        return ''
    
    # Process ANSI color codes
    color_map = {
        '\x1b[1m': '<strong>',
        '\x1b[0m': '</strong>',
        '\x1b[93m': '<span class="text-warning">',  # Yellow
        '\x1b[92m': '<span class="text-success">',  # Green
        '\x1b[95m': '<span class="text-info">',     # Light Blue (for magenta)
        '\x1b[91m': '<span class="text-danger">',   # Red
        '\x1b[94m': '<span class="text-primary">',  # Blue
    }

    # Replace color codes with Bootstrap classes
    for code, html in color_map.items():
        content = content.replace(code, html)

    # Remove any remaining ANSI escape sequences
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    content = ansi_escape.sub('', content)

    try:
        # Convert Markdown to HTML using markdown-it
        html_content = md.render(content)

        # Parse the HTML with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')

        # Add Bootstrap classes to elements
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            tag['class'] = tag.get('class', []) + ['mt-3', 'mb-2']
        
        for tag in soup.find_all('p'):
            tag['class'] = tag.get('class', []) + ['mb-2']
        
        for tag in soup.find_all('ul', 'ol'):
            tag['class'] = tag.get('class', []) + ['pl-4']
        
        for tag in soup.find_all('code'):
            tag['class'] = tag.get('class', []) + ['bg-light', 'p-1', 'rounded']

        # Convert back to string
        formatted_content = str(soup)

        # Ensure all spans are closed
        open_spans = formatted_content.count('<span')
        close_spans = formatted_content.count('</span>')
        if open_spans > close_spans:
            formatted_content += '</span>' * (open_spans - close_spans)

        return formatted_content
    except Exception as e:
        logging.error(f"Error formatting message: {str(e)}")
        return content  # Return original content if formatting fails

class DateProcessor:
    @staticmethod
    def process_relative_date(date_str: str) -> str:
        """
        Convert relative dates to YYYY-MM-DD format
        Supports:
        - NdaysAgo (e.g., 7daysAgo)
        - NmonthsAgo (e.g., 3monthsAgo)
        - today
        - yesterday
        - YYYY-MM-DD format
        """
        if date_str == 'today':
            return datetime.now().strftime('%Y-%m-%d')
        
        if date_str == 'yesterday':
            return (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            
        # Check for NmonthsAgo format
        months_match = re.match(r'^(\d+)monthsAgo$', date_str)
        if months_match:
            months = int(months_match.group(1))
            return (datetime.now() - relativedelta(months=months)).strftime('%Y-%m-%d')
            
        # Check for NdaysAgo format
        days_match = re.match(r'^(\d+)daysAgo$', date_str)
        if days_match:
            days = int(days_match.group(1))
            return (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            
        # Assume YYYY-MM-DD format
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
            return date_str
        except ValueError:
            raise ValueError(
                "Invalid date format. Use either:\n"
                "- YYYY-MM-DD (e.g., 2024-03-15)\n"
                "- 'today' or 'yesterday'\n"
                "- 'NdaysAgo' where N is a positive number (e.g., 7daysAgo)\n"
                "- 'NmonthsAgo' where N is a positive number (e.g., 3monthsAgo)"
            )

================
File: common/views.py
================
from django.shortcuts import render

# Create your views here.

================
File: file_manager/templatetags/file_extension.py
================
from django import template
import os
from urllib.parse import quote

register = template.Library()

@register.filter
def file_extension(value):
    _, extension = os.path.splitext(value)
    return extension.lower()


@register.filter
def encoded_file_path(path):
    return path.replace('/', '%slash%')

@register.filter
def encoded_path(path):
    return path.replace('\\', '/')

================
File: file_manager/templatetags/file_manager_extras.py
================
from django import template
import os

register = template.Library()

@register.filter
def basename(value):
    return os.path.basename(value)

================
File: file_manager/templatetags/info_value.py
================
from django import template
from apps.file_manager.models import FileInfo

register = template.Library()

@register.filter
def info_value(path):
    file_info = FileInfo.objects.filter(path=path)
    if file_info.exists():
        return file_info.first().info
    else:
        return ""

================
File: file_manager/admin.py
================
from django.contrib import admin

# Register your models here.

================
File: file_manager/apps.py
================
from django.apps import AppConfig


class FileManagerConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'apps.file_manager'

================
File: file_manager/models.py
================
from django.db import models

# Create your models here.


class FileInfo(models.Model):
    path = models.URLField()
    info = models.CharField(max_length=255)

    def __str__(self):
        return self.path

================
File: file_manager/tests.py
================
from django.test import TestCase

# Create your tests here.

================
File: file_manager/urls.py
================
from django.urls import path, re_path
from apps.file_manager import views

urlpatterns = [
    re_path(r'^file-manager(?:/(?P<directory>.*?)/?)?$', views.file_manager, name='file_manager'),
    re_path(r'^delete-file/(?P<file_path>.+)/$', views.delete_file, name='delete_file'),
    re_path(r'^download-file/(?P<file_path>.+)/$', views.download_file, name='download_file'),
    path('upload-file/', views.upload_file, name='upload_file'),
    re_path(r'^save-info/(?P<file_path>.+)/$', views.save_info, name='save_info'),
]

================
File: file_manager/views.py
================
import os
import csv
import uuid
import zipfile
import tempfile
from django.shortcuts import render, redirect
from django.http import HttpResponse, Http404
from django.conf import settings
from .models import *
from django.contrib.auth.decorators import login_required
from django.urls import reverse
from urllib.parse import unquote

def convert_csv_to_text(csv_file_path):
    with open(csv_file_path, 'r') as file:
        reader = csv.reader(file)
        rows = list(reader)

    text = ''
    for row in rows:
        text += ','.join(row) + '\n'

    return text

def get_directory_contents(directory_path, user_id):
    contents = []
    if not os.path.exists(directory_path):
        return contents
    
    for name in os.listdir(directory_path):
        path = os.path.join(directory_path, name)
        rel_path = os.path.relpath(path, os.path.join(settings.MEDIA_ROOT, user_id))
        if os.path.isdir(path):
            contents.append({
                'name': name,
                'type': 'directory',
                'path': rel_path
            })
        else:
            _, extension = os.path.splitext(name)
            contents.append({
                'name': name,
                'size' : get_file_size(path),
                'type': 'file',
                'path': rel_path,
                'extension': extension[1:].lower()  # Remove the dot and convert to lowercase
            })

    return sorted(contents, key=lambda x: (x['type'] == 'file', x['name'].lower()))

@login_required(login_url='/accounts/login/basic-login/')
def save_info(request, file_path):
    path = unquote(file_path)
    if request.method == 'POST':
        FileInfo.objects.update_or_create(
            path=path,
            defaults={
                'info': request.POST.get('info')
            }
        )
    
    return redirect(request.META.get('HTTP_REFERER'))

def get_breadcrumbs(request):
    path_components = [unquote(component) for component in request.path.split("/") if component]
    breadcrumbs = []
    url = ''

    for component in path_components:
        url += f'/{component}'
        if component == "file-manager":
            component = "media"

        breadcrumbs.append({'name': component, 'url': url})

    return breadcrumbs

def generate_nested_directory(root_path, current_path):
    directory = {}
    for name in os.listdir(current_path):
        path = os.path.join(current_path, name)
        rel_path = os.path.relpath(path, root_path)
        if os.path.isdir(path):
            directory[rel_path] = {
                'type': 'directory',
                'contents': generate_nested_directory(root_path, path)
            }
        else:
            directory[rel_path] = {
                'type': 'file'
            }
    return directory

@login_required(login_url='/accounts/login/illustration-login/')
def file_manager(request, directory=''):
    user_id = str(request.user.id)
    media_path = os.path.join(settings.MEDIA_ROOT, user_id)

    if not os.path.exists(media_path):
        os.makedirs(media_path)
        
    directory_structure = generate_nested_directory(media_path, media_path)
    
    selected_directory_path = os.path.join(media_path, unquote(directory))
    contents = get_directory_contents(selected_directory_path, user_id)

    breadcrumbs = get_breadcrumbs(request)

    context = {
        'page_title': 'File Manager',
        'directory': directory_structure, 
        'contents': contents,
        'selected_directory': directory,
        'segment': 'file_manager',
        'parent': 'apps',
        'breadcrumbs': breadcrumbs,
        'user_id': user_id,
    }
    return render(request, 'pages/apps/file-manager.html', context)

@login_required(login_url='/accounts/login/basic-login/')
def delete_file(request, file_path):
    user_id = str(request.user.id)
    path = unquote(file_path)
    absolute_file_path = os.path.join(settings.MEDIA_ROOT, user_id, path)
    if os.path.exists(absolute_file_path):
        if os.path.isdir(absolute_file_path):
            import shutil
            shutil.rmtree(absolute_file_path)
        else:
            os.remove(absolute_file_path)
        print("File/Directory deleted", absolute_file_path)
    return redirect(request.META.get('HTTP_REFERER'))

@login_required(login_url='/accounts/login/basic-login/')
def download_file(request, file_path):
    user_id = str(request.user.id)
    path = unquote(file_path)
    absolute_file_path = os.path.join(settings.MEDIA_ROOT, user_id, path)
    if os.path.exists(absolute_file_path):
        if os.path.isdir(absolute_file_path):
            # Create a temporary zip file
            with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                with zipfile.ZipFile(temp_file, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for root, dirs, files in os.walk(absolute_file_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, absolute_file_path)
                            zip_file.write(file_path, arcname)

            # Read the temporary file and create the response
            with open(temp_file.name, 'rb') as f:
                response = HttpResponse(f.read(), content_type='application/zip')
                response['Content-Disposition'] = f'attachment; filename="{os.path.basename(absolute_file_path)}.zip"'

            # Delete the temporary file
            os.unlink(temp_file.name)

            return response
        else:
            with open(absolute_file_path, 'rb') as fh:
                response = HttpResponse(fh.read(), content_type="application/octet-stream")
                response['Content-Disposition'] = 'attachment; filename=' + os.path.basename(absolute_file_path)
                return response
    raise Http404

@login_required(login_url='/accounts/login/basic-login/')
def upload_file(request):
    user_id = str(request.user.id)
    media_user_path = os.path.join(settings.MEDIA_ROOT, user_id)

    if not os.path.exists(media_user_path):
        os.makedirs(media_user_path)

    selected_directory = unquote(request.POST.get('directory', ''))
    selected_directory_path = os.path.join(media_user_path, selected_directory)

    if not os.path.exists(selected_directory_path):
        os.makedirs(selected_directory_path)

    if request.method == 'POST':
        file = request.FILES.get('file')
        file_path = os.path.join(selected_directory_path, file.name)

        with open(file_path, 'wb') as destination:
            for chunk in file.chunks():
                destination.write(chunk)

    return redirect(request.META.get('HTTP_REFERER'))

def get_file_size(file_path):
    try:
        return os.path.getsize(file_path)
    except OSError:
        return 0  # Return 0 if there's an error reading the file size

================
File: seo_manager/management/commands/backfill_rankings.py
================
from django.core.management.base import BaseCommand
from apps.seo_manager.models import Client
from apps.agents.tools.google_report_tool.google_rankings_tool import GoogleRankingsTool

class Command(BaseCommand):
    help = 'Backfill historical ranking data for all clients'

    def handle(self, *args, **options):
        tool = GoogleRankingsTool()
        clients = Client.objects.filter(sc_credentials__isnull=False)
        
        for client in clients:
            self.stdout.write(f"Processing client: {client.name}")
            tool._run(None, None, client.id)

================
File: seo_manager/management/commands/generate_monthly_report.py
================
from django.core.management.base import BaseCommand
from apps.seo_manager.models import Client, KeywordRankingHistory
from datetime import date
from dateutil.relativedelta import relativedelta

class Command(BaseCommand):
    help = 'Generate monthly SEO performance report'

    def handle(self, *args, **options):
        today = date.today()
        last_month = today - relativedelta(months=1)
        
        for client in Client.objects.all():
            # Get last month's rankings
            rankings = KeywordRankingHistory.objects.filter(
                client=client,
                date__year=last_month.year,
                date__month=last_month.month
            )

            # Analyze performance
            report = {
                'client': client.name,
                'period': last_month.strftime('%B %Y'),
                'targeted_keywords': {
                    'total': client.targeted_keywords.count(),
                    'improved': 0,
                    'declined': 0,
                    'unchanged': 0
                },
                'top_improvements': [],
                'needs_attention': []
            }

            # Calculate changes
            for keyword in client.targeted_keywords.all():
                change = keyword.get_position_change()
                if change:
                    if change > 0:
                        report['targeted_keywords']['improved'] += 1
                        if change > 5:  # Significant improvement
                            report['top_improvements'].append({
                                'keyword': keyword.keyword,
                                'improvement': change
                            })
                    elif change < 0:
                        report['targeted_keywords']['declined'] += 1
                        if change < -5:  # Significant decline
                            report['needs_attention'].append({
                                'keyword': keyword.keyword,
                                'decline': change
                            })
                    else:
                        report['targeted_keywords']['unchanged'] += 1

            self.stdout.write(
                self.style.SUCCESS(f"Report generated for {client.name}")
            )
            # You could email this report, save it to a file, etc.

================
File: seo_manager/templatetags/__init__.py
================
# Empty file, just needs to exist

================
File: seo_manager/templatetags/custom_filters.py
================
from django import template
from datetime import datetime

register = template.Library()

@register.filter(name='format_iso_date')
def format_iso_date(value, format_string):
  try:
      date = datetime.fromisoformat(value)
      return date.strftime(format_string)
  except (ValueError, TypeError):
      return value  # Return the original value if parsing fails

================
File: seo_manager/templatetags/form_tags.py
================
from django import template

register = template.Library()

@register.filter(name='addclass')
def addclass(field, css_class):
    return field.as_widget(attrs={"class": css_class})

================
File: seo_manager/templatetags/seo_manager_filters.py
================
from django import template
import os

register = template.Library()

@register.filter
def abs_value(value):
    try:
        return abs(value)
    except (TypeError, ValueError):
        return value

@register.filter
def basename(value):
    """Get the basename of a file path"""
    return os.path.basename(value)

@register.filter
def split(value, arg):
    return value.split(arg)

================
File: seo_manager/templatetags/seo_tags.py
================
from django import template

register = template.Library()

@register.filter
def get_item(dictionary, key):
    """Get an item from a dictionary using bracket notation"""
    return dictionary.get(key)

================
File: seo_manager/utils/meta_tags_analyzer.py
================
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import xml.etree.ElementTree as ET

def analyze_website_meta_tags(website_url):
    try:
        # Get sitemap URL
        sitemap_url = urljoin(website_url, 'sitemap.xml')
        
        # Fetch sitemap
        response = requests.get(sitemap_url)
        root = ET.fromstring(response.content)
        
        urls = []
        for url in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc'):
            urls.append(url.text)
        
        total_tags = 0
        issues = []
        meta_data = []
        
        # Analyze each URL
        for url in urls[:10]:  # Limit to first 10 URLs for performance
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Collect meta tags
            meta_tags = soup.find_all('meta')
            total_tags += len(meta_tags)
            
            page_meta = {
                'url': url,
                'title': soup.title.string if soup.title else None,
                'meta_tags': []
            }
            
            for tag in meta_tags:
                tag_data = {
                    'name': tag.get('name', tag.get('property', '')),
                    'content': tag.get('content', '')
                }
                page_meta['meta_tags'].append(tag_data)
                
                # Check for common issues
                if not tag.get('content'):
                    issues.append(f"Empty content in meta tag {tag_data['name']} on {url}")
            
            meta_data.append(page_meta)
        
        return {
            'total_tags': total_tags,
            'issues_count': len(issues),
            'issues': issues,
            'pages': meta_data
        }
        
    except Exception as e:
        raise Exception(f"Error analyzing meta tags: {str(e)}")

================
File: seo_manager/views/__init__.py
================
from .client_views import *
from .keyword_views import *
from .project_views import *
from .analytics_views import *
from .search_console_views import *
from .business_objective_views import *
from .ranking_views import *
from .report_views import *
from .activity_views import *
from .meta_tags_views import *

================
File: seo_manager/views/activity_views.py
================
from django.shortcuts import render
from django.contrib.auth.decorators import login_required
from ..models import UserActivity

@login_required
def activity_log(request):
    activities = UserActivity.objects.all().order_by('-timestamp')
    return render(request, 'seo_manager/activity_log.html', {'activities': activities})

================
File: seo_manager/views/analytics_views.py
================
import json
from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from django.core.exceptions import ValidationError
from ..models import Client, GoogleAnalyticsCredentials, OAuthManager, SearchConsoleCredentials
from ..google_auth import (
    get_analytics_accounts_oauth, 
    get_analytics_accounts_service_account,
    get_search_console_properties
)
from apps.common.tools.user_activity_tool import user_activity_tool
from ..exceptions import AuthError
from django.views.decorators.http import require_http_methods
from django.http import JsonResponse
import logging
import google.oauth2.credentials
from google_auth_oauthlib.flow import Flow
from django.conf import settings

logger = logging.getLogger(__name__)

@login_required
def client_ads(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    # Add page_title to the context
    context = {
        'client': client,
        'page_title': 'Google Ads',
    }
    return render(request, 'seo_manager/client_ads.html', context)

@login_required
def client_dataforseo(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    # Add page_title to the context
    context = {
        'client': client,
        'page_title': 'DataforSEO',
    }
    return render(request, 'seo_manager/client_dataforseo.html', context)

@login_required
def initiate_google_oauth(request, client_id, service_type):
    """Start OAuth flow for Google services"""
    client = get_object_or_404(Client, id=client_id)
    
    try:
        # Create state key
        state_key = f"{client_id}_{service_type}"
        
        # Get the current domain and scheme
        scheme = request.scheme
        domain = request.get_host()
        redirect_uri = f'{scheme}://{domain}/google/login/callback/'
        
        # Create OAuth flow with correct callback URL
        flow = Flow.from_client_secrets_file(
            settings.GOOGLE_CLIENT_SECRETS_FILE,
            scopes=[
                'https://www.googleapis.com/auth/analytics.readonly',
                'https://www.googleapis.com/auth/webmasters.readonly',
                'openid',
                'https://www.googleapis.com/auth/userinfo.email',
                'https://www.googleapis.com/auth/userinfo.profile'
            ],
            state=state_key,
            redirect_uri=redirect_uri
        )
        
        # Store both state and redirect URI in session
        request.session['oauth_state'] = state_key
        request.session['oauth_redirect_uri'] = redirect_uri
        request.session['oauth_service_type'] = service_type
        request.session.modified = True
        
        # Get authorization URL with proper parameters
        authorization_url, _ = flow.authorization_url(
            access_type='offline',
            include_granted_scopes='true',
            prompt='select_account'
        )
        
        logger.info(f"Initiating OAuth flow for client {client.name} ({service_type})")
        logger.debug(f"Using redirect URI: {redirect_uri}")
        
        return redirect(authorization_url)
        
    except Exception as e:
        logger.error(f"OAuth initiation error: {str(e)}", exc_info=True)
        messages.error(request, "Failed to start authentication process")
        return redirect('seo_manager:client_detail', client_id=client_id)

@login_required
def google_oauth_callback(request):
    """Handle OAuth callback"""
    try:
        # Verify state
        state = request.GET.get('state')
        stored_state = request.session.get('oauth_state')
        
        if not state or state != stored_state:
            logger.error("Invalid OAuth state")
            messages.error(request, "Invalid authentication state. Please try again.")
            return redirect('seo_manager:dashboard')
        
        # Extract client_id from state
        try:
            client_id = int(state.split('_')[0])
            service_type = state.split('_')[1]
        except (IndexError, ValueError):
            logger.error(f"Invalid state format: {state}")
            messages.error(request, "Invalid authentication data. Please try again.")
            return redirect('seo_manager:dashboard')
        
        client = get_object_or_404(Client, id=client_id)
        logger.info(f"Processing OAuth callback for client {client.name} ({service_type})")
        
        # Get the current domain and scheme for redirect URI
        scheme = request.scheme
        domain = request.get_host()
        redirect_uri = f'{scheme}://{domain}/google/login/callback/'
        
        # Complete OAuth flow
        try:
            flow = Flow.from_client_secrets_file(
                settings.GOOGLE_CLIENT_SECRETS_FILE,
                scopes=[
                    'https://www.googleapis.com/auth/analytics.readonly',
                    'https://www.googleapis.com/auth/webmasters.readonly',
                    'openid',
                    'https://www.googleapis.com/auth/userinfo.email',
                    'https://www.googleapis.com/auth/userinfo.profile'
                ],
                state=state,
                redirect_uri=redirect_uri
            )
            
            # Fetch token
            token = flow.fetch_token(code=request.GET.get('code'))
            
            # Convert token to Google Credentials
            credentials = google.oauth2.credentials.Credentials(
                token=token['access_token'],
                refresh_token=token.get('refresh_token'),
                token_uri='https://oauth2.googleapis.com/token',
                client_id=flow.client_config['client_id'],
                client_secret=flow.client_config['client_secret'],
                scopes=token.get('scope', '').split(' ') if isinstance(token.get('scope'), str) else token.get('scope', [])
            )
            
            # Store credentials in session
            request.session['oauth_credentials'] = OAuthManager.credentials_to_dict(credentials)
            
            if service_type == 'ga':
                try:
                    # Get available accounts using the Google Credentials
                    accounts = get_analytics_accounts_oauth(credentials)
                    
                    if not accounts:
                        logger.warning(f"No GA4 properties found for client {client.name}")
                        messages.warning(request, "No Google Analytics 4 properties were found.")
                        return redirect('seo_manager:client_detail', client_id=client_id)
                    
                    request.session['accounts'] = accounts
                    return redirect('seo_manager:select_analytics_account', client_id=client_id)
                    
                except Exception as e:
                    logger.error(f"Error fetching GA4 properties: {str(e)}", exc_info=True)
                    messages.error(request, "Failed to fetch Google Analytics properties.")
                    return redirect('seo_manager:client_detail', client_id=client_id)
            
            elif service_type == 'sc':
                try:
                    # Get available Search Console properties
                    properties = get_search_console_properties(credentials)
                    if not properties:
                        messages.warning(request, "No Search Console properties found. Please verify your permissions.")
                        return redirect('seo_manager:client_detail', client_id=client_id)
                    
                    request.session['sc_properties'] = properties
                    return redirect('seo_manager:select_search_console_property', client_id=client_id)
                    
                except Exception as e:
                    logger.error(f"Error fetching Search Console properties: {str(e)}", exc_info=True)
                    messages.error(request, "Failed to fetch Search Console properties. Please verify your permissions.")
                    return redirect('seo_manager:client_detail', client_id=client_id)
            
        except Exception as e:
            logger.error(f"OAuth flow error: {str(e)}", exc_info=True)
            messages.error(request, "Authentication failed. Please try again.")
            return redirect('seo_manager:client_detail', client_id=client_id)
            
    except Exception as e:
        logger.error(f"OAuth callback error: {str(e)}", exc_info=True)
        messages.error(request, "Authentication failed. Please try again.")
        return redirect('seo_manager:dashboard')
    
    return redirect('seo_manager:client_detail', client_id=client_id)

@login_required
def add_ga_credentials_oauth(request, client_id):
    """Handle Google Analytics OAuth credential addition"""
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'GET':
        try:
            # Create state key
            state_key = f"{client_id}_ga"
            
            # Create OAuth flow
            flow = OAuthManager.create_oauth_flow(
                request=request, 
                state_key=state_key
            )
            
            # Get authorization URL with proper parameters
            authorization_url, _ = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                prompt='consent'
            )
            
            # Store state in session
            request.session['oauth_state'] = state_key
            request.session['oauth_service_type'] = 'ga'
            request.session['oauth_client_id'] = client_id
            
            logger.info(f"Starting GA OAuth flow for client {client.name}")
            return redirect(authorization_url)
            
        except Exception as e:
            logger.error(f"Error starting GA OAuth flow: {str(e)}")
            messages.error(request, "Failed to start authentication process")
            return redirect('seo_manager:client_detail', client_id=client_id)
    
    return redirect('seo_manager:client_detail', client_id=client_id)

@login_required
def add_ga_credentials_service_account(request, client_id):
    """Handle Google Analytics service account credential addition"""
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'GET':
        return render(request, 'seo_manager/credentials/add_ga_service_account.html', {
            'client': client
        })
        
    elif request.method == 'POST':
        try:
            if 'service_account_file' not in request.FILES:
                raise AuthError("No service account file provided")
                
            service_account_file = request.FILES['service_account_file']
            service_account_json = json.load(service_account_file)
            
            ga_credentials, created = GoogleAnalyticsCredentials.objects.get_or_create(
                client=client,
                defaults={'user_email': request.user.email}
            )
            
            ga_credentials.handle_service_account(json.dumps(service_account_json))
            
            messages.success(request, "Service account credentials added successfully")
            return redirect('seo_manager:client_detail', client_id=client.id)
            
        except AuthError as e:
            messages.error(request, str(e))
        except json.JSONDecodeError:
            messages.error(request, "Invalid JSON file")
        except Exception as e:
            logger.error(f"Error adding service account: {str(e)}")
            messages.error(request, "Failed to add service account credentials")
            
        return redirect('seo_manager:client_detail', client_id=client.id)

@login_required
def remove_ga_credentials(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    if client.ga_credentials:
        client.ga_credentials.delete()
        user_activity_tool.run(request.user, 'delete', f"Removed Google Analytics credentials for client: {client.name}", client=client)
        messages.success(request, "Google Analytics credentials removed successfully.")
    return redirect('seo_manager:client_detail', client_id=client.id)

@login_required
def client_detail(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    try:
        # Handle GA credentials check
        ga_credentials = getattr(client, 'ga_credentials', None)
        if ga_credentials:
            try:
                ga_credentials.validate_credentials()
            except AuthError:
                messages.warning(request, "Google Analytics credentials need to be re-authenticated.")
                ga_credentials.delete()
        
        # Handle SC credentials check similarly
        sc_credentials = getattr(client, 'sc_credentials', None)
        if sc_credentials:
            try:
                sc_credentials.validate_credentials()
            except AuthError:
                messages.warning(request, "Search Console credentials need to be re-authenticated.")
                sc_credentials.delete()
                
    except Exception as e:
        logger.error(f"Error checking credentials: {str(e)}")
        messages.error(request, "Error validating credentials")

    return render(request, 'seo_manager/client_detail.html', {'client': client})

@login_required
def add_sc_credentials(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    # Start OAuth flow
    try:
        flow = OAuthManager.create_oauth_flow(
            request, 
            state=f"{client_id}_sc"
        )
        authorization_url, _ = flow.authorization_url(
            access_type='offline',
            include_granted_scopes='true',
            prompt='consent'
        )
        request.session['oauth_state'] = f"{client_id}_sc"
        return redirect(authorization_url)
        
    except Exception as e:
        logger.error(f"Error initiating Search Console OAuth: {str(e)}")
        messages.error(request, "Failed to start authentication process")
        return redirect('seo_manager:client_detail', client_id=client_id)

@login_required
def select_analytics_account(request, client_id):
    """Handle Analytics account selection"""
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'POST':
        try:
            selected_account = request.POST.get('selected_account')
            if not selected_account:
                raise ValidationError("No account selected")
                
            credentials_dict = request.session.get('oauth_credentials')
            if not credentials_dict:
                raise AuthError("No OAuth credentials found in session")
            
            logger.info(f"Creating/updating GA credentials for client {client.name}")
            logger.debug(f"Credentials dict: {credentials_dict}")
                
            ga_credentials, created = GoogleAnalyticsCredentials.objects.get_or_create(
                client=client,
                defaults={'user_email': request.user.email}
            )
            
            # Save credentials and account info
            ga_credentials.handle_oauth_response(credentials_dict)
            ga_credentials.view_id = selected_account
            ga_credentials.save()
            
            logger.info(f"Successfully saved GA credentials for client {client.name}")
            
            # Clean up session
            for key in ['oauth_credentials', 'accounts', 'oauth_state', 'oauth_service_type', 'oauth_client_id']:
                request.session.pop(key, None)
            
            messages.success(request, "Google Analytics credentials added successfully")
            return redirect('seo_manager:client_detail', client_id=client.id)
            
        except Exception as e:
            logger.error(f"Error saving GA credentials: {str(e)}", exc_info=True)
            messages.error(request, f"Failed to save Google Analytics credentials: {str(e)}")
            return redirect('seo_manager:client_detail', client_id=client.id)
    
    # Handle GET request
    accounts = request.session.get('accounts', [])
    if not accounts:
        messages.error(request, "No Analytics accounts found in session. Please try authenticating again.")
        return redirect('seo_manager:client_detail', client_id=client.id)
        
    return render(request, 'seo_manager/select_analytics_account.html', {
        'client': client,
        'accounts': accounts
    })

@login_required
def select_search_console_property(request, client_id):
    """Handle Search Console property selection"""
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'POST':
        try:
            selected_property = request.POST.get('selected_property')
            if not selected_property:
                raise ValidationError("No property selected")
                
            credentials_dict = request.session.get('oauth_credentials')
            if not credentials_dict:
                raise AuthError("No OAuth credentials found in session")
            
            logger.info(f"Creating/updating SC credentials for client {client.name}")
            logger.debug(f"Credentials dict: {credentials_dict}")
                
            sc_credentials, created = SearchConsoleCredentials.objects.get_or_create(
                client=client,
                defaults={}
            )
            
            # Save credentials and property info
            sc_credentials.handle_oauth_response(credentials_dict)
            sc_credentials.property_url = selected_property
            if request.user.email:
                sc_credentials.user_email = request.user.email
            sc_credentials.save()
            
            logger.info(f"Successfully saved SC credentials for client {client.name}")
            
            # Clean up session
            for key in ['oauth_credentials', 'sc_properties', 'oauth_state']:
                request.session.pop(key, None)
            
            messages.success(request, "Search Console credentials added successfully")
            return redirect('seo_manager:client_detail', client_id=client.id)
            
        except Exception as e:
            logger.error(f"Error saving SC credentials: {str(e)}", exc_info=True)
            messages.error(request, f"Failed to save Search Console credentials: {str(e)}")
            return redirect('seo_manager:client_detail', client_id=client.id)
    
    # Handle GET request
    properties = request.session.get('sc_properties', [])
    if not properties:
        messages.error(request, "No Search Console properties found in session. Please try authenticating again.")
        return redirect('seo_manager:client_detail', client_id=client.id)
        
    return render(request, 'seo_manager/select_search_console_property.html', {
        'client': client,
        'properties': properties
    })

@login_required
def add_sc_credentials_service_account(request, client_id):
    """Handle Search Console service account credential addition"""
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'GET':
        return render(request, 'seo_manager/credentials/add_sc_service_account.html', {
            'client': client
        })
        
    elif request.method == 'POST':
        try:
            if 'service_account_file' not in request.FILES:
                raise AuthError("No service account file provided")
                
            service_account_file = request.FILES['service_account_file']
            service_account_json = json.load(service_account_file)
            
            sc_credentials, created = SearchConsoleCredentials.objects.get_or_create(
                client=client,
                defaults={'user_email': request.user.email}
            )
            
            sc_credentials.handle_service_account(json.dumps(service_account_json))
            
            messages.success(request, "Service account credentials added successfully")
            return redirect('seo_manager:client_detail', client_id=client.id)
            
        except AuthError as e:
            messages.error(request, str(e))
        except json.JSONDecodeError:
            messages.error(request, "Invalid JSON file")
        except Exception as e:
            logger.error(f"Error adding service account: {str(e)}")
            messages.error(request, "Failed to add service account credentials")
            
        return redirect('seo_manager:client_detail', client_id=client.id)

================
File: seo_manager/views/auth_views.py
================


================
File: seo_manager/views/business_objective_views.py
================
from datetime import datetime
from django.shortcuts import get_object_or_404, redirect
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from django.shortcuts import render
from ..models import Client
from ..forms import BusinessObjectiveForm
from apps.common.tools.user_activity_tool import user_activity_tool
import json
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods

@login_required
def add_business_objective(request, client_id):
    if request.method == 'POST':
        client = get_object_or_404(Client, id=client_id)
        form = BusinessObjectiveForm(request.POST)
        
        if form.is_valid():
            new_objective = {
                'goal': form.cleaned_data['goal'],
                'metric': form.cleaned_data['metric'],
                'target_date': form.cleaned_data['target_date'].isoformat(),
                'status': form.cleaned_data['status'],
                'date_created': datetime.now().isoformat(),
                'date_last_modified': datetime.now().isoformat(),
            }
            
            if not client.business_objectives:
                client.business_objectives = []
            
            client.business_objectives.append(new_objective)
            client.save()
            
            user_activity_tool.run(
                request.user, 
                'create', 
                f"Added business objective for client: {client.name}", 
                client=client,
                details=new_objective
            )
            
            messages.success(request, "Business objective added successfully.")
        else:
            messages.error(request, "Error adding business objective. Please check the form.")
            
    return redirect('seo_manager:client_detail', client_id=client_id)

@login_required
def edit_business_objective(request, client_id, objective_index):
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'POST':
        form = BusinessObjectiveForm(request.POST)
        if form.is_valid():
            updated_objective = {
                'goal': form.cleaned_data['goal'],
                'metric': form.cleaned_data['metric'],
                'target_date': form.cleaned_data['target_date'].isoformat(),
                'status': form.cleaned_data['status'],
                'date_created': client.business_objectives[objective_index]['date_created'],
                'date_last_modified': datetime.now().isoformat(),
            }
            client.business_objectives[objective_index] = updated_objective
            client.save()
            user_activity_tool.run(request.user, 'update', f"Updated business objective for client: {client.name}", client=client, details=updated_objective)
            messages.success(request, "Business objective updated successfully.")
            return redirect('seo_manager:client_detail', client_id=client.id)
    else:
        objective = client.business_objectives[objective_index]
        initial_data = {
            'goal': objective['goal'],
            'metric': objective['metric'],
            'target_date': datetime.fromisoformat(objective['target_date']),
            'status': objective['status'],
        }
        form = BusinessObjectiveForm(initial=initial_data)
    
    context = {
        'page_title': 'Edit Business Objective',
        'client': client,
        'form': form,
        'objective_index': objective_index,
    }
    
    return render(request, 'seo_manager/edit_business_objective.html', context)

@login_required
def delete_business_objective(request, client_id, objective_index):
    if request.method == 'POST':
        client = get_object_or_404(Client, id=client_id)
        deleted_objective = client.business_objectives.pop(objective_index)
        client.save()
        user_activity_tool.run(request.user, 'delete', f"Deleted business objective for client: {client.name}", client=client, details=deleted_objective)
        messages.success(request, "Business objective deleted successfully.")
    return redirect('seo_manager:client_detail', client_id=client_id)

@require_http_methods(["POST"])
def update_objective_status(request, client_id, objective_index):
    try:
        client = Client.objects.get(id=client_id)
        data = json.loads(request.body)
        new_status = data.get('status')
        
        # Get the objectives list
        objectives = client.business_objectives
        
        # Update the status of the specific objective
        if 0 <= objective_index < len(objectives):
            objectives[objective_index]['status'] = new_status == 'active'
            
            # Update the last modified date
            objectives[objective_index]['date_last_modified'] = datetime.now().isoformat()
            
            # Save the updated objectives
            client.business_objectives = objectives
            client.save()
            
            return JsonResponse({'success': True})
        else:
            return JsonResponse({'success': False, 'error': 'Objective not found'})
            
    except Client.DoesNotExist:
        return JsonResponse({'success': False, 'error': 'Client not found'})
    except Exception as e:
        return JsonResponse({'success': False, 'error': str(e)})

================
File: seo_manager/views/client_views.py
================
import json
import logging
import os
import csv
from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from django.http import JsonResponse, HttpResponse
from django.conf import settings
from django.db.models import Min, Max, Q
from ..models import Client, KeywordRankingHistory, UserActivity, SearchConsoleCredentials
from ..forms import ClientForm, BusinessObjectiveForm, TargetedKeywordForm, KeywordBulkUploadForm, SEOProjectForm, ClientProfileForm
from apps.common.tools.user_activity_tool import user_activity_tool
from apps.agents.tools.client_profile_tool.client_profile_tool import ClientProfileTool
from apps.agents.models import Tool
from datetime import datetime, timedelta
from markdown_it import MarkdownIt  # Import markdown-it
from django.urls import reverse
from .search_console_views import get_search_console_data

logger = logging.getLogger(__name__)
__all__ = [
    'dashboard',
    'client_list',
    'add_client',
    'client_detail',
    'edit_client',
    'delete_client',
    'update_client_profile',
    'generate_magic_profile',
    'load_more_activities',
    'export_activities',
    'client_integrations',
]

@login_required
def dashboard(request):
    clients = Client.objects.all().order_by('name')
    form = ClientForm()
    return render(request, 'seo_manager/dashboard.html', {'page_title': 'Dashboard', 'clients': clients, 'form': form})

@login_required
def client_list(request):
    clients = Client.objects.all().order_by('name').select_related('group')
    form = ClientForm()
    return render(request, 'seo_manager/client_list.html', {'page_title': 'Clients', 'clients': clients, 'form': form})

@login_required
def add_client(request):
    if request.method == 'POST':
        form = ClientForm(request.POST)
        if form.is_valid():
            client = form.save()
            user_activity_tool.run(request.user, 'create', f"Added new client: {client.name}", client=client)
            messages.success(request, f"Client '{client.name}' has been added successfully.")
            
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'success': True,
                    'message': f"Client '{client.name}' has been added successfully.",
                    'redirect_url': reverse('seo_manager:client_detail', args=[client.id])
                })
            
            return redirect('seo_manager:client_detail', client_id=client.id)
        else:
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'success': False,
                    'errors': form.errors
                })
    else:
        form = ClientForm()
    
    return render(request, 'seo_manager/add_client.html', {'form': form})

@login_required
def client_detail(request, client_id):
    logger.debug(f"Accessing client_detail view for client_id: {client_id}")

    # Prefetch all related data in a single query
    client = get_object_or_404(
        Client.objects.prefetch_related(
            'targeted_keywords',
            'targeted_keywords__ranking_history',
            'seo_projects',
            'seo_projects__targeted_keywords',
            'seo_projects__targeted_keywords__ranking_history'
        ), 
        id=client_id
    )
    
    # Get keyword history in a single query
    keyword_history = KeywordRankingHistory.objects.filter(
        client_id=client_id
    ).select_related('keyword').order_by('-date')
    
    # Create a dictionary to store history by keyword
    history_by_keyword = {}
    for history in keyword_history:
        if history.keyword_id not in history_by_keyword:
            history_by_keyword[history.keyword_id] = []
        history_by_keyword[history.keyword_id].append(history)
    
    # Attach history to keywords
    for keyword in client.targeted_keywords.all():
        keyword.ranking_data = history_by_keyword.get(keyword.id, [])
    
    # Get client activities
    important_categories = ['create', 'update', 'delete', 'export', 'import', 'other']
    client_activities = UserActivity.objects.filter(
        client=client,
        category__in=important_categories
    ).select_related('user').order_by('-timestamp')[:10]

    # Initialize forms
    keyword_form = TargetedKeywordForm()
    import_form = KeywordBulkUploadForm()
    project_form = SEOProjectForm(client=client)
    business_objective_form = BusinessObjectiveForm()
    
    # Get meta tags files
    meta_tags_dir = os.path.join(settings.MEDIA_ROOT, 'meta-tags', str(client.id))
    meta_tags_files = []
    if os.path.exists(meta_tags_dir):
        meta_tags_files = sorted(
            [f for f in os.listdir(meta_tags_dir) if f.endswith('.json')],
            key=lambda x: os.path.getmtime(os.path.join(meta_tags_dir, x)),
            reverse=True
        )

    # Get ranking stats in a single query
    ranking_stats = keyword_history.aggregate(
        earliest_date=Min('date'),
        latest_date=Max('date')
    )
    
    latest_collection_date = ranking_stats['latest_date']
    logger.debug(f"Latest collection date: {latest_collection_date}")
    
    data_coverage_months = 0
    if ranking_stats['earliest_date'] and ranking_stats['latest_date']:
        date_diff = ranking_stats['latest_date'] - ranking_stats['earliest_date']
        data_coverage_months = round(date_diff.days / 30)
    
    tracked_keywords_count = keyword_history.values('keyword_text').distinct().count()

    # Get Search Console data
    search_console_data = []
    try:
        sc_credentials = getattr(client, 'sc_credentials', None)
        if sc_credentials:
            service = sc_credentials.get_service()
            if service:
                property_url = sc_credentials.get_property_url()
                if property_url:
                    end_date = datetime.now().strftime('%Y-%m-%d')
                    start_date = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')
                    search_console_data = get_search_console_data(
                        service, 
                        property_url,
                        start_date,
                        end_date
                    )
    except Exception as e:
        logger.error(f"Error fetching search console data: {str(e)}")

    context = {
        'page_title': 'Client Detail',
        'client': client,
        'client_activities': client_activities,
        'business_objectives': client.business_objectives,
        'business_objective_form': business_objective_form,
        'keyword_form': keyword_form,
        'import_form': import_form,
        'project_form': project_form,
        'meta_tags_files': meta_tags_files,
        'client_profile_html': client.client_profile,
        'profile_form': ClientProfileForm(initial={'client_profile': client.client_profile}),
        'latest_collection_date': latest_collection_date,
        'data_coverage_months': data_coverage_months,
        'tracked_keywords_count': tracked_keywords_count,
        'search_console_data': search_console_data,
    }
    
    return render(request, 'seo_manager/client_detail.html', context)

@login_required
def edit_client(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    if request.method == 'POST':
        form = ClientForm(request.POST, instance=client)
        if form.is_valid():
            form.save()
            user_activity_tool.run(request.user, 'update', f"Updated client details: {client.name}", client=client)
            
            # Check if request is AJAX
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'success': True,
                    'message': f"Client '{client.name}' has been updated successfully.",
                })
            
            messages.success(request, f"Client '{client.name}' has been updated successfully.")
            return redirect('seo_manager:client_detail', client_id=client.id)
        else:
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'success': False,
                    'errors': {field: [str(error) for error in errors] 
                              for field, errors in form.errors.items()}
                })
            
    else:
        form = ClientForm(instance=client)
    
    if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
        return JsonResponse({
            'success': False,
            'errors': {'form': ['Invalid form submission']}
        })
        
    return render(request, 'seo_manager/edit_client.html', {'form': form, 'client': client})

@login_required
def delete_client(request, client_id):
    if request.method == 'POST':
        client = get_object_or_404(Client, id=client_id)
        user_activity_tool.run(request.user, 'delete', f"Deleted client: {client.name}", client=client)
        client.delete()
        return JsonResponse({'success': True})
    return JsonResponse({'success': False}, status=400)

@login_required
def update_client_profile(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'POST':
        try:
            client_profile = request.POST.get('client_profile', '')
            if not client_profile:
                raise ValueError("Profile content cannot be empty")
                
            client.client_profile = client_profile
            client.save()
            
            user_activity_tool.run(
                request.user,
                'update',
                f"Updated client profile for: {client.name}",
                client=client
            )
            
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'success': True,
                    'message': "Client profile updated successfully."
                })
            
            messages.success(request, "Client profile updated successfully.")
            return redirect('seo_manager:client_detail', client_id=client.id)
            
        except Exception as e:
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'success': False,
                    'error': str(e)
                })
            
            messages.error(request, f"Error updating profile: {str(e)}")
            return redirect('seo_manager:client_detail', client_id=client.id)
    
    messages.error(request, "Invalid form submission.")
    return redirect('seo_manager:client_detail', client_id=client.id)

@login_required
def generate_magic_profile(request, client_id):
    if request.method == 'POST':
        try:
            # Get the tool ID
            tool = get_object_or_404(Tool, tool_subclass='ClientProfileTool')
            
            # Start Celery task
            from apps.agents.tasks import run_tool
            task = run_tool.delay(tool.id, {'client_id': str(client_id)})
            
            return JsonResponse({
                'success': True,
                'task_id': task.id,
                'message': 'Profile generation started'
            })
                
        except Exception as e:
            logger.error(f"Error generating magic profile: {str(e)}")
            return JsonResponse({
                'success': False,
                'error': str(e)
            })
            
    return JsonResponse({'success': False, 'error': 'Invalid request method'})

@login_required
def load_more_activities(request, client_id):
    page = int(request.GET.get('page', 1))
    per_page = 10
    start = (page - 1) * per_page
    end = start + per_page

    client = get_object_or_404(Client, id=client_id)
    important_categories = ['create', 'update', 'delete', 'export', 'import', 'other']
    
    activities = UserActivity.objects.filter(
        client=client,
        category__in=important_categories
    ).order_by('-timestamp')[start:end + 1]  # Get one extra to check if there are more

    has_more = len(activities) > per_page
    activities = activities[:per_page]  # Remove the extra item if it exists

    # Render activities to HTML
    html = render(request, 'seo_manager/includes/activity_items.html', {
        'client_activities': activities
    }).content.decode('utf-8')

    return JsonResponse({
        'success': True,
        'activities': html,
        'has_more': has_more
    })

@login_required
def export_activities(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    filter_type = request.GET.get('filter', 'all')
    
    # Get activities based on filter
    activities = UserActivity.objects.filter(client=client).order_by('-timestamp')
    if filter_type != 'all':
        activities = activities.filter(category=filter_type)
    
    # Create the HttpResponse object with CSV header
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = f'attachment; filename="{client.name}_activities_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv"'
    
    # Create CSV writer
    writer = csv.writer(response)
    writer.writerow(['Timestamp', 'User', 'Category', 'Action', 'Details'])
    
    # Write data
    for activity in activities:
        writer.writerow([
            activity.timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            activity.user.username if activity.user else 'System',
            activity.get_category_display(),
            activity.action,
            activity.details if activity.details else ''
        ])
    
    # Log the export activity
    user_activity_tool.run(
        request.user,
        'export',
        f"Exported {filter_type} activities",
        client=client
    )
    
    return response

@login_required
def client_integrations(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    context = {
        'page_title': 'Client Integrations',
        'client': client,
        'segment': 'clients',
        'subsegment': 'integrations'
    }
    return render(request, 'seo_manager/client_integrations.html', context)

================
File: seo_manager/views/keyword_views.py
================
import csv
import io
import logging
from django.shortcuts import get_object_or_404, redirect, render
from django.contrib.auth.decorators import login_required
from django.contrib.auth.mixins import LoginRequiredMixin
from django.contrib import messages
from django.views.generic import ListView, CreateView, UpdateView
from django.urls import reverse_lazy
from django.http import JsonResponse
from ..models import Client, TargetedKeyword, KeywordRankingHistory, SearchConsoleCredentials
from ..forms import TargetedKeywordForm, KeywordBulkUploadForm
from apps.common.tools.user_activity_tool import user_activity_tool
import json
from datetime import datetime, timedelta
from .search_console_views import get_search_console_data

logger = logging.getLogger(__name__)

class KeywordListView(LoginRequiredMixin, ListView):
    template_name = 'seo_manager/keywords/keyword_list.html'
    context_object_name = 'keywords'

    def get_queryset(self):
        return TargetedKeyword.objects.filter(client_id=self.kwargs['client_id'])

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['client'] = get_object_or_404(Client, id=self.kwargs['client_id'])
        context['import_form'] = KeywordBulkUploadForm()
        return context

class KeywordCreateView(LoginRequiredMixin, CreateView):
    model = TargetedKeyword
    form_class = TargetedKeywordForm
    template_name = 'seo_manager/keywords/keyword_form.html'

    def form_valid(self, form):
        form.instance.client_id = self.kwargs['client_id']
        response = super().form_valid(form)
        user_activity_tool.run(self.request.user, 'create', f"Added keyword: {form.instance.keyword}", client=form.instance.client)
        return response

    def get_success_url(self):
        return reverse_lazy('seo_manager:client_detail', kwargs={'client_id': self.kwargs['client_id']})

class KeywordUpdateView(LoginRequiredMixin, UpdateView):
    model = TargetedKeyword
    form_class = TargetedKeywordForm
    template_name = 'seo_manager/keywords/keyword_form.html'

    def get_queryset(self):
        # Ensure the keyword belongs to the correct client
        return TargetedKeyword.objects.filter(
            client_id=self.kwargs['client_id']
        )

    def form_valid(self, form):
        response = super().form_valid(form)
        user_activity_tool.run(
            self.request.user, 
            'update', 
            f"Updated keyword: {form.instance.keyword}", 
            client=form.instance.client
        )
        messages.success(self.request, "Keyword updated successfully.")
        return response

    def get_success_url(self):
        return reverse_lazy('seo_manager:client_detail', 
                          kwargs={'client_id': self.kwargs['client_id']})

@login_required
def keyword_import(request, client_id):
    if request.method == 'POST':
        form = KeywordBulkUploadForm(request.POST, request.FILES)
        if form.is_valid():
            client = get_object_or_404(Client, id=client_id)
            csv_file = request.FILES['csv_file']
            decoded_file = csv_file.read().decode('utf-8')
            csv_data = csv.DictReader(io.StringIO(decoded_file))
            
            for row in csv_data:
                TargetedKeyword.objects.create(
                    client=client,
                    keyword=row['keyword'],
                    priority=int(row['priority']),
                    notes=row.get('notes', '')
                )
            
            user_activity_tool.run(request.user, 'import', f"Imported keywords from CSV", client=client)
            messages.success(request, "Keywords imported successfully.")
            return redirect('seo_manager:client_detail', client_id=client_id)
    
    messages.error(request, "Invalid form submission.")
    return redirect('seo_manager:client_detail', client_id=client_id)

@login_required
def debug_keyword_data(request, client_id, keyword_id):
    """Debug view to check keyword data"""
    keyword = get_object_or_404(TargetedKeyword, id=keyword_id, client_id=client_id)
    
    rankings = KeywordRankingHistory.objects.filter(
        keyword=keyword
    ).order_by('-date')
    
    data = {
        'keyword': keyword.keyword,
        'current_position': keyword.current_position,
        'position_change': keyword.get_position_change(),
        'rankings': [
            {
                'date': r.date.strftime('%Y-%m-%d'),
                'position': r.average_position,
                'impressions': r.impressions,
                'clicks': r.clicks,
                'ctr': r.ctr
            }
            for r in rankings
        ]
    }
    
    return JsonResponse(data)

@login_required
def import_from_search_console(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    if request.method == 'POST':
        try:
            keywords_data = json.loads(request.body)
            imported_count = 0
            
            for keyword_data in keywords_data:
                keyword = keyword_data.get('keyword')
                if keyword:
                    # Check if keyword already exists
                    if not TargetedKeyword.objects.filter(client=client, keyword=keyword).exists():
                        # Create the keyword
                        keyword_obj = TargetedKeyword.objects.create(
                            client=client,
                            keyword=keyword,
                            notes=f"""Imported from Search Console
Initial Position: {keyword_data.get('position', 'N/A')}
Clicks: {keyword_data.get('clicks', 0)}
Impressions: {keyword_data.get('impressions', 0)}
CTR: {keyword_data.get('ctr', 0)}%"""
                        )

                        # Create initial ranking history entry
                        KeywordRankingHistory.objects.create(
                            keyword=keyword_obj,
                            client=client,
                            keyword_text=keyword,
                            date=datetime.now().date(),
                            average_position=keyword_data.get('position', 0),
                            clicks=keyword_data.get('clicks', 0),
                            impressions=keyword_data.get('impressions', 0),
                            ctr=keyword_data.get('ctr', 0)
                        )
                        
                        imported_count += 1
            
            user_activity_tool.run(
                request.user, 
                'import', 
                f"Imported {imported_count} keywords from Search Console", 
                client=client
            )
            
            return JsonResponse({
                'success': True,
                'message': f'Successfully imported {imported_count} keywords'
            })
            
        except json.JSONDecodeError:
            return JsonResponse({
                'success': False,
                'error': 'Invalid data format'
            }, status=400)
        except Exception as e:
            logger.error(f"Error importing keywords: {str(e)}")
            return JsonResponse({
                'success': False,
                'error': str(e)
            }, status=500)
            
    return JsonResponse({
        'success': False,
        'error': 'Invalid request method'
    }, status=405)

@login_required
def search_console_keywords(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    search_console_data = []
    
    try:
        sc_credentials = SearchConsoleCredentials.objects.get(client=client)
        if sc_credentials:
            service = sc_credentials.get_service()
            if service:
                property_url = sc_credentials.get_property_url()
                if property_url:
                    # Get last 90 days of data
                    end_date = datetime.now().strftime('%Y-%m-%d')
                    start_date = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')
                    search_console_data = get_search_console_data(
                        service, 
                        property_url,
                        start_date,
                        end_date
                    )
    except SearchConsoleCredentials.DoesNotExist:
        pass
    except Exception as e:
        logger.error(f"Error fetching search console data: {str(e)}")
    
    context = {
        'page_title': 'Search Console Keywords',
        'client': client,
        'search_console_data': search_console_data,
    }
    return render(request, 'seo_manager/keywords/search_console_keywords.html', context)

================
File: seo_manager/views/meta_tags_views.py
================
import json
import os
from django.shortcuts import get_object_or_404, render
from django.contrib.auth.decorators import login_required
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from django.conf import settings
from ..models import Client
from ..sitemap_extractor import extract_sitemap_and_meta_tags, extract_sitemap_and_meta_tags_from_url

def get_snapshot_stats(file_path):
    """Get statistics from a meta tags snapshot file"""
    try:
        if not os.path.exists(file_path):
            return {
                'total_pages': 0,
                'total_tags': 0,
                'issues': 0
            }

        # Check file extension
        if file_path.endswith('.csv'):
            import csv
            total_pages = 0
            total_tags = 0
            issues = 0
            
            with open(file_path, 'r') as f:
                csv_reader = csv.DictReader(f)
                for row in csv_reader:
                    total_pages += 1
                    # Assuming meta tags are comma-separated in a column
                    if 'meta_tags' in row:
                        tags = row['meta_tags'].split(',')
                        total_tags += len(tags)
                    # Count issues if there's an issues column
                    if 'issues' in row and row['issues']:
                        issues += 1
                        
            return {
                'total_pages': total_pages,
                'total_tags': total_tags,
                'issues': issues
            }
        else:
            # Handle JSON files as before
            with open(file_path, 'r') as f:
                data = json.load(f)
                total_pages = len(data.get('pages', []))
                total_tags = sum(len(page.get('meta_tags', [])) for page in data.get('pages', []))
                issues = sum(1 for page in data.get('pages', []) 
                            for tag in page.get('meta_tags', []) 
                            if tag.get('issues'))
                return {
                    'total_pages': total_pages,
                    'total_tags': total_tags,
                    'issues': issues
                }
    except Exception as e:
        print(f"Error reading meta tags file: {file_path}")
        print(f"Error details: {str(e)}")
        return {
            'total_pages': 0,
            'total_tags': 0,
            'issues': 0
        }

@login_required
def meta_tags_dashboard(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    # Get list of meta tags files for this client
    meta_tags_dir = os.path.join(settings.MEDIA_ROOT, str(request.user.id), 'meta-tags')
    meta_tags_files = []
    latest_stats = None
    
    if os.path.exists(meta_tags_dir):
        meta_tags_files = sorted(
            [os.path.join(str(request.user.id), 'meta-tags', f) 
             for f in os.listdir(meta_tags_dir) if f.endswith('.csv')],
            reverse=True
        )
        
        # Get stats for the latest snapshot
        if meta_tags_files:
            latest_file = os.path.join(settings.MEDIA_ROOT, meta_tags_files[0])
            latest_stats = get_snapshot_stats(latest_file)

    context = {
        'page_title': 'Meta Tags Dashboard',
        'client': client,
        'meta_tags_files': meta_tags_files,
        'latest_stats': latest_stats
    }
    
    return render(request, 'seo_manager/meta_tags/meta_tags_dashboard.html', context)

@login_required
def create_meta_tags_snapshot(request, client_id):
    if request.method == 'POST':
        client = get_object_or_404(Client, id=client_id)
        try:
            file_path = extract_sitemap_and_meta_tags(client, request.user)
            print(f"File being saved at: {file_path}")
            
            # Ensure the directory exists
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            return JsonResponse({
                'success': True,
                'message': f"Meta tags snapshot created successfully. File saved as {os.path.basename(file_path)}"
            })
        except Exception as e:
            import traceback
            print(traceback.format_exc())  # This will help debug any errors
            return JsonResponse({
                'success': False,
                'message': f"An error occurred while creating the snapshot: {str(e)}"
            })
    else:
        return JsonResponse({
            'success': False,
            'message': "Invalid request method."
        })

@login_required
@require_http_methods(["POST"])
def create_meta_tags_snapshot_url(request):
    data = json.loads(request.body)
    url = data.get('url')
    if not url:
        return JsonResponse({
            'success': False,
            'message': "URL is required."
        })
    
    try:
        file_path = extract_sitemap_and_meta_tags_from_url(url, request.user)
        return JsonResponse({
            'success': True,
            'message': f"Meta tags snapshot created successfully. File saved as {os.path.basename(file_path)}"
        })
    except Exception as e:
        return JsonResponse({
            'success': False,
            'message': f"An error occurred while creating the snapshot: {str(e)}"
        })

================
File: seo_manager/views/project_views.py
================
from datetime import timedelta
from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required
from django.contrib.auth.mixins import LoginRequiredMixin
from django.contrib import messages
from django.views.generic import ListView, CreateView, DetailView
from django.urls import reverse_lazy
from django.db.models import Avg
import json
from ..models import Client, SEOProject
from ..forms import SEOProjectForm
from apps.common.tools.user_activity_tool import user_activity_tool

class ProjectListView(LoginRequiredMixin, ListView):
    template_name = 'seo_manager/projects/project_list.html'
    context_object_name = 'projects'

    def get_queryset(self):
        return SEOProject.objects.filter(client_id=self.kwargs['client_id'])

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['client'] = get_object_or_404(Client, id=self.kwargs['client_id'])
        return context

class ProjectCreateView(LoginRequiredMixin, CreateView):
    model = SEOProject
    form_class = SEOProjectForm
    template_name = 'seo_manager/projects/project_form.html'

    def get_form_kwargs(self):
        kwargs = super().get_form_kwargs()
        kwargs['client'] = get_object_or_404(Client, id=self.kwargs['client_id'])
        return kwargs

    def form_valid(self, form):
        form.instance.client_id = self.kwargs['client_id']
        # Capture initial rankings for targeted keywords
        initial_rankings = {}
        for keyword in form.cleaned_data['targeted_keywords']:
            latest_ranking = keyword.ranking_history.first()
            if latest_ranking:
                initial_rankings[keyword.keyword] = latest_ranking.average_position
        form.instance.initial_rankings = initial_rankings
        
        response = super().form_valid(form)
        user_activity_tool.run(self.request.user, 'create', f"Created SEO project: {form.instance.title}", client=form.instance.client)
        return response

    def get_success_url(self):
        return reverse_lazy('seo_manager:client_detail', kwargs={'client_id': self.kwargs['client_id']})

class ProjectDetailView(LoginRequiredMixin, DetailView):
    model = SEOProject
    template_name = 'seo_manager/projects/project_detail.html'
    context_object_name = 'project'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        
        # Get the implementation date
        implementation_date = self.object.implementation_date
        
        # Calculate periods for comparison
        pre_period_start = implementation_date - timedelta(days=30)
        post_period_end = implementation_date + timedelta(days=30)
        
        # Prepare data for the ranking history chart and performance metrics
        ranking_data = {
            'labels': [],
            'datasets': []
        }
        
        performance_metrics = []
        
        for keyword in self.object.targeted_keywords.all():
            # Get rankings for before and after implementation
            rankings = keyword.ranking_history.filter(
                date__range=(pre_period_start, post_period_end)
            ).order_by('date')
            
            # Calculate average positions for before and after
            pre_avg = rankings.filter(
                date__lt=implementation_date
            ).aggregate(Avg('average_position'))['average_position__avg']
            
            post_avg = rankings.filter(
                date__gte=implementation_date
            ).aggregate(Avg('average_position'))['average_position__avg']
            
            # Calculate improvement
            improvement = pre_avg - post_avg if pre_avg and post_avg else None
            
            # Add to performance metrics
            performance_metrics.append({
                'keyword': keyword.keyword,
                'initial_position': self.object.initial_rankings.get(keyword.keyword),
                'current_position': keyword.ranking_history.first().average_position if keyword.ranking_history.exists() else None,
                'pre_avg': round(pre_avg, 1) if pre_avg else None,
                'post_avg': round(post_avg, 1) if post_avg else None,
                'improvement': round(improvement, 1) if improvement else None
            })
            
            # Prepare chart dataset
            dataset = {
                'label': keyword.keyword,
                'data': [],
                'borderColor': f'#{hash(keyword.keyword) % 0xFFFFFF:06x}',
                'tension': 0.4,
                'fill': False
            }
            
            for ranking in rankings:
                if ranking.date.isoformat() not in ranking_data['labels']:
                    ranking_data['labels'].append(ranking.date.isoformat())
                dataset['data'].append(ranking.average_position)
            
            ranking_data['datasets'].append(dataset)
        
        # Add implementation date marker to chart
        ranking_data['implementation_date'] = implementation_date.isoformat()
        
        context.update({
            'ranking_history_data': json.dumps(ranking_data),
            'performance_metrics': performance_metrics,
            'implementation_date': implementation_date,
            'pre_period_start': pre_period_start,
            'post_period_end': post_period_end
        })
        
        return context

@login_required
def edit_project(request, client_id, project_id):
    """View for editing an existing SEO project."""
    project = get_object_or_404(SEOProject, id=project_id, client_id=client_id)
    
    if request.method == 'POST':
        form = SEOProjectForm(request.POST, instance=project, client=project.client)
        if form.is_valid():
            form.save()
            messages.success(request, 'Project updated successfully.')
            return redirect('seo_manager:client_detail', client_id=client_id)
    else:
        form = SEOProjectForm(instance=project, client=project.client)
    
    context = {
        'page_title': 'Edit Project',
        'form': form,
        'project': project,
        'client_id': client_id,
    }
    
    return render(request, 'seo_manager/projects/edit_project.html', context)

@login_required
def delete_project(request, client_id, project_id):
    """View for deleting an SEO project."""
    project = get_object_or_404(SEOProject, id=project_id, client_id=client_id)
    
    if request.method == 'POST':
        project.delete()
        messages.success(request, 'Project deleted successfully.')
        return redirect('seo_manager:client_detail', client_id=client_id)
    
    return redirect('seo_manager:client_detail', client_id=client_id)

================
File: seo_manager/views/ranking_views.py
================
import csv
from django.shortcuts import get_object_or_404, redirect
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from django.http import JsonResponse, HttpResponse
from django.views.decorators.http import require_http_methods
from django.utils import timezone
from datetime import timedelta
from django.core.paginator import Paginator
from django.shortcuts import render
from django.db.models import Min, Max
from ..models import Client, KeywordRankingHistory
from ..forms import RankingImportForm
from apps.agents.tools.google_report_tool.google_rankings_tool import GoogleRankingsTool
import logging

logger = logging.getLogger(__name__)

@login_required
def ranking_import(request, client_id):
    if request.method == 'POST':
        form = RankingImportForm(request.POST, request.FILES)
        if form.is_valid():
            form.process_import(request.user)
            messages.success(request, "Rankings imported successfully.")
            return redirect('seo_manager:client_detail', client_id=client_id)
    else:
        form = RankingImportForm()
    
    return render(request, 'seo_manager/keywords/ranking_import.html', {
        'form': form,
        'client': get_object_or_404(Client, id=client_id)
    })

@login_required
@require_http_methods(["POST"])
def collect_rankings(request, client_id):
    try:
        tool = GoogleRankingsTool()
        # Get just the last 30 days of data
        end_date = timezone.now().date()
        start_date = end_date - timedelta(days=7)
        
        result = tool._run(
            start_date=start_date.strftime('%Y-%m-%d'),
            end_date=end_date.strftime('%Y-%m-%d'),
            client_id=client_id
        )
        
        if result.get('success'):
            # Only show success if we actually stored some data
            if result.get('stored_rankings_count', 0) > 0:
                messages.success(request, "Latest rankings collected successfully")
                return JsonResponse({
                    'success': True,
                    'message': "Latest rankings data has been collected and stored"
                })
            else:
                return JsonResponse({
                    'success': False,
                    'error': "No ranking data was collected. Please check your Search Console credentials."
                })
        else:
            error_msg = result.get('error', 'Failed to collect rankings')
            if 'invalid_grant' in error_msg or 'expired' in error_msg:
                error_msg = "Your Search Console access has expired. Please reconnect your Search Console account."
            
            return JsonResponse({
                'success': False,
                'error': error_msg
            })
    except Exception as e:
        logger.error(f"Error in collect_rankings view: {str(e)}")
        return JsonResponse({
            'success': False,
            'error': str(e)
        })

@login_required
@require_http_methods(["POST"])
def backfill_rankings(request, client_id):
    try:
        tool = GoogleRankingsTool()
        # Pass None for start_date and end_date to trigger 12-month backfill
        result = tool._run(
            start_date=None,
            end_date=None,
            client_id=client_id
        )
        
        if result['success']:
            messages.success(request, "Historical rankings collected successfully")
            return JsonResponse({
                'success': True,
                'message': "12 months of historical ranking data has been collected and stored"
            })
        else:
            return JsonResponse({
                'success': False,
                'error': result.get('error', 'Unknown error occurred')
            })
    except Exception as e:
        return JsonResponse({
            'success': False,
            'error': str(e)
        })

@login_required
def ranking_data_management(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    # Get ranking data statistics
    ranking_stats = KeywordRankingHistory.objects.filter(
        client_id=client_id
    ).aggregate(
        earliest_date=Min('date'),
        latest_date=Max('date')
    )
    
    latest_collection_date = ranking_stats['latest_date']

    # Calculate data coverage in months
    data_coverage_months = 0
    if ranking_stats['earliest_date'] and ranking_stats['latest_date']:
        date_diff = ranking_stats['latest_date'] - ranking_stats['earliest_date']
        data_coverage_months = round(date_diff.days / 30)
    
    # Get search query
    search_query = request.GET.get('search', '')
    
    # Get sort parameters
    sort_by = request.GET.get('sort', '-date')  # Default sort by date descending
    if sort_by.startswith('-'):
        order_by = sort_by
        sort_dir = 'desc'
    else:
        order_by = sort_by
        sort_dir = 'asc'
    
    # Get items per page
    items_per_page = int(request.GET.get('items', 25))
    
    # Get rankings with filtering, sorting and pagination
    rankings_list = KeywordRankingHistory.objects.filter(client_id=client_id)
    
    # Apply search filter if provided
    if search_query:
        rankings_list = rankings_list.filter(keyword_text__icontains=search_query)
    
    # Apply sorting
    rankings_list = rankings_list.order_by(order_by)
    
    paginator = Paginator(rankings_list, items_per_page)
    page = request.GET.get('page')
    rankings = paginator.get_page(page)
    
    # Count unique keywords
    tracked_keywords_count = KeywordRankingHistory.objects.filter(
        client_id=client_id
    ).values('keyword_text').distinct().count()
    
    context = {
        'page_title': 'Rankings',
        'client': client,
        'latest_collection_date': latest_collection_date,
        'data_coverage_months': data_coverage_months,
        'tracked_keywords_count': tracked_keywords_count,
        'rankings': rankings,
        'sort_by': sort_by,
        'sort_dir': sort_dir,
        'search_query': search_query,
        'items': items_per_page,
    }
    
    return render(request, 'seo_manager/ranking_data_management.html', context)

@login_required
def export_rankings_csv(request, client_id):
    # Get search query
    search_query = request.GET.get('search', '')
    
    # Get rankings
    rankings = KeywordRankingHistory.objects.filter(client_id=client_id)
    if search_query:
        rankings = rankings.filter(keyword_text__icontains=search_query)
    
    # Create CSV response
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = f'attachment; filename="rankings_{client_id}.csv"'
    
    writer = csv.writer(response)
    writer.writerow(['Keyword', 'Position', 'Change', 'Impressions', 'Clicks', 'CTR', 'Date'])
    
    for ranking in rankings:
        writer.writerow([
            ranking.keyword_text,
            ranking.average_position,
            ranking.position_change,
            ranking.impressions,
            ranking.clicks,
            f"{ranking.ctr:.2f}%",
            ranking.date.strftime("%Y-%m-%d")
        ])
    
    return response

================
File: seo_manager/views/report_views.py
================
import logging
from django.shortcuts import get_object_or_404
from django.contrib.auth.decorators import login_required
from django.http import JsonResponse
from django.views.decorators.http import require_http_methods
from django.template.loader import render_to_string
from django.utils import timezone
from dateutil.relativedelta import relativedelta
from ..models import Client

logger = logging.getLogger(__name__)

@login_required
@require_http_methods(["POST"])
def generate_report(request, client_id):
    try:
        client = get_object_or_404(Client.objects.select_related(), id=client_id)
        
        # Get the report data
        today = timezone.now().date()
        last_month = today - relativedelta(months=1)
        
        # Use select_related to optimize queries
        keywords = client.targeted_keywords.select_related().all()
        
        report = {
            'period': last_month.strftime('%B %Y'),
            'keywords': {
                'total': keywords.count(),
                'improved': 0,
                'declined': 0,
                'unchanged': 0
            },
            'top_improvements': [],
            'needs_attention': []
        }

        # Process keyword data
        for keyword in keywords:
            change = keyword.get_position_change()
            if change:
                if change > 0:
                    report['keywords']['improved'] += 1
                    if change > 5:
                        report['top_improvements'].append({
                            'keyword': keyword.keyword,
                            'improvement': change
                        })
                elif change < 0:
                    report['keywords']['declined'] += 1
                    if change < -5:
                        report['needs_attention'].append({
                            'keyword': keyword.keyword,
                            'decline': abs(change)
                        })
                else:
                    report['keywords']['unchanged'] += 1

        # Sort improvements and needs attention lists
        report['top_improvements'].sort(key=lambda x: x['improvement'], reverse=True)
        report['needs_attention'].sort(key=lambda x: x['decline'], reverse=True)

        # Limit to top 5 for each list
        report['top_improvements'] = report['top_improvements'][:5]
        report['needs_attention'] = report['needs_attention'][:5]

        # Render the report template
        report_html = render_to_string(
            'seo_manager/reports/monthly_report.html',
            {'report': report, 'client': client},
            request=request
        )

        return JsonResponse({
            'success': True,
            'report_html': report_html
        })
        
    except Exception as e:
        logger.error(f"Error generating report: {str(e)}")
        return JsonResponse({
            'success': False,
            'error': f"Error generating report: {str(e)}"
        })

================
File: seo_manager/views/search_console_views.py
================
from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from ..models import Client, SearchConsoleCredentials
from ..google_auth import get_google_auth_flow, get_search_console_properties
from apps.common.tools.user_activity_tool import user_activity_tool
import json
import logging

logger = logging.getLogger(__name__)

@login_required
def client_search_console(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    return render(request, 'seo_manager/client_search_console.html', {'client': client})

@login_required
def add_sc_credentials(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    # Check if client already has credentials
    if hasattr(client, 'sc_credentials'):
        messages.warning(request, "Search Console credentials already exist for this client. Remove them first to add new ones.")
        return redirect('seo_manager:client_detail', client_id=client.id)
    
    # Handle POST request for property selection
    if request.method == 'POST':
        selected_property = request.POST.get('selected_property')
        if selected_property:
            try:
                # Extract just the URL
                try:
                    property_data = json.loads(selected_property)
                    logger.info(f"property_data: {property_data}")
                    property_url = property_data['url']
                except (json.JSONDecodeError, KeyError):
                    property_url = selected_property

                logger.info(f"""
                    Storing Search Console credentials for {client.name}:
                    property_url: {property_url}
                    access_token: {bool(request.session.get('access_token'))}
                    refresh_token: {bool(request.session.get('refresh_token'))}
                    token_uri: {bool(request.session.get('token_uri'))}
                    client_id: {bool(request.session.get('client_id'))}
                    client_secret: {bool(request.session.get('client_secret'))}
                """)

                credentials = SearchConsoleCredentials.objects.update_or_create(
                    client=client,
                    defaults={
                        'property_url': property_url,
                        'access_token': request.session.get('access_token'),
                        'refresh_token': request.session.get('refresh_token'),
                        'token_uri': request.session.get('token_uri'),
                        'sc_client_id': request.session.get('client_id'),
                        'client_secret': request.session.get('client_secret'),
                    }
                )[0]
                user_activity_tool.run(request.user, 'create', f"Added Search Console credentials for client: {client.name}", client=client)
                messages.success(request, "Search Console credentials added successfully.")
                
                # Clean up session
                for key in ['properties', 'access_token', 'refresh_token', 'token_uri', 'client_id', 'client_secret']:
                    request.session.pop(key, None)
                
                return redirect('seo_manager:client_detail', client_id=client.id)
            except Exception as e:
                messages.error(request, f"Error saving Search Console credentials: {str(e)}")
        else:
            messages.error(request, "Please select a property.")
    
    # If we have properties in session, show selection page
    if 'properties' in request.session:
        return render(request, 'seo_manager/select_search_console_property.html', {
            'client': client,
            'properties': request.session['properties'],
        })
    
    # Start OAuth flow if no properties in session
    flow = get_google_auth_flow(request)
    authorization_url, state = flow.authorization_url(
        access_type='offline',
        include_granted_scopes='true',
        state=f"{client_id}_sc",
        prompt='consent'
    )
    request.session['oauth_state'] = state
    return redirect(authorization_url)

@login_required
def remove_sc_credentials(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    try:
        if hasattr(client, 'sc_credentials'):
            client.sc_credentials.delete()
            user_activity_tool.run(request.user, 'delete', f"Removed Search Console credentials for client: {client.name}", client=client)
            messages.success(request, "Search Console credentials removed successfully.")
        else:
            messages.warning(request, "No Search Console credentials found for this client.")
    except Exception as e:
        messages.error(request, f"Error removing Search Console credentials: {str(e)}")
    
    for key in ['properties', 'access_token', 'refresh_token', 'token_uri', 'client_id', 'client_secret']:
        request.session.pop(key, None)
    
    return redirect('seo_manager:client_detail', client_id=client.id)

@login_required
def add_sc_credentials_service_account(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    # Clear any existing session data
    for key in ['properties', 'service_account_json']:
        request.session.pop(key, None)
    
    if request.method == 'POST':
        if 'selected_property' in request.POST:
            selected_property = request.POST.get('selected_property')
            if selected_property:
                properties = request.session.get('properties', [])
                property_data = next((prop for prop in properties if prop['url'] == selected_property), None)
                if property_data:
                    SearchConsoleCredentials.objects.update_or_create(
                        client=client,
                        defaults={
                            'service_account_json': request.session.get('service_account_json', ''),
                            'property_url': property_data['url'],
                        }
                    )
                    user_activity_tool.run(request.user, 'create', f"Added Search Console credentials (Service Account) for client: {client.name}", client=client)
                    messages.success(request, "Search Console credentials (Service Account) added successfully.")
                    return redirect('seo_manager:client_detail', client_id=client.id)
                else:
                    messages.error(request, "Selected property not found. Please try again.")
            else:
                messages.error(request, "Please select a property.")
        elif 'service_account_file' in request.FILES:
            service_account_file = request.FILES['service_account_file']
            try:
                service_account_info = json.load(service_account_file)
                service_account_json = json.dumps(service_account_info)
                properties = get_search_console_properties(service_account_json)
                request.session['properties'] = properties
                request.session['service_account_json'] = service_account_json
                return render(request, 'seo_manager/select_search_console_property.html', {
                    'client': client,
                    'properties': properties,
                })
            except json.JSONDecodeError:
                messages.error(request, "Invalid JSON file. Please upload a valid service account JSON file.")
        else:
            messages.error(request, "No file uploaded. Please select a service account JSON file.")
    
    # If no POST or no properties in session, show the upload form
    return render(request, 'seo_manager/add_sc_credentials_service_account.html', {'client': client})

def get_search_console_data(service, property_url, start_date, end_date):
    try:
        response = service.searchanalytics().query(
            siteUrl=property_url,
            body={
                'startDate': start_date,
                'endDate': end_date,
                'dimensions': ['query'],
                'rowLimit': 1000
            }
        ).execute()
        
        search_console_data = []
        for row in response.get('rows', []):
            search_console_data.append({
                'query': row['keys'][0],
                'clicks': row['clicks'],
                'impressions': row['impressions'],
                'ctr': row['ctr'] * 100,  # Convert to percentage
                'position': row['position']
            })
        
        search_console_data.sort(key=lambda x: x['impressions'], reverse=True)
        
        return search_console_data
    except Exception as e:
        logger.error(f"An error occurred: {str(e)}")
        return []

__all__ = [
    'client_search_console',
    'add_sc_credentials',
    'add_sc_credentials_service_account',
    'remove_sc_credentials'
]

================
File: seo_manager/admin.py
================
from django.contrib import admin
from .models import ClientGroup, Client, SEOData, AIProvider

@admin.register(ClientGroup)
class ClientGroupAdmin(admin.ModelAdmin):
    list_display = ('name', 'parent')
    search_fields = ('name',)

@admin.register(Client)
class ClientAdmin(admin.ModelAdmin):
    list_display = ('name', 'website_url', 'status', 'group')
    list_filter = ('status', 'group')
    search_fields = ('name', 'website_url')

@admin.register(SEOData)
class SEODataAdmin(admin.ModelAdmin):
    list_display = ('client', 'date', 'traffic', 'keywords')
    list_filter = ('client', 'date')
    date_hierarchy = 'date'

@admin.register(AIProvider)
class AIProviderAdmin(admin.ModelAdmin):
    list_display = ('name', 'model', 'is_active')
    list_filter = ('is_active',)
    search_fields = ('name', 'model')

================
File: seo_manager/apps.py
================
from django.apps import AppConfig

class SeoManagerConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'apps.seo_manager'

================
File: seo_manager/exceptions.py
================
class AuthError(Exception):
    """Custom exception for authentication errors"""
    pass

================
File: seo_manager/forms.py
================
from django import forms
from django.core.exceptions import ValidationError
from .models import Client, TargetedKeyword, KeywordRankingHistory, SEOProject
import csv
import io
from django.utils import timezone

class ClientForm(forms.ModelForm):
    class Meta:
        model = Client
        fields = ['name', 'website_url', 'status', 'group', 'target_audience']
        widgets = {
            'name': forms.TextInput(attrs={
                'class': 'form-control',
                'placeholder': 'Enter client name'
            }),
            'website_url': forms.URLInput(attrs={
                'class': 'form-control',
                'placeholder': 'https://example.com'
            }),
            'status': forms.Select(attrs={
                'class': 'form-select'
            }),
            'group': forms.Select(attrs={
                'class': 'form-select'
            }),
            'target_audience': forms.Textarea(attrs={
                'class': 'form-control',
                'rows': 3,
                'placeholder': 'Describe the target audience'
            }),
        }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Add any additional field customization here
        for field in self.fields.values():
            if not isinstance(field.widget, forms.CheckboxInput):
                field.widget.attrs.update({
                    'class': field.widget.attrs.get('class', '') + ' form-control'
                })

class BusinessObjectiveForm(forms.Form):
    goal = forms.CharField(required=True)
    metric = forms.CharField(required=True)
    target_date = forms.DateField(required=True)
    status = forms.BooleanField(required=False, initial=True)

    def clean_target_date(self):
        target_date = self.cleaned_data.get('target_date')
        if target_date and target_date < timezone.now().date():
            raise ValidationError("Target date cannot be in the past")
        return target_date

class TargetedKeywordForm(forms.ModelForm):
    class Meta:
        model = TargetedKeyword
        fields = ['keyword', 'priority', 'notes']
        widgets = {
            'keyword': forms.TextInput(attrs={'class': 'form-control'}),
            'priority': forms.Select(attrs={'class': 'form-select'}),
            'notes': forms.Textarea(attrs={'class': 'form-control', 'rows': 3}),
        }

class KeywordBulkUploadForm(forms.Form):
    csv_file = forms.FileField(
        label='CSV File',
        help_text='Upload a CSV file with columns: keyword, priority (1-5), notes (optional)',
        widget=forms.FileInput(attrs={'class': 'form-control'})
    )

    def clean_csv_file(self):
        csv_file = self.cleaned_data['csv_file']
        if not csv_file.name.endswith('.csv'):
            raise ValidationError('File must be a CSV')
        
        # Validate CSV structure
        try:
            decoded_file = csv_file.read().decode('utf-8')
            csv_data = csv.DictReader(io.StringIO(decoded_file))
            required_fields = ['keyword', 'priority']
            
            if not all(field in csv_data.fieldnames for field in required_fields):
                raise ValidationError('CSV must contain keyword and priority columns')
            
            # Reset file pointer
            csv_file.seek(0)
            return csv_file
        except Exception as e:
            raise ValidationError(f'Invalid CSV file: {str(e)}')

class RankingImportForm(forms.Form):
    IMPORT_SOURCE_CHOICES = [
        ('search_console', 'Google Search Console'),
        ('csv', 'CSV Upload'),
        ('manual', 'Manual Entry'),
    ]

    import_source = forms.ChoiceField(
        choices=IMPORT_SOURCE_CHOICES,
        widget=forms.RadioSelect(attrs={'class': 'form-check-input'})
    )
    
    date_from = forms.DateField(
        widget=forms.DateInput(attrs={
            'class': 'form-control flatpickr-date',
            'data-toggle': 'flatpickr'
        })
    )
    
    date_to = forms.DateField(
        widget=forms.DateInput(attrs={
            'class': 'form-control flatpickr-date',
            'data-toggle': 'flatpickr'
        })
    )
    
    csv_file = forms.FileField(
        required=False,
        widget=forms.FileInput(attrs={'class': 'form-control'})
    )

    def clean(self):
        cleaned_data = super().clean()
        import_source = cleaned_data.get('import_source')
        csv_file = cleaned_data.get('csv_file')

        if import_source == 'csv' and not csv_file:
            raise ValidationError('CSV file is required when importing from CSV')

        return cleaned_data

    def process_import(self, user):
        import_source = self.cleaned_data['import_source']
        
        if import_source == 'search_console':
            return self._process_search_console_import()
        elif import_source == 'csv':
            return self._process_csv_import()
        else:
            return self._process_manual_entry()

    def _process_search_console_import(self):
        # Implementation for Search Console import
        pass

    def _process_csv_import(self):
        # Implementation for CSV import
        pass

    def _process_manual_entry(self):
        # Implementation for manual entry
        pass

class SEOProjectForm(forms.ModelForm):
    class Meta:
        model = SEOProject
        fields = ['title', 'description', 'status', 
                 'implementation_date', 'completion_date', 
                 'targeted_keywords']
        widgets = {
            'implementation_date': forms.DateInput(attrs={
                'class': 'form-control datepicker',
                'type': 'date'
            }),
            'completion_date': forms.DateInput(attrs={
                'class': 'form-control datepicker',
                'type': 'date'
            }),
            'description': forms.Textarea(attrs={
                'rows': 3, 
                'class': 'form-control'
            }),
            'title': forms.TextInput(attrs={
                'class': 'form-control'
            }),
            'status': forms.Select(attrs={
                'class': 'form-control'
            }),
            'targeted_keywords': forms.SelectMultiple(attrs={
                'class': 'form-control'
            }),
        }

    def __init__(self, *args, **kwargs):
        client = kwargs.pop('client', None)
        super().__init__(*args, **kwargs)
        
        if client:
            self.fields['targeted_keywords'].queryset = TargetedKeyword.objects.filter(
                client=client
            ).order_by('keyword')

    def clean(self):
        cleaned_data = super().clean()
        implementation_date = cleaned_data.get('implementation_date')
        completion_date = cleaned_data.get('completion_date')

        if completion_date and implementation_date and completion_date < implementation_date:
            raise ValidationError({
                'completion_date': 'Completion date cannot be earlier than implementation date.'
            })

        return cleaned_data

class ClientProfileForm(forms.Form):
    client_profile = forms.CharField(
        widget=forms.Textarea(attrs={
            'class': 'form-control',
            'rows': 10,
            'placeholder': 'Enter a detailed 300-500 word profile of the client\'s business, goals, and SEO strategy'
        }),
        help_text="Provide a comprehensive overview of the client's business, target market, goals, and SEO strategy."
    )

================
File: seo_manager/google_auth.py
================
from google_auth_oauthlib.flow import Flow
from django.conf import settings
from django.urls import reverse
from google.oauth2.credentials import Credentials
from google.oauth2 import service_account
from googleapiclient.discovery import build
import json
import logging

logger = logging.getLogger(__name__)

def get_google_auth_flow(request):
    """
    Creates OAuth2 flow for Google Analytics and Search Console authentication.
    """
    flow = Flow.from_client_secrets_file(
        settings.GOOGLE_CLIENT_SECRETS_FILE,
        scopes=[
            'https://www.googleapis.com/auth/analytics.readonly',  # GA4 scope
            'https://www.googleapis.com/auth/webmasters.readonly',
            'openid',
            'https://www.googleapis.com/auth/userinfo.email',
            'https://www.googleapis.com/auth/userinfo.profile'
        ],
        redirect_uri=request.build_absolute_uri('/google/login/callback/')
    )
    return flow

def get_analytics_accounts_oauth(credentials):
    """Get GA4 accounts using OAuth credentials"""
    try:
        # Create the Analytics Admin API client
        analytics = build('analyticsadmin', 'v1beta', credentials=credentials)
        
        try:
            # First get accounts
            accounts_request = analytics.accounts().list().execute()
            accounts = []
            
            # For each account, get its properties
            for account in accounts_request.get('accounts', []):
                account_id = account['name']  # Format: "accounts/1234567"
                account_name = account.get('displayName', 'Unknown Account')
                #logger.info(f"Fetching properties for account: {account_name}")
                
                try:
                    # Initialize pagination variables
                    page_token = None
                    page_num = 1
                    properties_count = 0
                    
                    while True:
                        # List properties for this account with pagination
                        request_params = {
                            'filter': f'parent:{account_id}',
                            'pageSize': 200  # Maximum allowed page size
                        }
                        if page_token:
                            request_params['pageToken'] = page_token
                            
                        properties_request = analytics.properties().list(**request_params).execute()
                        
                        for property in properties_request.get('properties', []):
                            property_id = property['name']
                            property_name = property.get('displayName', 'Unknown Property')
                            
                            accounts.append({
                                'property_id': property_id,
                                'property_name': property_name,
                                'account_name': account_name
                            })
                            properties_count += 1
                        
                        # Check if there are more pages
                        page_token = properties_request.get('nextPageToken')
                        if not page_token:
                            break
                            
                        page_num += 1
                    
                    # logger.info(f"Found {properties_count} properties in account: {account_name}")
                        
                except Exception as e:
                    logger.error(f"Error listing properties for account {account_id}: {str(e)}", exc_info=True)
                    continue
            
            logger.info(f"Total GA4 properties found across all accounts: {len(accounts)}")
            return accounts
            
        except Exception as e:
            logger.error(f"Error listing GA4 accounts: {str(e)}", exc_info=True)
            return []
            
    except Exception as e:
        logger.error(f"Error building GA4 service: {str(e)}", exc_info=True)
        return []

def get_analytics_accounts_service_account(service_account_json):
    """Get GA4 properties using service account credentials"""
    try:
        credentials = service_account.Credentials.from_service_account_info(
            json.loads(service_account_json),
            scopes=['https://www.googleapis.com/auth/analytics.readonly']
        )
        analytics = build('analyticsadmin', 'v1beta', credentials=credentials)
        return fetch_analytics_accounts(analytics)
    except Exception as e:
        logger.error(f"Error building GA4 service with service account: {str(e)}")
        return []

def fetch_analytics_accounts(analytics):
    """Fetch GA4 properties using the Analytics Admin API"""
    try:
        # List all GA4 properties accessible to the user
        request = analytics.accounts().list()
        response = request.execute()
        
        accounts = []
        for account in response.get('accounts', []):
            account_id = account['name'].split('/')[-1]  # Format: "accounts/123456"
            account_name = account.get('displayName', 'Unknown Account')
            
            # Get properties for this account using the correct API call
            properties_request = analytics.properties().list(
                filter=f"parent:accounts/{account_id}"  # Changed from parent parameter to filter
            )
            properties_response = properties_request.execute()
            
            for property in properties_response.get('properties', []):
                property_id = property['name'].split('/')[-1]  # Format: "properties/123456"
                accounts.append({
                    'account_id': account_id,
                    'account_name': account_name,
                    'property_id': property_id,
                    'property_name': property.get('displayName', 'Unknown Property')
                })
        
        logger.info(f"Found {len(accounts)} GA4 properties")
        if not accounts:
            logger.warning("No GA4 properties found for this account")
            
        return accounts
        
    except Exception as e:
        logger.error(f"Error fetching GA4 properties: {str(e)}")
        return []

def get_search_console_service(credentials):
    return build('searchconsole', 'v1', credentials=credentials)

def get_search_console_properties(credentials_or_json):
    """Get list of Search Console properties"""
    try:
        if isinstance(credentials_or_json, str):
            service_account_info = json.loads(credentials_or_json)
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info,
                scopes=['https://www.googleapis.com/auth/webmasters.readonly']
            )
        else:
            credentials = credentials_or_json

        service = build('searchconsole', 'v1', credentials=credentials)
        sites = service.sites().list().execute()
        
        properties = []
        if 'siteEntry' in sites:
            for site in sites['siteEntry']:
                # logger.info(f"Found Search Console site: {site}")
                properties.append({
                    'url': site['siteUrl'],
                    'permission_level': site.get('permissionLevel', '')
                })
        
        return properties

    except Exception as e:
        logger.error(f"Error getting Search Console properties: {str(e)}")
        raise

================
File: seo_manager/middleware.py
================
from .exceptions import AuthError
from django.shortcuts import redirect
from django.contrib import messages

class GoogleAuthMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response

    def __call__(self, request):
        response = self.get_response(request)
        return response

    def process_exception(self, request, exception):
        if isinstance(exception, AuthError):
            messages.error(request, str(exception))
            return redirect('seo_manager:client_list')
        return None

================
File: seo_manager/models.py
================
from django.db import models
from django.contrib.auth.models import User
from django.utils import timezone
from django.db.models import Q
from django.db.models import Avg
import logging
from dateutil.relativedelta import relativedelta
import json
from datetime import datetime
from google.oauth2.credentials import Credentials
from google.oauth2 import service_account
from google.analytics.data_v1beta import BetaAnalyticsDataClient
from googleapiclient.discovery import build
import google.auth.transport.requests
from google.auth.transport.requests import Request
from google_auth_oauthlib.flow import Flow
from django.urls import reverse
from django.conf import settings
from .exceptions import AuthError

logger = logging.getLogger(__name__)

class ClientGroup(models.Model):
    name = models.CharField(max_length=100)
    parent = models.ForeignKey('self', on_delete=models.SET_NULL, null=True, blank=True, related_name='children')

    def __str__(self):
        return self.name

class Client(models.Model):
    STATUS_CHOICES = [
        ('active', 'Active'),
        ('inactive', 'Inactive'),
        ('on_hold', 'On Hold'),
    ]

    name = models.CharField(max_length=100)
    website_url = models.URLField()
    status = models.CharField(max_length=10, choices=STATUS_CHOICES, default='active')
    group = models.ForeignKey(ClientGroup, on_delete=models.SET_NULL, null=True, blank=True, related_name='clients')
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    business_objectives = models.JSONField(default=list, blank=True)
    target_audience = models.TextField(blank=True, null=True)
    # New field
    client_profile = models.TextField(
        help_text="Detailed 300-500 word profile of the client's business, goals, and SEO strategy",
        blank=True
    )
    distilled_website = models.TextField(
        help_text="Distilled version of the client's website content for SEO purposes",
        blank=True
    )
    distilled_website_date = models.DateTimeField(
        null=True,
        blank=True,
        help_text="The last time the distilled website content was modified or created"
    )

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.business_objectives is None:
            self.business_objectives = []

    def __str__(self):
        return self.name
    
    def save(self, *args, **kwargs):
        if self.pk:
            old_client = Client.objects.get(pk=self.pk)
            if old_client.distilled_website != self.distilled_website:
                self.distilled_website_date = timezone.now()
        else:
            if self.distilled_website:
                self.distilled_website_date = timezone.now()
        super().save(*args, **kwargs)
        
    def get_keyword_rankings_summary(self):
        """Get summary of current keyword rankings"""
        latest_rankings = KeywordRankingHistory.objects.filter(
            client=self,
            keyword__isnull=False,  # Only targeted keywords
            date=KeywordRankingHistory.objects.filter(
                client=self,
                keyword__isnull=False
            ).values('date').order_by('-date').first()['date']
        ).select_related('keyword')

        return {
            'total_keywords': latest_rankings.count(),
            'avg_position': latest_rankings.aggregate(
                Avg('average_position')
            )['average_position__avg'],
            'rankings': latest_rankings
        }

class SEOData(models.Model):
    client = models.ForeignKey(Client, on_delete=models.CASCADE, related_name='seo_data')
    date = models.DateField()
    traffic = models.IntegerField()
    keywords = models.IntegerField()
    rankings = models.JSONField()  # Store rankings as JSON

    class Meta:
        unique_together = ['client', 'date']

class AIProvider(models.Model):
    name = models.CharField(max_length=100)
    api_key = models.CharField(max_length=255)
    model = models.CharField(max_length=100)
    parameters = models.JSONField(default=dict)
    is_active = models.BooleanField(default=True)

    def __str__(self):
        return self.name

class OAuthManager:
    """Manages OAuth operations for Google services"""
    
    @staticmethod
    def create_oauth_flow(request, state_key=None):
        """Create OAuth flow for Google services"""
        logger.info(f"Creating OAuth flow with state key: {state_key}")
        
        # Get the current domain and scheme
        scheme = request.scheme
        domain = request.get_host()
        
        # Use the same redirect URI that's configured in Google Console
        redirect_uri = f'{scheme}://{domain}/google/login/callback/'
        
        logger.info(f"Using redirect URI: {redirect_uri}")
        
        try:
            flow = Flow.from_client_secrets_file(
                settings.GOOGLE_CLIENT_SECRETS_FILE,
                scopes=[
                    'https://www.googleapis.com/auth/analytics.readonly',
                    'https://www.googleapis.com/auth/webmasters.readonly',
                    'openid',
                    'https://www.googleapis.com/auth/userinfo.email',
                    'https://www.googleapis.com/auth/userinfo.profile'
                ],
                state=state_key,
                redirect_uri=redirect_uri
            )
            
            # Store the redirect URI in session
            request.session['oauth_redirect_uri'] = redirect_uri
            request.session.modified = True  # Ensure session is saved
            
            logger.info(f"OAuth flow created successfully with redirect URI: {redirect_uri}")
            return flow
            
        except Exception as e:
            logger.error(f"Error creating OAuth flow: {str(e)}", exc_info=True)
            raise
    
    @staticmethod
    def handle_oauth_callback(request, code, state):
        """Handle OAuth callback and return credentials"""
        try:
            # Get the stored redirect URI from session
            redirect_uri = request.session.get('oauth_redirect_uri')
            if not redirect_uri:
                raise AuthError("Missing redirect URI in session")
            
            flow = OAuthManager.create_oauth_flow(request, state_key=state)
            
            # Ensure the redirect URI matches
            flow.redirect_uri = redirect_uri
            
            # Fetch token with explicit redirect URI
            credentials = flow.fetch_token(
                code=code,
                redirect_uri=redirect_uri
            )
            
            # Clean up session
            request.session.pop('oauth_redirect_uri', None)
            
            return credentials
            
        except Exception as e:
            logger.error(f"OAuth callback error: {str(e)}")
            raise AuthError("Failed to complete OAuth flow")
    
    @staticmethod
    def credentials_to_dict(credentials):
        """Convert OAuth credentials to dictionary for session storage"""
        if hasattr(credentials, 'token'):
            # Handle Google OAuth2Credentials
            return {
                'token': credentials.token,
                'refresh_token': credentials.refresh_token,
                'token_uri': credentials.token_uri,
                'client_id': credentials.client_id,
                'client_secret': credentials.client_secret,
                'scopes': credentials.scopes
            }
        else:
            # Handle OAuth2Token
            return {
                'token': credentials['access_token'],
                'refresh_token': credentials.get('refresh_token'),
                'token_uri': 'https://oauth2.googleapis.com/token',  # Standard Google OAuth2 token endpoint
                'client_id': credentials.get('client_id'),
                'client_secret': credentials.get('client_secret'),
                'scopes': credentials.get('scope', '').split(' ') if isinstance(credentials.get('scope'), str) else credentials.get('scope', [])
            }

class GoogleAnalyticsCredentials(models.Model):
    client = models.OneToOneField(Client, on_delete=models.CASCADE, related_name='ga_credentials')
    view_id = models.CharField(max_length=100, blank=True, null=True)
    access_token = models.TextField(blank=True, null=True)
    refresh_token = models.TextField(blank=True, null=True)
    token_uri = models.URLField(blank=True, null=True)
    ga_client_id = models.CharField(max_length=100, blank=True, null=True)
    client_secret = models.CharField(max_length=100, blank=True, null=True)
    use_service_account = models.BooleanField(default=False)
    service_account_json = models.TextField(blank=True, null=True)
    user_email = models.EmailField()
    scopes = models.JSONField(default=list)

    def __str__(self):
        return f"GA Credentials for {self.client.name}"

    @property
    def required_scopes(self):
        return ['https://www.googleapis.com/auth/analytics.readonly']

    def get_credentials(self):
        """Returns refreshed Google Analytics credentials"""
        try:
            if self.use_service_account and self.service_account_json:
                service_account_info = json.loads(self.service_account_json)
                return service_account.Credentials.from_service_account_info(
                    service_account_info,
                    scopes=['https://www.googleapis.com/auth/analytics.readonly']
                )

            # For OAuth, create credentials from stored values
            if not all([self.refresh_token, self.token_uri, self.ga_client_id, self.client_secret]):
                logger.error("Missing required OAuth fields")
                return None

            credentials = Credentials(
                token=self.access_token,
                refresh_token=self.refresh_token,
                token_uri=self.token_uri,
                client_id=self.ga_client_id,
                client_secret=self.client_secret,
                scopes=['https://www.googleapis.com/auth/analytics.readonly']
            )

            # Always try to refresh if we have a refresh token
            if credentials.refresh_token:
                request = google.auth.transport.requests.Request()
                credentials.refresh(request)
                self.access_token = credentials.token
                self.save(update_fields=['access_token'])
                logger.info(f"Refreshed access token for {self.client.name}")

            return credentials

        except Exception as e:
            logger.error(f"Error getting credentials: {str(e)}")
            if 'invalid_grant' in str(e):
                self.access_token = None
                self.refresh_token = None
                self.save(update_fields=['access_token', 'refresh_token'])
                raise AuthError("OAuth credentials expired. Please re-authenticate.")
            return None

    def get_property_id(self):
        """Get the clean property ID without 'properties/' prefix"""
        if self.view_id:
            return self.view_id.replace('properties/', '')
        return None

    def get_service(self):
        """Returns an authenticated Analytics service"""
        try:
            credentials = self.get_credentials()
            if not credentials:
                return None

            return BetaAnalyticsDataClient(credentials=credentials)

        except Exception as e:
            logger.error(f"Error creating Analytics service: {str(e)}")
            return None

    def refresh_credentials(self):
        """Handle token refresh according to OAuth 2.0 spec"""
        try:
            if not self.refresh_token:
                raise AuthError("No refresh token available. Re-authorization required.")

            credentials = Credentials(
                token=self.access_token,
                refresh_token=self.refresh_token,
                token_uri=self.token_uri,
                client_id=self.ga_client_id,
                client_secret=self.client_secret,
                scopes=self.required_scopes
            )

            request = google.auth.transport.requests.Request()
            credentials.refresh(request)
            
            # Update stored credentials
            self.access_token = credentials.token
            self.save(update_fields=['access_token'])
            
            return True
            
        except Exception as e:
            if 'invalid_grant' in str(e):
                # Clear credentials if refresh token is invalid
                self.access_token = None
                self.refresh_token = None
                self.save(update_fields=['access_token', 'refresh_token'])
                raise AuthError("Refresh token expired or revoked. Re-authorization required.")
            raise

    def save_oauth_credentials(self, credentials):
        """Save OAuth credentials from flow"""
        try:
            # Log the credentials we're trying to save
            logger.info(f"""
            Attempting to save OAuth credentials for {self.client.name}:
            Token: {'Present' if credentials.token else 'Missing'}
            Refresh Token: {'Present' if credentials.refresh_token else 'Missing'}
            Token URI: {credentials.token_uri if credentials.token_uri else 'Missing'}
            Client ID: {'Present' if credentials.client_id else 'Missing'}
            Client Secret: {'Present' if credentials.client_secret else 'Missing'}
            """)

            # Save all fields
            self.access_token = credentials.token
            self.refresh_token = credentials.refresh_token
            self.token_uri = credentials.token_uri
            self.ga_client_id = credentials.client_id
            self.client_secret = credentials.client_secret
            self.scopes = credentials.scopes
            self.use_service_account = False
            
            self.save()
            
            # Verify saved credentials
            saved_creds = self.get_credentials()
            if not saved_creds:
                raise ValueError("Failed to verify saved credentials")

            logger.info(f"Successfully saved and verified OAuth credentials for {self.client.name}")

        except Exception as e:
            logger.error(f"Error saving OAuth credentials: {str(e)}")
            raise

    def validate_credentials(self):
        """Validate stored credentials"""
        try:
            if not self.access_token and not self.refresh_token and not self.service_account_json:
                raise AuthError("No valid credentials found")
                
            credentials = self.get_credentials()
            if not credentials:
                raise AuthError("Failed to get valid credentials")
                
            return True
            
        except Exception as e:
            logger.error(f"Credential validation error: {str(e)}")
            raise AuthError(f"Credential validation failed: {str(e)}")

    def handle_oauth_response(self, credentials_dict):
        """Handle OAuth response and save credentials"""
        try:
            self.access_token = credentials_dict['token']
            self.refresh_token = credentials_dict['refresh_token']
            self.token_uri = credentials_dict['token_uri']
            self.ga_client_id = credentials_dict['client_id']
            self.client_secret = credentials_dict['client_secret']
            self.use_service_account = False
            self.scopes = credentials_dict['scopes']
            self.save()
            
            logger.info(f"Saved GA OAuth credentials for {self.client.name}")
            return True
            
        except Exception as e:
            logger.error(f"Error saving GA OAuth credentials: {str(e)}", exc_info=True)
            raise AuthError(f"Failed to save credentials: {str(e)}")

    def handle_service_account(self, service_account_json):
        """Handle service account setup"""
        try:
            self.service_account_json = service_account_json
            self.use_service_account = True
            self.access_token = None
            self.refresh_token = None
            self.save()
            
            # Validate the service account works
            credentials = self.get_credentials()
            if not credentials:
                raise AuthError("Invalid service account credentials")
                
            return True
            
        except Exception as e:
            logger.error(f"Service account setup error: {str(e)}")
            raise AuthError(f"Failed to setup service account: {str(e)}")

class SearchConsoleCredentials(models.Model):
    client = models.OneToOneField(Client, on_delete=models.CASCADE, related_name='sc_credentials')
    property_url = models.TextField()
    access_token = models.TextField(blank=True, null=True)
    refresh_token = models.TextField(blank=True, null=True)
    token_uri = models.URLField(blank=True, null=True)
    sc_client_id = models.CharField(max_length=100, blank=True, null=True)
    client_secret = models.CharField(max_length=100, blank=True, null=True)
    service_account_json = models.TextField(blank=True, null=True)
    last_validated = models.DateTimeField(auto_now=True)
    user_email = models.EmailField(blank=True, null=True)

    def __str__(self):
        return f"Search Console Credentials for {self.client.name}"

    def get_credentials(self):
        """Returns refreshed Google OAuth2 credentials"""
        try:
            # Handle service account authentication
            if self.service_account_json:
                service_account_info = json.loads(self.service_account_json)
                return service_account.Credentials.from_service_account_info(
                    service_account_info,
                    scopes=['https://www.googleapis.com/auth/webmasters.readonly']
                )

            # Handle OAuth2 authentication
            if not self.refresh_token:
                raise AuthError("No refresh token available. Reauthorization required.")

            credentials = Credentials(
                token=self.access_token,
                refresh_token=self.refresh_token,
                token_uri=self.token_uri,
                client_id=self.sc_client_id,
                client_secret=self.client_secret,
                scopes=['https://www.googleapis.com/auth/webmasters.readonly']
            )

            # Only refresh if token is expired or missing
            if not self.access_token or not credentials.valid:
                request = google.auth.transport.requests.Request()
                try:
                    credentials.refresh(request)
                    
                    # Update stored credentials
                    self.access_token = credentials.token
                    self.save(update_fields=['access_token'])
                    
                    logger.info(f"Successfully refreshed Search Console OAuth credentials for {self.client.name}")
                except Exception as e:
                    if 'invalid_grant' in str(e):
                        # Clear invalid credentials to force reauthorization
                        self.access_token = None
                        self.refresh_token = None
                        self.save(update_fields=['access_token', 'refresh_token'])
                        raise AuthError("Stored credentials are no longer valid. Please reauthorize Search Console access.")
                    raise

            return credentials

        except Exception as e:
            logger.error(f"Failed to refresh Search Console credentials for {self.client.name}: {str(e)}")
            raise AuthError(f"Failed to get valid Search Console credentials: {str(e)}")

    def get_service(self):
        """Returns an authenticated Search Console service"""
        try:
            credentials = self.get_credentials()
            if not credentials:
                logger.warning(f"No valid credentials available for {self.client.name}")
                return None
                
            return build('searchconsole', 'v1', credentials=credentials)
        except Exception as e:
            logger.error(f"Error creating Search Console service for {self.client.name}: {str(e)}")
            return None

    def get_property_url(self):
        """Parse and return the correct property URL format"""
        try:
            if isinstance(self.property_url, str):
                if '{' in self.property_url:
                    # Parse JSON-like string
                    data = json.loads(self.property_url.replace("'", '"'))
                    return data.get('url')  # Use get() to safely access 'url'
                return self.property_url
            elif isinstance(self.property_url, dict):
                return self.property_url.get('url')
            return self.property_url
        except Exception as e:
            logger.error(f"Error parsing property URL for {self.client.name}: {str(e)}")
            return None

    def save_oauth_credentials(self, credentials):
        """Save OAuth credentials from flow"""
        creds_dict = OAuthManager.credentials_to_dict(credentials)
        self.access_token = creds_dict['token']
        self.refresh_token = creds_dict['refresh_token']
        self.token_uri = creds_dict['token_uri']
        self.sc_client_id = creds_dict['client_id']
        self.client_secret = creds_dict['client_secret']
        self.save()

    def validate_credentials(self):
        """Validate stored credentials"""
        try:
            if not self.access_token and not self.refresh_token and not self.service_account_json:
                raise AuthError("No valid credentials found")
                
            credentials = self.get_credentials()
            if not credentials:
                raise AuthError("Failed to get valid credentials")
                
            return True
            
        except Exception as e:
            logger.error(f"Search Console credential validation error: {str(e)}")
            raise AuthError(f"Credential validation failed: {str(e)}")

    def handle_oauth_response(self, credentials_dict):
        """Handle OAuth response and save credentials"""
        try:
            self.access_token = credentials_dict['token']
            self.refresh_token = credentials_dict['refresh_token']
            self.token_uri = credentials_dict['token_uri']
            self.sc_client_id = credentials_dict['client_id']
            self.client_secret = credentials_dict['client_secret']
            self.save()
            
            logger.info(f"Saved SC OAuth credentials for {self.client.name}")
            return True
            
        except Exception as e:
            logger.error(f"Error saving SC OAuth credentials: {str(e)}", exc_info=True)
            raise AuthError(f"Failed to save credentials: {str(e)}")

class SummarizerUsage(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    query = models.TextField()
    compressed_content = models.TextField()
    response = models.TextField()
    created_at = models.DateTimeField(auto_now_add=True)
    duration = models.DurationField()
    content_token_size = models.IntegerField()
    content_character_count = models.IntegerField()
    total_input_tokens = models.IntegerField()
    total_output_tokens = models.IntegerField()
    model_used = models.CharField(max_length=100)

class UserActivity(models.Model):
    CATEGORY_CHOICES = [
        ('login', 'Login'),
        ('logout', 'Logout'),
        ('view', 'View'),
        ('create', 'Create'),
        ('update', 'Update'),
        ('delete', 'Delete'),
        ('export', 'Export'),
        ('import', 'Import'),
        ('other', 'Other'),
    ]

    user = models.ForeignKey(User, on_delete=models.CASCADE)
    client = models.ForeignKey(Client, on_delete=models.CASCADE, null=True, blank=True)
    category = models.CharField(max_length=20, choices=CATEGORY_CHOICES)
    action = models.CharField(max_length=255)
    timestamp = models.DateTimeField(auto_now_add=True)
    details = models.JSONField(null=True, blank=True)

    def __str__(self):
        return f"{self.user.username} - {self.category} - {self.action}"

class TargetedKeyword(models.Model):
    PRIORITY_CHOICES = [
        (1, 'Highest'),
        (2, 'High'),
        (3, 'Medium'),
        (4, 'Low'),
        (5, 'Lowest'),
    ]

    client = models.ForeignKey(
        Client, 
        on_delete=models.CASCADE,
        related_name='targeted_keywords'
    )
    keyword = models.CharField(max_length=255)
    priority = models.IntegerField(
        choices=PRIORITY_CHOICES,
        default=3,
        help_text="Priority level for this keyword"
    )
    notes = models.TextField(blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    def get_monthly_rankings(self, months=12):
        """Get monthly ranking history"""
        end_date = timezone.now().date()
        start_date = end_date - relativedelta(months=months)
        
        # logger.debug(
        #     f"Fetching monthly rankings for keyword '{self.keyword}' (ID: {self.id})"
        #     f"\nDate range: {start_date} to {end_date}"
        # )
        
        rankings = self.ranking_history.filter(
            date__gte=start_date,
            date__lte=end_date
        ).order_by('date')
        
        # logger.debug(f"Found {rankings.count()} ranking records")
        
        # Group by month and get the monthly record
        monthly_data = {}
        for ranking in rankings:
            month_key = ranking.date.strftime('%Y-%m')
            if month_key not in monthly_data:
                data = {
                    'date': ranking.date,
                    'position': float(ranking.average_position),
                    'impressions': int(ranking.impressions),
                    'clicks': int(ranking.clicks),
                    'ctr': float(ranking.ctr)
                }
                monthly_data[month_key] = data
                # logger.debug(
                #     f"Added data for {month_key}:"
                #     f"\nPosition: {data['position']}"
                #     f"\nImpressions: {data['impressions']}"
                #     f"\nClicks: {data['clicks']}"
                #     f"\nCTR: {data['ctr']}"
                # )
        
        result = [monthly_data[k] for k in sorted(monthly_data.keys())]
        # logger.debug(
        #     f"Returning {len(result)} months of data for {self.keyword}"
        # )
        return result

    def get_ranking_history(self):
        """Get all ranking history entries for this keyword"""
        return KeywordRankingHistory.objects.filter(
            Q(keyword=self) | 
            Q(keyword_text=self.keyword, client=self.client)
        ).order_by('-date')

    @property
    def current_position(self):
        """Get the most recent average position"""
        latest = self.get_ranking_history().first()
        return round(latest.average_position, 1) if latest else None

    def get_position_change(self, months=1):
        """Calculate position change over specified number of months"""
        history = self.get_ranking_history()[:2]  # Get latest two entries
        if len(history) < 2:
            return None
            
        current = history[0].average_position
        previous = history[1].average_position
        
        return round(previous - current, 1)

    @property
    def position_trend(self):
        """Returns trend indicator based on 30-day change"""
        change = self.get_position_change()
        if change is None:
            return 'neutral'
        if change > 0.5:  # Improved by more than 0.5 positions
            return 'up'
        if change < -0.5:  # Declined by more than 0.5 positions
            return 'down'
        return 'neutral'

    class Meta:
        unique_together = ['client', 'keyword']
        ordering = ['priority', 'keyword']

    def __str__(self):
        return f"{self.keyword} ({self.client.name})"

class KeywordRankingHistory(models.Model):
    client = models.ForeignKey(
        Client,
        on_delete=models.CASCADE,
        related_name='keyword_rankings'
    )
    keyword = models.ForeignKey(
        TargetedKeyword,
        on_delete=models.SET_NULL,
        null=True,
        blank=True,
        related_name='ranking_history'
    )
    keyword_text = models.CharField(
        max_length=255,
        help_text="Actual keyword text, useful when no TargetedKeyword reference exists"
    )
    date = models.DateField()
    impressions = models.IntegerField(default=0)
    clicks = models.IntegerField(default=0)
    ctr = models.FloatField(
        verbose_name="Click-Through Rate",
        help_text="Click-through rate as a decimal (e.g., 0.15 for 15%)"
    )
    average_position = models.FloatField()
    
    class Meta:
        unique_together = ['client', 'keyword_text', 'date']
        ordering = ['-date']
        get_latest_by = 'date'
        indexes = [
            models.Index(fields=['-date']),  # Optimize date-based queries
            models.Index(fields=['client', '-date']),  # Optimize client+date queries
        ]

    @classmethod
    def get_rankings_for_period(cls, client, start_date, end_date, keyword=None):
        """Get rankings for a specific period"""
        query = cls.objects.filter(
            client=client,
            date__range=[start_date, end_date]
        )
        
        if keyword:
            query = query.filter(
                Q(keyword=keyword) | Q(keyword_text=keyword.keyword)
            )
            
        return query.order_by('date')

    def __str__(self):
        return f"{self.keyword_text} - {self.client.name} - {self.date}"

    @property
    def position_change(self):
        """Calculate position change from previous entry"""
        previous = KeywordRankingHistory.objects.filter(
            client=self.client,
            keyword_text=self.keyword_text,
            date__lt=self.date
        ).order_by('-date').first()
        
        if previous:
            return previous.average_position - self.average_position
        return 0

class SEOProject(models.Model):
    client = models.ForeignKey(
        Client,
        on_delete=models.CASCADE,
        related_name='seo_projects'
    )
    title = models.CharField(max_length=200)
    description = models.TextField()
    implementation_date = models.DateField()
    completion_date = models.DateField(null=True, blank=True)
    targeted_keywords = models.ManyToManyField(
        TargetedKeyword,
        related_name='related_projects'
    )
    documentation_file = models.FileField(
        upload_to='seo_projects/%Y/%m/',
        null=True,
        blank=True
    )
    initial_rankings = models.JSONField(
        default=dict,
        help_text="Snapshot of keyword rankings before project implementation"
    )
    status = models.CharField(
        max_length=20,
        choices=[
            ('planned', 'Planned'),
            ('in_progress', 'In Progress'),
            ('completed', 'Completed'),
            ('on_hold', 'On Hold'),
        ],
        default='planned'
    )
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    class Meta:
        ordering = ['-implementation_date']

    def __str__(self):
        return f"{self.title} - {self.client.name}"

    # Add method to analyze project impact
    def analyze_impact(self):
        implementation_date = self.implementation_date
        pre_period = implementation_date - relativedelta(months=1)
        post_period = implementation_date + relativedelta(months=1)

        results = {}
        for keyword in self.targeted_keywords.all():
            rankings = keyword.ranking_history.filter(
                date__range=[pre_period, post_period]
            ).order_by('date')

            pre_avg = rankings.filter(date__lt=implementation_date).aggregate(
                Avg('average_position'))['average_position__avg']
            post_avg = rankings.filter(date__gte=implementation_date).aggregate(
                Avg('average_position'))['average_position__avg']

            results[keyword.keyword] = {
                'pre_implementation_avg': pre_avg,
                'post_implementation_avg': post_avg,
                'improvement': pre_avg - post_avg if pre_avg and post_avg else None,
                'impressions_change': self._calculate_impressions_change(rankings),
                'clicks_change': self._calculate_clicks_change(rankings)
            }

        return results

================
File: seo_manager/services.py
================
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from google.auth.exceptions import RefreshError
from google.auth.transport.requests import Request  # Add this import
from google.analytics.data_v1beta import BetaAnalyticsDataClient
from google.analytics.data_v1beta.types import (
  DateRange,
  Dimension,
  Metric,
  RunReportRequest,
)

def get_analytics_service(ga_credentials, request):
  print("Entering get_analytics_service")
  try:
      print("GA Credentials:", ga_credentials)
      print("User Email:", ga_credentials.user_email)
      credentials = Credentials(
          token=ga_credentials.access_token,
          refresh_token=ga_credentials.refresh_token,
          token_uri=ga_credentials.token_uri,
          client_id=ga_credentials.ga_client_id,
          client_secret=ga_credentials.client_secret,
          scopes=ga_credentials.scopes
      )
      print("Credentials created, refreshing...")
      credentials.refresh(Request())
      print("Credentials refreshed successfully.")
      client = BetaAnalyticsDataClient(credentials=credentials)
      print("Analytics client created successfully, client:", client)
      return client
  except RefreshError as e:
      print(f"Error refreshing credentials: {e}")
      raise e
  finally:
      print("Exiting get_analytics_service")

def get_analytics_data(client, property_id, start_date, end_date):
  print("Entering get_analytics_data")
  print(f"Fetching analytics data for Property ID: {property_id}, Start Date: {start_date}, End Date: {end_date}")
  
  try:
      request = RunReportRequest(
          property=f"properties/{property_id}",
          dimensions=[Dimension(name="date")],
          metrics=[
              Metric(name="sessions"),
              Metric(name="screenPageViews")  # Changed from "pageviews" to "screenPageViews"
          ],
          date_ranges=[DateRange(start_date=start_date, end_date=end_date)],
      )
      response = client.run_report(request)
      print("Analytics data fetched successfully.")
      return response
  except Exception as e:
      print(f"Error fetching analytics data: {e}")
      raise e
  finally:
      print("Exiting get_analytics_data")

================
File: seo_manager/sitemap_extractor.py
================
import os
import csv
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from django.conf import settings
from datetime import datetime
from apps.common.tools.user_activity_tool import user_activity_tool
import logging

logger = logging.getLogger(__name__)

def extract_sitemap_and_meta_tags(client, user):
  base_url = client.website_url.rstrip('/')  # Remove trailing slash if present
  fqdn = urlparse(base_url).netloc
  date_str = datetime.now().strftime("%y-%m-%d")
  file_name = f"{fqdn}-{date_str}.csv"
  file_path = os.path.join(settings.MEDIA_ROOT, str(user.id), 'meta-tags', file_name)

  # Ensure the directory exists
  os.makedirs(os.path.dirname(file_path), exist_ok=True)

  visited_urls = set()
  urls_to_visit = set()

  def process_sitemap(sitemap_url):
      logger.debug(f"Processing sitemap: {sitemap_url}")
      try:
          response = requests.get(sitemap_url, headers={'User-Agent': 'Mozilla/5.0'})
          if response.status_code == 200:
              soup = BeautifulSoup(response.content, 'xml')
              for loc in soup.find_all('loc'):
                  url = loc.text.strip()
                  if url.endswith('.xml'):
                      process_sitemap(url)
                  else:
                      urls_to_visit.add(url)
      except requests.RequestException as e:
          logger.error(f"Error processing sitemap {sitemap_url}: {e}")

  # Step 1: Look for sitemaps
  sitemap_urls = [
      f"{base_url}/sitemap_index.xml",
      f"{base_url}/sitemap.xml",
      f"{base_url}/sitemap",
  ]

  for sitemap_url in sitemap_urls:
      process_sitemap(sitemap_url)

  # If no sitemap found, start with the base URL
  if not urls_to_visit:
      urls_to_visit.add(base_url)

  with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:
      fieldnames = ['url', 'title', 'meta_description', 'meta_charset', 'viewport', 'robots', 'canonical', 'og_title', 'og_description', 'og_image', 'twitter_card', 'twitter_title', 'twitter_description', 'twitter_image', 'author', 'language']
      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
      writer.writeheader()

      while urls_to_visit:
          url = urls_to_visit.pop()

          if url in visited_urls:
              continue

          # Step 4: Exclude URLs with specific words, anchor links, and query strings
          if any(word in url for word in ['blog', 'product-id', 'search', 'page', 'wp-content']) or '#' in url or '?' in url:
              continue

          try:
              logger.debug(f"Visiting URL: {url}")
              response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
              logger.debug(f"Response: {response.status_code}")
              if response.status_code == 200:
                  soup = BeautifulSoup(response.content, 'html.parser')
                  # Step 3: Extract meta tags
                  meta_tags = {
                      'url': url,
                      'title': soup.title.string if soup.title else '',
                      'meta_description': soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else '',
                      'meta_charset': soup.find('meta', attrs={'charset': True})['charset'] if soup.find('meta', attrs={'charset': True}) else '',
                      'viewport': soup.find('meta', attrs={'name': 'viewport'})['content'] if soup.find('meta', attrs={'name': 'viewport'}) else '',
                      'robots': soup.find('meta', attrs={'name': 'robots'})['content'] if soup.find('meta', attrs={'name': 'robots'}) else '',
                      'canonical': soup.find('link', attrs={'rel': 'canonical'})['href'] if soup.find('link', attrs={'rel': 'canonical'}) else '',
                      'og_title': soup.find('meta', attrs={'property': 'og:title'})['content'] if soup.find('meta', attrs={'property': 'og:title'}) else '',
                      'og_description': soup.find('meta', attrs={'property': 'og:description'})['content'] if soup.find('meta', attrs={'property': 'og:description'}) else '',
                      'og_image': soup.find('meta', attrs={'property': 'og:image'})['content'] if soup.find('meta', attrs={'property': 'og:image'}) else '',
                      'twitter_card': soup.find('meta', attrs={'name': 'twitter:card'})['content'] if soup.find('meta', attrs={'name': 'twitter:card'}) else '',
                      'twitter_title': soup.find('meta', attrs={'name': 'twitter:title'})['content'] if soup.find('meta', attrs={'name': 'twitter:title'}) else '',
                      'twitter_description': soup.find('meta', attrs={'name': 'twitter:description'})['content'] if soup.find('meta', attrs={'name': 'twitter:description'}) else '',
                      'twitter_image': soup.find('meta', attrs={'name': 'twitter:image'})['content'] if soup.find('meta', attrs={'name': 'twitter:image'}) else '',
                      'author': soup.find('meta', attrs={'name': 'author'})['content'] if soup.find('meta', attrs={'name': 'author'}) else '',
                      'language': soup.find('html').get('lang', '') if soup.find('html') else '',
                  }

                  writer.writerow(meta_tags)

                  # Step 2: Extract internal links
                  for link in soup.find_all('a', href=True):
                      href = link['href']
                      # Ignore anchor links and query strings
                      if '#' in href or '?' in href:
                          continue
                      full_url = urljoin(url, href)
                      # Remove any fragments from the URL
                      full_url = full_url.split('#')[0]
                      if full_url.startswith(base_url) and full_url not in visited_urls and full_url not in urls_to_visit:
                          urls_to_visit.add(full_url)

                  visited_urls.add(url)

          except requests.RequestException as e:
              logger.error(f"Error processing URL {url}: {e}")

  # Step 7: Log the activity
  user_activity_tool.run(user, 'create', f"Created meta tags snapshot for client: {client.name}", client=client, details={'file_name': file_name})

  return file_path

def extract_sitemap_and_meta_tags_from_url(url, user):
    base_url = url.rstrip('/')  # Remove trailing slash if present
    fqdn = urlparse(base_url).netloc
    date_str = datetime.now().strftime("%y-%m-%d")
    file_name = f"{fqdn}-{date_str}.csv"
    file_path = os.path.join(settings.MEDIA_ROOT, str(user.id), 'meta-tags', file_name)

    # Ensure the directory exists
    os.makedirs(os.path.dirname(file_path), exist_ok=True)

    visited_urls = set()
    urls_to_visit = set()

    def process_sitemap(sitemap_url):
        logger.debug(f"Processing sitemap: {sitemap_url}")
        try:
            response = requests.get(sitemap_url, headers={'User-Agent': 'Mozilla/5.0'})
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'xml')
                for loc in soup.find_all('loc'):
                    url = loc.text.strip()
                    if url.endswith('.xml'):
                        process_sitemap(url)
                    else:
                        urls_to_visit.add(url)
        except requests.RequestException as e:
            logger.error(f"Error processing sitemap {sitemap_url}: {e}")

    # Step 1: Look for sitemaps
    sitemap_urls = [
        f"{base_url}/sitemap_index.xml",
        f"{base_url}/sitemap.xml",
        f"{base_url}/sitemap",
    ]

    for sitemap_url in sitemap_urls:
        process_sitemap(sitemap_url)

    # If no sitemap found, start with the base URL
    if not urls_to_visit:
        urls_to_visit.add(base_url)

    with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['url', 'title', 'meta_description', 'meta_charset', 'viewport', 'robots', 'canonical', 'og_title', 'og_description', 'og_image', 'twitter_card', 'twitter_title', 'twitter_description', 'twitter_image', 'author', 'language']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        while urls_to_visit:
            url = urls_to_visit.pop()

            if url in visited_urls:
                continue

            # Step 4: Exclude URLs with specific words, anchor links, and query strings
            if any(word in url for word in ['blog', 'product-id', 'search', 'page', 'wp-content']) or '#' in url or '?' in url:
                continue

            try:
                logger.debug(f"Visiting URL: {url}")
                response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
                logger.debug(f"Response: {response.status_code}")
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    # Step 3: Extract meta tags
                    meta_tags = {
                        'url': url,
                        'title': soup.title.string if soup.title else '',
                        'meta_description': soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else '',
                        'meta_charset': soup.find('meta', attrs={'charset': True})['charset'] if soup.find('meta', attrs={'charset': True}) else '',
                        'viewport': soup.find('meta', attrs={'name': 'viewport'})['content'] if soup.find('meta', attrs={'name': 'viewport'}) else '',
                        'robots': soup.find('meta', attrs={'name': 'robots'})['content'] if soup.find('meta', attrs={'name': 'robots'}) else '',
                        'canonical': soup.find('link', attrs={'rel': 'canonical'})['href'] if soup.find('link', attrs={'rel': 'canonical'}) else '',
                        'og_title': soup.find('meta', attrs={'property': 'og:title'})['content'] if soup.find('meta', attrs={'property': 'og:title'}) else '',
                        'og_description': soup.find('meta', attrs={'property': 'og:description'})['content'] if soup.find('meta', attrs={'property': 'og:description'}) else '',
                        'og_image': soup.find('meta', attrs={'property': 'og:image'})['content'] if soup.find('meta', attrs={'property': 'og:image'}) else '',
                        'twitter_card': soup.find('meta', attrs={'name': 'twitter:card'})['content'] if soup.find('meta', attrs={'name': 'twitter:card'}) else '',
                        'twitter_title': soup.find('meta', attrs={'name': 'twitter:title'})['content'] if soup.find('meta', attrs={'name': 'twitter:title'}) else '',
                        'twitter_description': soup.find('meta', attrs={'name': 'twitter:description'})['content'] if soup.find('meta', attrs={'name': 'twitter:description'}) else '',
                        'twitter_image': soup.find('meta', attrs={'name': 'twitter:image'})['content'] if soup.find('meta', attrs={'name': 'twitter:image'}) else '',
                        'author': soup.find('meta', attrs={'name': 'author'})['content'] if soup.find('meta', attrs={'name': 'author'}) else '',
                        'language': soup.find('html').get('lang', '') if soup.find('html') else '',
                    }

                    writer.writerow(meta_tags)

                    # Step 2: Extract internal links
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        # Ignore anchor links and query strings
                        if '#' in href or '?' in href:
                            continue
                        full_url = urljoin(url, href)
                        # Remove any fragments from the URL
                        full_url = full_url.split('#')[0]
                        if full_url.startswith(base_url) and full_url not in visited_urls and full_url not in urls_to_visit:
                            urls_to_visit.add(full_url)

                    visited_urls.add(url)

            except requests.RequestException as e:
                logger.error(f"Error processing URL {url}: {e}")

    # At the end, log the activity without a client
    user_activity_tool.run(user, 'create', f"Created meta tags snapshot for URL: {url}", details={'file_name': file_name})

    return file_path

================
File: seo_manager/urls.py
================
from django.urls import path, include
from . import views_summarizer, views_analytics
from .views import (
    KeywordListView, KeywordCreateView, KeywordUpdateView,
    ProjectListView, ProjectCreateView, ProjectDetailView
)
from .views import client_views, activity_views, analytics_views, business_objective_views, keyword_views, project_views, meta_tags_views, ranking_views, report_views, project_views, search_console_views

app_name = 'seo_manager'

urlpatterns = [
    # Main URLs
    path('', client_views.dashboard, name='dashboard'),
    path('summarize/', views_summarizer.summarize_view, name='summarize_view'),
    path('task_status/<str:task_id>/', views_summarizer.task_status, name='task_status'),
    
    # Client URLs
    path('clients/', include([
        path('', client_views.client_list, name='client_list'),
        path('add/', client_views.add_client, name='add_client'),
        path('<int:client_id>/', include([
            path('', client_views.client_detail, name='client_detail'),
            path('edit/', client_views.edit_client, name='edit_client'),
            path('delete/', client_views.delete_client, name='delete_client'),
            path('analytics/', views_analytics.client_analytics, name='client_analytics'),
            path('search-console/', search_console_views.client_search_console, name='client_search_console'),
            path('ads/', analytics_views.client_ads, name='client_ads'),
            path('dataforseo/', analytics_views.client_dataforseo, name='client_dataforseo'),   
            path('load-more-activities/', client_views.load_more_activities, name='load_more_activities'),
            path('export-activities/', client_views.export_activities, name='export_activities'),
            # Keyword Management URLs
            path('keywords/', include([
                path('', KeywordListView.as_view(), name='keyword_list'),
                path('add/', KeywordCreateView.as_view(), name='keyword_create'),
                path('import/', keyword_views.keyword_import, name='keyword_import'),
                path('<int:pk>/edit/', KeywordUpdateView.as_view(), name='keyword_update'),
                path('<int:pk>/rankings/', ranking_views.ranking_import, name='ranking_import'),
                path('search-console/', 
                     keyword_views.search_console_keywords, 
                     name='search_console_keywords'),
            ])),
            
            # SEO Project URLs
            path('projects/', include([
                path('', ProjectListView.as_view(), name='project_list'),
                path('add/', ProjectCreateView.as_view(), name='project_create'),
                path('<int:pk>/', ProjectDetailView.as_view(), name='project_detail'),
                path('<int:project_id>/edit/', project_views.edit_project, name='edit_project'),
                path('<int:project_id>/delete/', project_views.delete_project, name='delete_project'),
            ])),
            
            # Credentials URLs
            path('credentials/', include([
                # Google Analytics URLs
                path('ga/', include([
                    path('oauth/add/', 
                         analytics_views.add_ga_credentials_oauth, 
                         name='add_ga_credentials_oauth'),
                    path('service-account/add/', 
                         analytics_views.add_ga_credentials_service_account, 
                         name='add_ga_credentials_service_account'),
                    path('select-account/', 
                         analytics_views.select_analytics_account, 
                         name='select_analytics_account'),
                    path('remove/', 
                         analytics_views.remove_ga_credentials, 
                         name='remove_ga_credentials'),
                ])),
                
                # Search Console URLs
                path('sc/add/', 
                     search_console_views.add_sc_credentials, 
                     name='add_sc_credentials'),
                path('sc/remove/', 
                     search_console_views.remove_sc_credentials, 
                     name='remove_sc_credentials'),
            ])),
            
            # Business Objective URLs
            path('objectives/', include([
                path('add/', business_objective_views.add_business_objective, name='add_business_objective'),
                path('edit/<int:objective_index>/', business_objective_views.edit_business_objective, name='edit_business_objective'),
                path('delete/<int:objective_index>/', business_objective_views.delete_business_objective, name='delete_business_objective'),
                path('update-status/<int:objective_index>/', business_objective_views.update_objective_status, name='update_objective_status'),
            ])),
            
            # Profile URLs
            path('profile/', include([
                path('update/', client_views.update_client_profile, name='update_client_profile'),
                path('generate-magic/', client_views.generate_magic_profile, name='generate_magic_profile'),
            ])),
            
            # Meta Tags URLs
            path('meta-tags/', include([
                path('snapshot/', meta_tags_views.create_meta_tags_snapshot, name='create_meta_tags_snapshot'),
                path('', meta_tags_views.meta_tags_dashboard, name='meta_tags_dashboard'),
            ])),
            
            # Rankings URLs
            path('rankings/', include([
                path('collect/', ranking_views.collect_rankings, name='collect_rankings'),
                path('report/', report_views.generate_report, name='generate_report'),
                path('backfill/', ranking_views.backfill_rankings, name='backfill_rankings'),
                path('manage/', ranking_views.ranking_data_management, name='ranking_data_management'),
                path('export-csv/', ranking_views.export_rankings_csv, name='export_rankings_csv'),
            ])),
            
            # Search Console URLs
            path('select-property/', 
                 analytics_views.select_search_console_property, 
                 name='select_search_console_property'),
            path('add-service-account/', 
                 analytics_views.add_sc_credentials_service_account, 
                 name='add_sc_credentials_service_account'),
            path('integrations/', client_views.client_integrations, name='client_integrations'),
            path('import-from-search-console/', 
                 keyword_views.import_from_search_console, 
                 name='import_from_search_console'),
            path('meta-tags/', meta_tags_views.meta_tags_dashboard, name='meta_tags_dashboard'),
        ])),
    ])),
    
    # Other URLs
    path('activity-log/', activity_views.activity_log, name='activity_log'),
    path('create-meta-tags-snapshot-url/', meta_tags_views.create_meta_tags_snapshot_url, name='create_meta_tags_snapshot_url'),
    
    # OAuth URLs
    path('google/', include([
        path('login/callback/', 
             analytics_views.google_oauth_callback, 
             name='google_oauth_callback'),
        path('oauth/', include([
            path('init/<int:client_id>/<str:service_type>/', 
                 analytics_views.initiate_google_oauth, 
                 name='initiate_google_oauth'),
        ])),
    ])),
    path('clients/<int:client_id>/objectives/<int:objective_index>/update-status/',
         business_objective_views.update_objective_status, name='update_objective_status'),
]

================
File: seo_manager/utils.py
================
from datetime import datetime, date, timedelta
from dateutil.relativedelta import relativedelta
from typing import List, Tuple

def get_monthly_date_ranges(months_back: int = 12) -> List[Tuple[date, date]]:
    """
    Generate a list of (start_date, end_date) tuples for each month
    going back X months from today.
    """
    today = date.today()
    ranges = []
    
    for i in range(months_back):
        # Get first day of the month
        end_date = today - relativedelta(months=i)
        start_date = end_date.replace(day=1)
        
        # For current month, use today as end_date
        if i == 0:
            ranges.append((start_date, today))
        else:
            # Get last day of the month
            end_date = (start_date + relativedelta(months=1) - timedelta(days=1))
            ranges.append((start_date, end_date))
    
    return ranges

================
File: seo_manager/views_analytics.py
================
import json
import logging
from django.shortcuts import render, get_object_or_404, redirect
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from .models import Client, GoogleAnalyticsCredentials, SearchConsoleCredentials
from .google_auth import get_search_console_properties
from datetime import datetime, timedelta
from google.auth.exceptions import RefreshError
from googleapiclient.errors import HttpError
from apps.agents.tools.google_analytics_tool.google_analytics_tool import GoogleAnalyticsTool
from django.core.serializers.json import DjangoJSONEncoder
from apps.common.tools.user_activity_tool import user_activity_tool

logger = logging.getLogger(__name__)

@login_required
def client_analytics(request, client_id):
    client = get_object_or_404(Client, id=client_id)
    
    # Get credentials without forcing 404
    try:
        ga_credentials = GoogleAnalyticsCredentials.objects.get(client=client)
    except GoogleAnalyticsCredentials.DoesNotExist:
        ga_credentials = None
        
    try:
        sc_credentials = SearchConsoleCredentials.objects.get(client=client)
    except SearchConsoleCredentials.DoesNotExist:
        sc_credentials = None

    context = {
        'page_title': 'Client Analytics',
        'client': client,
        'analytics_data': None,
        'search_console_data': None,
    }

    # Only process GA data if credentials exist
    if ga_credentials:
        ga_range = request.GET.get('ga_range', '30')
        ga_end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        if ga_range == 'custom':
            ga_start_date = request.GET.get('ga_start_date')
            ga_end_date = request.GET.get('ga_end_date')
            if not ga_start_date or not ga_end_date:
                messages.error(request, "Invalid GA date range provided")
                return redirect('seo_manager:client_analytics', client_id=client_id)
        else:
            try:
                days = int(ga_range)
                ga_start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            except ValueError:
                messages.error(request, "Invalid GA date range")
                return redirect('seo_manager:client_analytics', client_id=client_id)

        context.update({
            'ga_start_date': ga_start_date,
            'ga_end_date': ga_end_date,
            'selected_ga_range': ga_range,
        })

        try:
            logger.info("Fetching data using GoogleAnalyticsTool")
            ga_tool = GoogleAnalyticsTool()
            
            analytics_data = ga_tool._run(
                start_date=ga_start_date,
                end_date=ga_end_date,
                client_id=client_id
            )
            
            if analytics_data['success']:
                logger.info(f"Number of data points: {len(analytics_data['analytics_data'])}")
                if analytics_data['analytics_data']:
                    logger.info(f"Sample data point: {analytics_data['analytics_data'][0]}")
                
                context['analytics_data'] = json.dumps(analytics_data['analytics_data'])
                context['start_date'] = analytics_data['start_date']
                context['end_date'] = analytics_data['end_date']
            else:
                logger.warning(f"Failed to fetch GA data: {analytics_data.get('error')}")
                messages.warning(request, "Unable to fetch Google Analytics data.")
                
        except Exception as e:
            logger.error(f"Error fetching GA data: {str(e)}", exc_info=True)
            messages.warning(request, "Unable to fetch Google Analytics data.")

    # Only process SC data if credentials exist
    if sc_credentials:
        sc_range = request.GET.get('sc_range', '30')
        sc_end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        if sc_range == 'custom':
            sc_start_date = request.GET.get('sc_start_date')
            sc_end_date = request.GET.get('sc_end_date')
            if not sc_start_date or not sc_end_date:
                messages.error(request, "Invalid SC date range provided")
                return redirect('seo_manager:client_analytics', client_id=client_id)
        else:
            try:
                days = int(sc_range)
                sc_start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            except ValueError:
                messages.error(request, "Invalid SC date range")
                return redirect('seo_manager:client_analytics', client_id=client_id)

        context.update({
            'sc_start_date': sc_start_date,
            'sc_end_date': sc_end_date,
            'selected_sc_range': sc_range,
        })

        try:
            search_console_service = sc_credentials.get_service()
            if search_console_service:
                property_url = sc_credentials.get_property_url()
                if property_url:
                    search_console_data = get_search_console_data(
                        search_console_service, 
                        property_url,
                        sc_start_date,
                        sc_end_date
                    )
                    context['search_console_data'] = search_console_data
                else:
                    messages.warning(request, "Invalid Search Console property URL format.")
            else:
                messages.warning(request, "Search Console credentials are incomplete.")
        except Exception as e:
            logger.error(f"Error fetching Search Console data: {str(e)}")
            messages.warning(request, "Unable to fetch Search Console data.")

    return render(request, 'seo_manager/client_analytics.html', context)

def get_search_console_service(credentials, request):
    from googleapiclient.discovery import build
    from google.oauth2.credentials import Credentials
    
    creds = Credentials(
        token=credentials.access_token,
        refresh_token=credentials.refresh_token,
        token_uri=credentials.token_uri,
        client_id=credentials.sc_client_id,
        client_secret=credentials.client_secret
    )
    
    return build('searchconsole', 'v1', credentials=creds)

def get_search_console_data(service, property_url, start_date, end_date):
    try:
        response = service.searchanalytics().query(
            siteUrl=property_url,
            body={
                'startDate': start_date,
                'endDate': end_date,
                'dimensions': ['query'],
                'rowLimit': 1000
            }
        ).execute()
        
        search_console_data = []
        for row in response.get('rows', []):
            search_console_data.append({
                'query': row['keys'][0],
                'clicks': row['clicks'],
                'impressions': row['impressions'],
                'ctr': row['ctr'] * 100,  # Convert to percentage
                'position': row['position']
            })
        
        search_console_data.sort(key=lambda x: x['impressions'], reverse=True)
        
        return search_console_data
    except HttpError as error:
        print(f"An error occurred: {error}")
        return []

================
File: seo_manager/views_summarizer.py
================
from django.contrib.auth.models import User
from django.conf import settings

from apps.tasks.tasks import summarize_content
import mistune

from django.http import JsonResponse
from celery.result import AsyncResult
import logging
from apps.common.utils import get_models
from apps.common.tools.user_activity_tool import user_activity_tool  # Add this import

import json
from django.shortcuts import render
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from .models import SummarizerUsage



@login_required
def summarize_view(request):
  models = get_models()
  logging.info(f'request.user.id: {request.user.id}')
  model_selected = settings.SUMMARIZER
  
  if request.method == 'POST':
    text_to_summarize = request.POST.get('query_text_value')
    model_selected = request.POST.get('model_selected_value')
    task = summarize_content.delay(text_to_summarize, request.user.id, model_selected)
    
    # Log user activity
    user_activity_tool.run(
      user=request.user,
      category='summarize',
      action=f"Used summarizer with model: {model_selected}",
      details={"text_length": len(text_to_summarize)}
    )
    
    return JsonResponse({'task_id': task.id})
  
  user = User.objects.get(id=request.user.id)
  summarizations = SummarizerUsage.objects.filter(user=user).order_by('-created_at')
  
  for summ in summarizations:
    summ.html_result = mistune.html(summ.response + '\n\n---Detail---------\n\n'+summ.compressed_content)
    
  task_result = None
  task_status = None
  model_selected =  settings.SUMMARIZER
  context = {
    'page_title': 'Summarize',
    'task_result': task_result,
    'task_status': task_status,
    'summarizations': summarizations,
    'models': models,
    'model_selected': model_selected
  }
  
  # Log user activity for viewing summarize page
  user_activity_tool.run(
    user=request.user,
    category='view',
    action="Viewed summarize page"
  )
  
  return render(request, 'pages/apps/summarize.html', context)

def task_status(request, task_id):
    current_chunk = 0
    total_chunks = 1
    task_result = AsyncResult(task_id)
    if task_result.info is not None:
      if task_result.state == 'SUCCESS':
          result = task_result.result
          html_result = mistune.html(result)
          return JsonResponse({'status': 'SUCCESS', 'result': html_result})
      elif task_result.state == 'FAILURE':
          error = str(task_result.result)
          return JsonResponse({'status': 'FAILURE', 'result': error})
      elif task_result.status == 'processing':
          progress = task_result.info
          current_chunk = progress.get('current_chunk', 0)
          total_chunks = progress.get('total_chunks', 0)
          return JsonResponse({'status': task_result.status, 'current': current_chunk, 'total': total_chunks})
      else:
          if task_result.status:
              return JsonResponse({'status': task_result.status})
          else:
              return JsonResponse({'status': 'PENDING'})
    else:
        return JsonResponse({'status': 'PENDING'})

================
File: seo_manager/views.py
================
from .views.client_views import *
from .views.keyword_views import *
from .views.project_views import *
from .views.analytics_views import *
from .views.search_console_views import *
from .views.business_objective_views import *
from .views.ranking_views import *
from .views.report_views import *
from .views.activity_views import *
from .views.meta_tags_views import *

# This file now serves as a compatibility layer, importing all views from their respective modules.
# All views are now organized in separate files in the views/ directory for better maintainability.

================
File: users/admin.py
================
from django.contrib import admin

# Register your models here.

================
File: users/apps.py
================
from django.apps import AppConfig


class UsersConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'apps.users'

    def ready(self):
        import apps.users.signals

================
File: users/forms.py
================
from django import forms
from django.utils.translation import gettext_lazy as _
from apps.users.models import Profile


class ProfileForm(forms.ModelForm):
    class Meta:
        model = Profile
        exclude = ('user', 'role', 'avatar',)

    def __init__(self, *args, **kwargs):
        super(ProfileForm, self).__init__(*args, **kwargs)

        for field_name, field in self.fields.items():
            self.fields[field_name].widget.attrs['placeholder'] = field.label
            self.fields[field_name].widget.attrs['class'] = 'form-control'
            self.fields[field_name].widget.attrs['required'] = False


class QuillFieldForm(forms.ModelForm):
    class Meta:
        model = Profile
        fields = ('bio',)

================
File: users/models.py
================
import json
from django.db import models
from django.contrib.auth.models import User
from django_quill.fields import QuillField

# Create your models here.

ROLE_CHOICES = (
    ('admin'  , 'Admin'),
    ('user'  , 'User'),
)

def avatar_with_id(instance, filename):
    return "{}/avatar/{}".format(f"{instance.user.id}", filename)

def convert_to_quill():
    converted_data = {
        "delta": "",
        "html": "Write something #cool about you.",
    }
    return json.dumps(converted_data)


class Profile(models.Model):
    user      = models.OneToOneField(User, on_delete=models.CASCADE)
    role      = models.CharField(max_length=20, choices=ROLE_CHOICES, default='user')
    full_name = models.CharField(max_length=255, null=True, blank=True)
    country   = models.CharField(max_length=255, null=True, blank=True)
    city      = models.CharField(max_length=255, null=True, blank=True)
    zip_code  = models.CharField(max_length=255, null=True, blank=True)
    address   = models.CharField(max_length=255, null=True, blank=True)
    phone     = models.CharField(max_length=255, null=True, blank=True)
    avatar    = models.ImageField(upload_to=avatar_with_id, null=True, blank=True)
    bio       = QuillField(default=convert_to_quill())
    dark_mode = models.BooleanField(default=False)


    def __str__(self):
        return self.user.username

================
File: users/signals.py
================
from django.contrib.auth.models import User
from apps.users.models import Profile
from django.db.models.signals import post_save
from django.dispatch import receiver

@receiver(post_save, sender=User)
def create_profile(sender, instance, created, **kwargs):
    if created:
        profile = Profile.objects.create(user=instance)
        if instance.is_superuser:
            profile.role = "admin"
            profile.save()

================
File: users/tests.py
================
from django.test import TestCase

# Create your tests here.

================
File: users/urls.py
================
from django.urls import path
from apps.users import views


urlpatterns = [
    path('profile/', views.profile, name='profile'),
    path('upload-avatar/', views.upload_avatar, name='upload_avatar'),
    path('change-password/', views.change_password, name='change_password'),
    path('change-mode/', views.change_mode, name='change_mode'),
]

================
File: users/views.py
================
from django.shortcuts import render, redirect, get_object_or_404
from apps.users.models import Profile
from apps.users.forms import ProfileForm, QuillFieldForm
from django.contrib.auth.decorators import login_required
from django.contrib.auth.hashers import check_password
from django.contrib import messages

# Create your views here.


@login_required(login_url='/accounts/login/basic-login/')
def profile(request):
    profile = get_object_or_404(Profile, user=request.user)
    form = QuillFieldForm(instance=profile)
    if request.method == 'POST':

        if request.POST.get('email'):
            request.user.email = request.POST.get('email')
            request.user.save()

        for attribute, value in request.POST.items():
            if attribute == 'csrfmiddlewaretoken':
                continue

            setattr(profile, attribute, value)
            profile.save()

        messages.success(request, 'Profile updated successfully')
        return redirect(request.META.get('HTTP_REFERER'))

    context = {
        'page_title': 'Profile',
        'segment': 'profile',
        'parent': 'apps',
        'form': form
    }
    return render(request, 'pages/apps/user-profile.html', context)


def upload_avatar(request):
    profile = get_object_or_404(Profile, user=request.user)
    if request.method == 'POST':
        profile.avatar = request.FILES.get('avatar')
        profile.save()
        messages.success(request, 'Avatar uploaded successfully')
    return redirect(request.META.get('HTTP_REFERER'))


def change_password(request):
    user = request.user
    if request.method == 'POST':
        new_password = request.POST.get('new_password')
        confirm_new_password = request.POST.get('confirm_new_password')

        if new_password == confirm_new_password:
            if check_password(request.POST.get('current_password'), user.password):
                user.set_password(new_password)
                user.save()
                messages.success(request, 'Password changed successfully')
            else:
                messages.error(request, "Old password doesn't match!")
        else:
            messages.error(request, "Password doesn't match!")

    return redirect(request.META.get('HTTP_REFERER'))


@login_required(login_url='/accounts/login/basic-login/')
def change_mode(request):
    profile = get_object_or_404(Profile, user=request.user)
    if profile.dark_mode:
        profile.dark_mode = False
    else:
        profile.dark_mode = True
    
    profile.save()

    return redirect(request.META.get('HTTP_REFERER'))
