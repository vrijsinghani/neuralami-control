{"status": "success", "website_url": "https://docs.crawl4ai.com", "total_pages": 41, "max_depth_reached": 1, "crawl_result_id": 838, "content": "URL: https://docs.crawl4ai.com/\n\nTitle: Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Quick Start What Does Crawl4AI Do? Documentation Structure How You Can Support Quick Links \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper ![unclecode%2Fcrawl4ai | Trendshift ](https://trendshift.io/repositories/11716) ![GitHub Stars ](https://github.com/unclecode/crawl4ai/stargazers) ![GitHub Forks ](https://github.com/unclecode/crawl4ai/network/members) ![PyPI version ](https://badge.fury.io/py/crawl4ai) ![Python Version ](https://pypi.org/project/crawl4ai/) ![Downloads ](https://pepy.tech/project/crawl4ai) ![License ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE) Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. > Note : If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") Print the extracted content print(result.markdown) Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown : Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control : Hooks, proxies, stealth modes, session re-use\u2014fine-grained control. 4. High Performance : Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source : No forced API keys, no paywalls\u2014everyone can access their data. Core Philosophies : - Democratize Data : Free to use, transparent, and highly configurable. - LLM Friendly : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we\u2019ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you\u2019ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests : Whether it\u2019s a small fix, a big feature, or better docs\u2014contributions are always welcome. Join Discord : Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word : Mention Crawl4AI in your blog posts, talks, or on social media. Our mission : to empower everyone\u2014students, researchers, entrepreneurs, data scientists\u2014to access, parse, and shape the world\u2019s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let\u2019s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! \u2014 Unclecode, Founder & Maintainer of Crawl4AI Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/quickstart/\n\nTitle: Quick Start - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Getting Started with Crawl4AI 1. Introduction 2. Your First Crawl 3. Basic Configuration (Light Introduction) 4. Generating Markdown Output 5. Simple Data Extraction (CSS-based) 6. Simple Data Extraction (LLM-based) 7. Multi-URL Concurrency (Preview) 8. Dynamic Content Example 9. Next Steps Getting Started with Crawl4AI Welcome to Crawl4AI , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you\u2019ll: 1. Run your first crawl using minimal configuration. 2. Generate Markdown output (and learn how it\u2019s influenced by content filters). 3. Experiment with a simple CSS-based extraction strategy. 4. See a glimpse of LLM-based extraction (including open-source and closed-source model options). 5. Crawl a dynamic page that loads content via JavaScript. 1. Introduction Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler. Configurable browser and run settings via BrowserConfig and CrawlerRunConfig. Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or \u201ctraditional\u201d CSS/XPath-based). By the end of this guide, you\u2019ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses \u201cLoad More\u201d buttons or JavaScript updates. 2. Your First Crawl Here\u2019s a minimal Python script that creates an AsyncWebCrawler , fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\") print(result.markdown[:300]) Print first 300 chars if name == \"main\": asyncio.run(main()) What\u2019s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com. - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) Crawl4AI\u2019s crawler can be heavily customized using two main classes: 1. BrowserConfig : Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2. CrawlerRunConfig : Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browserconf = BrowserConfig(headless=True) or False to see the browser runconf = CrawlerRunConfig( cachemode=CacheMode.BYPASS ) async with AsyncWebCrawler(config=browserconf) as crawler: result = await crawler.arun( url=\"https://example.com\", config=runconf ) print(result.markdown) if name == \"main\": asyncio.run(main()) > IMPORTANT: By default cache mode is set to CacheMode.ENABLED. So to have fresh content, you need to set it to CacheMode.BYPASS We\u2019ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter. result.markdown : The direct HTML-to-Markdown conversion. result.markdown.fitmarkdown : The same content after applying any configured content filter (e.g., PruningContentFilter). Example: Using a Filter with DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator mdgenerator = DefaultMarkdownGenerator( contentfilter=PruningContentFilter(threshold=0.4, thresholdtype=\"fixed\") ) config = CrawlerRunConfig( cachemode=CacheMode.BYPASS, markdowngenerator=mdgenerator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://news.ycombinator.com\", config=config) print(\"Raw Markdown length:\", len(result.markdown.rawmarkdown)) print(\"Fit Markdown length:\", len(result.markdown.fitmarkdown)) Note : If you do not specify a content filter or markdown generator, you\u2019ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We\u2019ll dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSS-based) Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: > New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extractionstrategy import JsonCssExtractionStrategy Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" Using OpenAI (requires API token) schema = JsonCssExtractionStrategy.generateschema( html, llmprovider=\"openai/gpt-4o\", Default provider apitoken=\"your-openai-token\" Required for OpenAI ) Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy.generateschema( html, llmprovider=\"ollama/llama3.3\", Open source alternative apitoken=None Not needed for Ollama ) Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies. Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): schema = { \"name\": \"Example Items\", \"baseSelector\": \"div.item\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } rawhtml = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"raw://\" + rawhtml, config=CrawlerRunConfig( cachemode=CacheMode.BYPASS, extractionstrategy=JsonCssExtractionStrategy(schema) ) ) The JSON output is stored in 'extractedcontent' data = json.loads(result.extractedcontent) print(data) if name == \"main\": asyncio.run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. > Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw://. 6. Simple Data Extraction (LLM-based) For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3, notoken) OpenAI Models (e.g., openai/gpt-4, requires apitoken) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import LLMExtractionStrategy class OpenAIModelFee(BaseModel): modelname: str = Field(..., description=\"Name of the OpenAI model.\") inputfee: str = Field(..., description=\"Fee for input token for the OpenAI model.\") outputfee: str = Field( ..., description=\"Fee for output token for the OpenAI model.\" ) async def extractstructureddatausingllm( provider: str, apitoken: str = None, extraheaders: Dict[str, str] = None ): print(f\"\\n--- Extracting Structured Data with {provider} ---\") if apitoken is None and provider != \"ollama\": print(f\"API token is required for {provider}. Skipping this example.\") return browserconfig = BrowserConfig(headless=True) extraargs = {\"temperature\": 0, \"topp\": 0.9, \"maxtokens\": 2000} if extraheaders: extraargs[\"extraheaders\"] = extraheaders crawlerconfig = CrawlerRunConfig( cachemode=CacheMode.BYPASS, wordcountthreshold=1, pagetimeout=80000, extractionstrategy=LLMExtractionStrategy( provider=provider, apitoken=apitoken, schema=OpenAIModelFee.modeljsonschema(), extractiontype=\"schema\", instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\", extraargs=extraargs, ), ) async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun( url=\"https://openai.com/api/pricing/\", config=crawlerconfig ) print(result.extractedcontent) if name == \"main\": Use ollama with llama3.3 asyncio.run( extractstructureddatausingllm( provider=\"ollama/llama3.3\", apitoken=\"no-token\" ) ) asyncio.run( extractstructureddatausingllm( provider=\"openai/gpt-4o\", apitoken=os.getenv(\"OPENAIAPIKEY\") ) ) What\u2019s happening? - We define a Pydantic schema (PricingInfo) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and apitoken , you can use local models or a remote API. 7. Multi-URL Concurrency (Preview) If you need to crawl multiple URLs in parallel , you can use arunmany(). By default, Crawl4AI employs a MemoryAdaptiveDispatcher , automatically adjusting concurrency based on system resources. Here\u2019s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quickparallelexample(): urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] runconf = CrawlerRunConfig( cachemode=CacheMode.BYPASS, stream=True Enable streaming mode ) async with AsyncWebCrawler() as crawler: Stream results as they complete async for result in await crawler.arunmany(urls, config=runconf): if result.success: print(f\"[OK] {result.url}, length: {len(result.markdownv2.rawmarkdown)}\") else: print(f\"[ERROR] {result.url} => {result.errormessage}\") Or get all results at once (default behavior) runconf = runconf.clone(stream=False) results = await crawler.arunmany(urls, config=runconf) for res in results: if res.success: print(f\"[OK] {res.url}, length: {len(res.markdownv2.rawmarkdown)}\") else: print(f\"[ERROR] {res.url} => {res.errormessage}\") if name == \"main\": asyncio.run(quickparallelexample()) The example above shows two ways to handle multiple URLs: 1. Streaming mode (stream=True): Process results as they become available using async for 2. Batch mode (stream=False): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling , or customized rate limiting), see Advanced Multi-URL Crawling. 8. Dynamic Content Example Some sites require multiple \u201cpage clicks\u201d or dynamic JavaScript updates. Below is an example showing how to click a \u201cNext Page\u201d button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig : import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractstructureddatausingcssextractor(): print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\") schema = { \"name\": \"KidoCode Courses\", \"baseSelector\": \"section.charge-methodology .w-tab-content > div\", \"fields\": [ { \"name\": \"sectiontitle\", \"selector\": \"h3.heading-50\", \"type\": \"text\", }, { \"name\": \"sectiondescription\", \"selector\": \".charge-content\", \"type\": \"text\", }, { \"name\": \"coursename\", \"selector\": \".text-block-93\", \"type\": \"text\", }, { \"name\": \"coursedescription\", \"selector\": \".course-content-text\", \"type\": \"text\", }, { \"name\": \"courseicon\", \"selector\": \".image-92\", \"type\": \"attribute\", \"attribute\": \"src\", }, ], } browserconfig = BrowserConfig(headless=True, javascriptenabled=True) jsclicktabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawlerconfig = CrawlerRunConfig( cachemode=CacheMode.BYPASS, extractionstrategy=JsonCssExtractionStrategy(schema), jscode=[jsclicktabs], ) async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun( url=\"https://www.kidocode.com/degrees/technology\", config=crawlerconfig ) companies = json.loads(result.extractedcontent) print(f\"Successfully extracted {len(companies)} companies\") print(json.dumps(companies[0], indent=2)) async def main(): await extractstructureddatausingcssextractor() if name == \"main\": asyncio.run(main()) Key Points : BrowserConfig(headless=False) : We want to watch it click \u201cNext Page.\u201d CrawlerRunConfig(...) : We specify the extraction strategy, pass sessionid to reuse the same page. jscode and waitfor are used for subsequent pages (page > 0) to click the \u201cNext\u201d button and wait for new commits to load. jsonly=True indicates we\u2019re not re-navigating but continuing the existing session. Finally, we call killsession() to clean up the page and browser session. 9. Next Steps Congratulations! You have: 1. Performed a basic crawl and printed Markdown. 2. Used content filters with a markdown generator. 3. Extracted JSON via CSS or LLM strategies. 4. Handled dynamic pages with JavaScript triggers. If you\u2019re ready for more, check out: Installation : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth: Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management : Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/installation/\n\nTitle: Installation - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Installation & Setup (2023 Edition) 1. Basic Installation 2. Initial Setup & Diagnostics 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor) 4. Advanced Installation (Optional) 5. Docker (Experimental) 6. Local Server Mode (Legacy) Summary Installation & Setup (2023 Edition) 1. Basic Installation pip install crawl4ai This installs the core Crawl4AI library along with essential dependencies. No advanced features (like transformers or PyTorch) are included yet. 2. Initial Setup & Diagnostics 2.1 Run the Setup Command After installing, call: crawl4ai-setup What does it do? - Installs or updates required Playwright browsers (Chromium, Firefox, etc.) - Performs OS-level checks (e.g., missing libs on Linux) - Confirms your environment is ready to crawl 2.2 Diagnostics Optionally, you can run diagnostics to confirm everything is functioning: crawl4ai-doctor This command attempts to: - Check Python version compatibility - Verify Playwright installation - Inspect environment variables or library conflicts If any issues arise, follow its suggestions (e.g., installing additional system packages) and re-run crawl4ai-setup. 3. Verifying Installation: A Simple Crawl (Skip this step if you already run crawl4ai-doctor) Below is a minimal Python script demonstrating a basic crawl. It uses our new BrowserConfig and CrawlerRunConfig for clarity, though no custom settings are passed in this example: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://www.example.com\", ) print(result.markdown[:300]) Show the first 300 characters of extracted text if name == \"main\": asyncio.run(main()) Expected outcome: - A headless browser session loads example.com - Crawl4AI returns 300 characters of markdown. If errors occur, rerun crawl4ai-doctor or manually ensure Playwright is installed correctly. 4. Advanced Installation (Optional) Warning : Only install these if you truly need them. They bring in larger dependencies, including big models, which can increase disk usage and memory load significantly. 4.1 Torch, Transformers, or All Text Clustering (Torch) pip install crawl4ai[torch] crawl4ai-setup Installs PyTorch-based features (e.g., cosine similarity or advanced semantic chunking). Transformers pip install crawl4ai[transformer] crawl4ai-setup Adds Hugging Face-based summarization or generation strategies. All Features pip install crawl4ai[all] crawl4ai-setup (Optional) Pre-Fetching Models crawl4ai-download-models This step caches large models locally (if needed). Only do this if your workflow requires them. 5. Docker (Experimental) We provide a temporary Docker approach for testing. It\u2019s not stable and may break with future releases. We plan a major Docker revamp in a future stable version, 2025 Q1. If you still want to try: docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic You can then make POST requests to http://localhost:11235/crawl to perform crawls. Production usage is discouraged until our new Docker approach is ready (planned in Jan or Feb 2025). 6. Local Server Mode (Legacy) Some older docs mention running Crawl4AI as a local server. This approach has been partially replaced by the new Docker-based prototype and upcoming stable server release. You can experiment, but expect major changes. Official local server instructions will arrive once the new Docker architecture is finalized. Summary 1. Install with pip install crawl4ai and run crawl4ai-setup. 2. Diagnose with crawl4ai-doctor if you see errors. 3. Verify by crawling example.com with minimal BrowserConfig + CrawlerRunConfig. 4. Advanced features (Torch, Transformers) are optional \u2014avoid them if you don\u2019t need them (they significantly increase resource usage). 5. Docker is experimental \u2014use at your own risk until the stable version is released. 6. Local server references in older docs are largely deprecated; a new solution is in progress. Got questions? Check GitHub issues for updates or ask the community! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/docker-deploymeny/\n\nTitle: Docker Deployment - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Docker Deployment Quick Start \ud83d\ude80 Running with Docker Compose \ud83d\udc33 API Security \ud83d\udd12 Configuration Options \ud83d\udd27 Usage Examples \ud83d\udcdd Platform-Specific Instructions \ud83d\udcbb Testing \ud83e\uddea Advanced Configuration \u2699\ufe0f Troubleshooting \ud83d\udd0d Best Practices \ud83c\udf1f API Reference \ud83d\udcda Docker Deployment Crawl4AI provides official Docker images for easy deployment and scalability. This guide covers installation, configuration, and usage of Crawl4AI in Docker environments. Quick Start \ud83d\ude80 Pull and run the basic version: Basic run without security docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic Run with API security enabled docker run -p 11235:11235 -e CRAWL4AIAPITOKEN=yoursecrettoken unclecode/crawl4ai:basic Running with Docker Compose \ud83d\udc33 Use Docker Compose (From Local Dockerfile or Docker Hub) Crawl4AI provides flexibility to use Docker Compose for managing your containerized services. You can either build the image locally from the provided Dockerfile or use the pre-built image from Docker Hub. Option 1: Using Docker Compose to Build Locally If you want to build the image locally, use the provided docker-compose.local.yml file. docker-compose -f docker-compose.local.yml up -d This will: 1. Build the Docker image from the provided Dockerfile. 2. Start the container and expose it on http://localhost:11235. Option 2: Using Docker Compose with Pre-Built Image from Hub If you prefer using the pre-built image on Docker Hub, use the docker-compose.hub.yml file. docker-compose -f docker-compose.hub.yml up -d This will: 1. Pull the pre-built image unclecode/crawl4ai:basic (or all, depending on your configuration). 2. Start the container and expose it on http://localhost:11235. Stopping the Running Services To stop the services started via Docker Compose, you can use: docker-compose -f docker-compose.local.yml down OR docker-compose -f docker-compose.hub.yml down If the containers don\u2019t stop and the application is still running, check the running containers: docker ps Find the CONTAINER ID of the running service and stop it forcefully: docker stop <CONTAINERID> Debugging with Docker Compose Check Logs : To view the container logs: docker-compose -f docker-compose.local.yml logs -f Remove Orphaned Containers : If the service is still running unexpectedly: docker-compose -f docker-compose.local.yml down --remove-orphans Manually Remove Network : If the network is still in use: docker network ls docker network rm crawl4aidefault Why Use Docker Compose? Docker Compose is the recommended way to deploy Crawl4AI because: 1. It simplifies multi-container setups. 2. Allows you to define environment variables, resources, and ports in a single file. 3. Makes it easier to switch between local development and production-ready images. For example, your docker-compose.yml could include API keys, token settings, and memory limits, making deployment quick and consistent. API Security \ud83d\udd12 Understanding CRAWL4AIAPITOKEN The CRAWL4AIAPITOKEN provides optional security for your Crawl4AI instance: If CRAWL4AIAPITOKEN is set: All API endpoints (except /health) require authentication If CRAWL4AIAPITOKEN is not set: The API is publicly accessible Secured Instance docker run -p 11235:11235 -e CRAWL4AIAPITOKEN=yoursecrettoken unclecode/crawl4ai:all Unsecured Instance docker run -p 11235:11235 unclecode/crawl4ai:all Making API Calls For secured instances, include the token in all requests: import requests Setup headers if token is being used apitoken = \"yoursecrettoken\" Same token set in CRAWL4AIAPITOKEN headers = {\"Authorization\": f\"Bearer {apitoken}\"} if apitoken else {} Making authenticated requests response = requests.post( \"http://localhost:11235/crawl\", headers=headers, json={ \"urls\": \"https://example.com\", \"priority\": 10 } ) Checking task status taskid = response.json()[\"taskid\"] status = requests.get( f\"http://localhost:11235/task/{taskid}\", headers=headers ) Using with Docker Compose In your docker-compose.yml: services: crawl4ai: image: unclecode/crawl4ai:all environment: - CRAWL4AIAPITOKEN=${CRAWL4AIAPITOKEN:-} Optional ... other configuration Then either: 1. Set in .env file: CRAWL4AIAPITOKEN=yoursecrettoken 1. Or set via command line: CRAWL4AIAPITOKEN=yoursecrettoken docker-compose up > Security Note : If you enable the API token, make sure to keep it secure and never commit it to version control. The token will be required for all API endpoints except the health check endpoint (/health). Configuration Options \ud83d\udd27 Environment Variables You can configure the service using environment variables: Basic configuration docker run -p 11235:11235 \\ -e MAXCONCURRENTTASKS=5 \\ unclecode/crawl4ai:all With security and LLM support docker run -p 11235:11235 \\ -e CRAWL4AIAPITOKEN=yoursecrettoken \\ -e OPENAIAPIKEY=sk-... \\ -e ANTHROPICAPIKEY=sk-ant-... \\ unclecode/crawl4ai:all Using Docker Compose (Recommended) \ud83d\udc33 Create a docker-compose.yml: version: '3.8' services: crawl4ai: image: unclecode/crawl4ai:all ports: - \"11235:11235\" environment: - CRAWL4AIAPITOKEN=${CRAWL4AIAPITOKEN:-} Optional API security - MAXCONCURRENTTASKS=5 LLM Provider Keys - OPENAIAPIKEY=${OPENAIAPIKEY:-} - ANTHROPICAPIKEY=${ANTHROPICAPIKEY:-} volumes: - /dev/shm:/dev/shm deploy: resources: limits: memory: 4G reservations: memory: 1G You can run it in two ways: 1. Using environment variables directly: CRAWL4AIAPITOKEN=secret123 OPENAIAPIKEY=sk-... docker-compose up 2. Using a .env file (recommended): Create a .env file in the same directory: API Security (optional) CRAWL4AIAPITOKEN=yoursecrettoken LLM Provider Keys OPENAIAPIKEY=sk-... ANTHROPICAPIKEY=sk-ant-... Other Configuration MAXCONCURRENTTASKS=5 Then simply run: docker-compose up Testing the Deployment \ud83e\uddea import requests For unsecured instances def testunsecured(): Health check health = requests.get(\"http://localhost:11235/health\") print(\"Health check:\", health.json()) Basic crawl response = requests.post( \"http://localhost:11235/crawl\", json={ \"urls\": \"https://www.nbcnews.com/business\", \"priority\": 10 } ) taskid = response.json()[\"taskid\"] print(\"Task ID:\", taskid) For secured instances def testsecured(apitoken): headers = {\"Authorization\": f\"Bearer {apitoken}\"} Basic crawl with authentication response = requests.post( \"http://localhost:11235/crawl\", headers=headers, json={ \"urls\": \"https://www.nbcnews.com/business\", \"priority\": 10 } ) taskid = response.json()[\"taskid\"] print(\"Task ID:\", taskid) LLM Extraction Example \ud83e\udd16 When you've configured your LLM provider keys (via environment variables or .env), you can use LLM extraction: request = { \"urls\": \"https://example.com\", \"extractionconfig\": { \"type\": \"llm\", \"params\": { \"provider\": \"openai/gpt-4\", \"instruction\": \"Extract main topics from the page\" } } } Make the request (add headers if using API security) response = requests.post(\"http://localhost:11235/crawl\", json=request) > Note : Remember to add .env to your .gitignore to keep your API keys secure! Usage Examples \ud83d\udcdd Basic Crawling request = { \"urls\": \"https://www.nbcnews.com/business\", \"priority\": 10 } response = requests.post(\"http://localhost:11235/crawl\", json=request) taskid = response.json()[\"taskid\"] Get results result = requests.get(f\"http://localhost:11235/task/{taskid}\") Structured Data Extraction schema = { \"name\": \"Crypto Prices\", \"baseSelector\": \".cds-tableRow-t45thuk\", \"fields\": [ { \"name\": \"crypto\", \"selector\": \"td:nth-child(1) h2\", \"type\": \"text\", }, { \"name\": \"price\", \"selector\": \"td:nth-child(2)\", \"type\": \"text\", } ], } request = { \"urls\": \"https://www.coinbase.com/explore\", \"extractionconfig\": { \"type\": \"jsoncss\", \"params\": {\"schema\": schema} } } Dynamic Content Handling request = { \"urls\": \"https://www.nbcnews.com/business\", \"jscode\": [ \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\" ], \"waitfor\": \"article.tease-card:nth-child(10)\" } AI-Powered Extraction (Full Version) request = { \"urls\": \"https://www.nbcnews.com/business\", \"extractionconfig\": { \"type\": \"cosine\", \"params\": { \"semanticfilter\": \"business finance economy\", \"wordcountthreshold\": 10, \"maxdist\": 0.2, \"topk\": 3 } } } Platform-Specific Instructions \ud83d\udcbb macOS docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic Ubuntu Basic version docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic With GPU support docker pull unclecode/crawl4ai:gpu docker run --gpus all -p 11235:11235 unclecode/crawl4ai:gpu Windows (PowerShell) docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic Testing \ud83e\uddea Save this as testdocker.py: import requests import json import time import sys class Crawl4AiTester: def init(self, baseurl: str = \"http://localhost:11235\"): self.baseurl = baseurl def submitandwait(self, requestdata: dict, timeout: int = 300) -> dict: Submit crawl job response = requests.post(f\"{self.baseurl}/crawl\", json=requestdata) taskid = response.json()[\"taskid\"] print(f\"Task ID: {taskid}\") Poll for result starttime = time.time() while True: if time.time() - starttime > timeout: raise TimeoutError(f\"Task {taskid} timeout\") result = requests.get(f\"{self.baseurl}/task/{taskid}\") status = result.json() if status[\"status\"] == \"completed\": return status time.sleep(2) def testdeployment(): tester = Crawl4AiTester() Test basic crawl request = { \"urls\": \"https://www.nbcnews.com/business\", \"priority\": 10 } result = tester.submitandwait(request) print(\"Basic crawl successful!\") print(f\"Content length: {len(result['result']['markdown'])}\") if name == \"main\": testdeployment() Advanced Configuration \u2699\ufe0f Crawler Parameters The crawlerparams field allows you to configure the browser instance and crawling behavior. Here are key parameters you can use: request = { \"urls\": \"https://example.com\", \"crawlerparams\": { Browser Configuration \"headless\": True, Run in headless mode \"browsertype\": \"chromium\", chromium/firefox/webkit \"useragent\": \"custom-agent\", Custom user agent \"proxy\": \"http://proxy:8080\", Proxy configuration Performance & Behavior \"pagetimeout\": 30000, Page load timeout (ms) \"verbose\": True, Enable detailed logging \"semaphorecount\": 5, Concurrent request limit Anti-Detection Features \"simulateuser\": True, Simulate human behavior \"magic\": True, Advanced anti-detection \"overridenavigator\": True, Override navigator properties Session Management \"userdatadir\": \"./browser-data\", Browser profile location \"usemanagedbrowser\": True, Use persistent browser } } Extra Parameters The extra field allows passing additional parameters directly to the crawler's arun function: request = { \"urls\": \"https://example.com\", \"extra\": { \"wordcountthreshold\": 10, Min words per block \"onlytext\": True, Extract only text \"bypasscache\": True, Force fresh crawl \"processiframes\": True, Include iframe content } } Complete Examples 1. Advanced News Crawling request = { \"urls\": \"https://www.nbcnews.com/business\", \"crawlerparams\": { \"headless\": True, \"pagetimeout\": 30000, \"removeoverlayelements\": True Remove popups }, \"extra\": { \"wordcountthreshold\": 50, Longer content blocks \"bypasscache\": True Fresh content }, \"cssselector\": \".article-body\" } 2. Anti-Detection Configuration request = { \"urls\": \"https://example.com\", \"crawlerparams\": { \"simulateuser\": True, \"magic\": True, \"overridenavigator\": True, \"useragent\": \"Mozilla/5.0 ...\", \"headers\": { \"Accept-Language\": \"en-US,en;q=0.9\" } } } 3. LLM Extraction with Custom Parameters request = { \"urls\": \"https://openai.com/pricing\", \"extractionconfig\": { \"type\": \"llm\", \"params\": { \"provider\": \"openai/gpt-4\", \"schema\": pricingschema } }, \"crawlerparams\": { \"verbose\": True, \"pagetimeout\": 60000 }, \"extra\": { \"wordcountthreshold\": 1, \"onlytext\": True } } 4. Session-Based Dynamic Content request = { \"urls\": \"https://example.com\", \"crawlerparams\": { \"sessionid\": \"dynamicsession\", \"headless\": False, \"pagetimeout\": 60000 }, \"jscode\": [\"window.scrollTo(0, document.body.scrollHeight);\"], \"waitfor\": \"js:() => document.querySelectorAll('.item').length > 10\", \"extra\": { \"delaybeforereturnhtml\": 2.0 } } 5. Screenshot with Custom Timing request = { \"urls\": \"https://example.com\", \"screenshot\": True, \"crawlerparams\": { \"headless\": True, \"screenshotwaitfor\": \".main-content\" }, \"extra\": { \"delaybeforereturnhtml\": 3.0 } } Parameter Reference Table Category | Parameter | Type | Description ---|---|---|--- Browser | headless | bool | Run browser in headless mode Browser | browsertype | str | Browser engine selection Browser | useragent | str | Custom user agent string Network | proxy | str | Proxy server URL Network | headers | dict | Custom HTTP headers Timing | pagetimeout | int | Page load timeout (ms) Timing | delaybeforereturnhtml | float | Wait before capture Anti-Detection | simulateuser | bool | Human behavior simulation Anti-Detection | magic | bool | Advanced protection Session | sessionid | str | Browser session ID Session | userdatadir | str | Profile directory Content | wordcountthreshold | int | Minimum words per block Content | onlytext | bool | Text-only extraction Content | processiframes | bool | Include iframe content Debug | verbose | bool | Detailed logging Debug | logconsole | bool | Browser console logs Troubleshooting \ud83d\udd0d Common Issues 1. Connection Refused Error: Connection refused at localhost:11235 Solution: Ensure the container is running and ports are properly mapped. 2. Resource Limits Error: No available slots Solution: Increase MAXCONCURRENTTASKS or container resources. 3. GPU Access Error: GPU not found Solution: Ensure proper NVIDIA drivers and use --gpus all flag. Debug Mode Access container for debugging: docker run -it --entrypoint /bin/bash unclecode/crawl4ai:all View container logs: docker logs [containerid] Best Practices \ud83c\udf1f 1. Resource Management - Set appropriate memory and CPU limits - Monitor resource usage via health endpoint - Use basic version for simple crawling tasks 2. Scaling - Use multiple containers for high load - Implement proper load balancing - Monitor performance metrics 3. Security - Use environment variables for sensitive data - Implement proper network isolation - Regular security updates API Reference \ud83d\udcda Health Check GET /health Submit Crawl Task POST /crawl Content-Type: application/json { \"urls\": \"string or array\", \"extractionconfig\": { \"type\": \"basic|llm|cosine|jsoncss\", \"params\": {} }, \"priority\": 1-10, \"ttl\": 3600 } Get Task Status GET /task/{taskid} For more details, visit the official documentation. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/blog/\n\nTitle: Blog Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Crawl4AI Blog Latest Release Project History Stay Updated Crawl4AI Blog Welcome to the Crawl4AI blog! Here you'll find detailed release notes, technical insights, and updates about the project. Whether you're looking for the latest improvements or want to dive deep into web crawling techniques, this is the place. Latest Release 0.4.2 - Configurable Crawlers, Session Management, and Smarter Screenshots December 12, 2024 The 0.4.2 update brings massive improvements to configuration, making crawlers and browsers easier to manage with dedicated objects. You can now import/export local storage for seamless session management. Plus, long-page screenshots are faster and cleaner, and full-page PDF exports are now possible. Check out all the new features to make your crawling experience even smoother. Read full release notes \u2192 0.4.1 - Smarter Crawling with Lazy-Load Handling, Text-Only Mode, and More December 8, 2024 This release brings major improvements to handling lazy-loaded images, a blazing-fast Text-Only Mode, full-page scanning for infinite scrolls, dynamic viewport adjustments, and session reuse for efficient crawling. If you're looking to improve speed, reliability, or handle dynamic content with ease, this update has you covered. Read full release notes \u2192 0.4.0 - Major Content Filtering Update December 1, 2024 Introduced significant improvements to content filtering, multi-threaded environment handling, and user-agent generation. This release features the new PruningContentFilter, enhanced thread safety, and improved test coverage. Read full release notes \u2192 Project History Curious about how Crawl4AI has evolved? Check out our complete changelog for a detailed history of all versions and updates. Stay Updated Star us on GitHub Follow @unclecode on Twitter Join our community discussions on GitHub Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/simple-crawling/\n\nTitle: Simple Crawling - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Simple Crawling Basic Usage Understanding the Response Adding Basic Options Handling Errors Logging and Debugging Complete Example Simple Crawling This guide covers the basics of web crawling with Crawl4AI. You'll learn how to set up a crawler, make your first request, and understand the response. Basic Usage Set up a simple crawl using BrowserConfig and CrawlerRunConfig: import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai.asyncconfigs import BrowserConfig, CrawlerRunConfig async def main(): browserconfig = BrowserConfig() Default browser configuration runconfig = CrawlerRunConfig() Default crawl run configuration async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun( url=\"https://example.com\", config=runconfig ) print(result.markdown) Print clean markdown content if name == \"main\": asyncio.run(main()) Understanding the Response The arun() method returns a CrawlResult object with several useful properties. Here's a quick overview (see CrawlResult for complete details): result = await crawler.arun( url=\"https://example.com\", config=CrawlerRunConfig(fitmarkdown=True) ) Different content formats print(result.html) Raw HTML print(result.cleanedhtml) Cleaned HTML print(result.markdown) Markdown version print(result.fitmarkdown) Most relevant content in markdown Check success status print(result.success) True if crawl succeeded print(result.statuscode) HTTP status code (e.g., 200, 404) Access extracted media and links print(result.media) Dictionary of found media (images, videos, audio) print(result.links) Dictionary of internal and external links Adding Basic Options Customize your crawl using CrawlerRunConfig: runconfig = CrawlerRunConfig( wordcountthreshold=10, Minimum words per content block excludeexternallinks=True, Remove external links removeoverlayelements=True, Remove popups/modals processiframes=True Process iframe content ) result = await crawler.arun( url=\"https://example.com\", config=runconfig ) Handling Errors Always check if the crawl was successful: runconfig = CrawlerRunConfig() result = await crawler.arun(url=\"https://example.com\", config=runconfig) if not result.success: print(f\"Crawl failed: {result.errormessage}\") print(f\"Status code: {result.statuscode}\") Logging and Debugging Enable verbose logging in BrowserConfig: browserconfig = BrowserConfig(verbose=True) async with AsyncWebCrawler(config=browserconfig) as crawler: runconfig = CrawlerRunConfig() result = await crawler.arun(url=\"https://example.com\", config=runconfig) Complete Example Here's a more comprehensive example demonstrating common usage patterns: import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai.asyncconfigs import BrowserConfig, CrawlerRunConfig, CacheMode async def main(): browserconfig = BrowserConfig(verbose=True) runconfig = CrawlerRunConfig( Content filtering wordcountthreshold=10, excludedtags=['form', 'header'], excludeexternallinks=True, Content processing processiframes=True, removeoverlayelements=True, Cache control cachemode=CacheMode.ENABLED Use cache if available ) async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun( url=\"https://example.com\", config=runconfig ) if result.success: Print clean content print(\"Content:\", result.markdown[:500]) First 500 chars Process images for image in result.media[\"images\"]: print(f\"Found image: {image['src']}\") Process links for link in result.links[\"internal\"]: print(f\"Internal link: {link['href']}\") else: print(f\"Crawl failed: {result.errormessage}\") if name == \"main\": asyncio.run(main()) Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/crawler-result/\n\nTitle: Crawler Result - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Crawl Result and Output 1. The CrawlResult Model 2. HTML Variants 3. Markdown Generation 4. Structured Extraction: extractedcontent 5. More Fields: Links, Media, and More 6. Accessing These Fields 7. Next Steps Crawl Result and Output When you call arun() on a page, Crawl4AI returns a CrawlResult object containing everything you might need\u2014raw HTML, a cleaned version, optional screenshots or PDFs, structured extraction results, and more. This document explains those fields and how they map to different output types. 1. The CrawlResult Model Below is the core schema. Each field captures a different aspect of the crawl\u2019s result: class MarkdownGenerationResult(BaseModel): rawmarkdown: str markdownwithcitations: str referencesmarkdown: str fitmarkdown: Optional[str] = None fithtml: Optional[str] = None class CrawlResult(BaseModel): url: str html: str success: bool cleanedhtml: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} downloadedfiles: Optional[List[str]] = None screenshot: Optional[str] = None pdf : Optional[bytes] = None markdown: Optional[Union[str, MarkdownGenerationResult]] = None markdownv2: Optional[MarkdownGenerationResult] = None extractedcontent: Optional[str] = None metadata: Optional[dict] = None errormessage: Optional[str] = None sessionid: Optional[str] = None responseheaders: Optional[dict] = None statuscode: Optional[int] = None sslcertificate: Optional[SSLCertificate] = None class Config: arbitrarytypesallowed = True Table: Key Fields in CrawlResult Field (Name & Type) | Description ---|--- url (str) | The final or actual URL crawled (in case of redirects). html (str) | Original, unmodified page HTML. Good for debugging or custom processing. success (bool) | True if the crawl completed without major errors, else False. cleanedhtml (Optional[str]) | Sanitized HTML with scripts/styles removed; can exclude tags if configured via excludedtags etc. media (Dict[str, List[Dict]]) | Extracted media info (images, audio, etc.), each with attributes like src, alt, score, etc. links (Dict[str, List[Dict]]) | Extracted link data, split by internal and external. Each link usually has href, text, etc. downloadedfiles (Optional[List[str]]) | If acceptdownloads=True in BrowserConfig, this lists the filepaths of saved downloads. screenshot (Optional[str]) | Screenshot of the page (base64-encoded) if screenshot=True. pdf (Optional[bytes]) | PDF of the page if pdf=True. markdown (Optional[str or MarkdownGenerationResult]) | For now, markdownv2 holds a MarkdownGenerationResult. Over time, this will be consolidated into markdown. The generator can provide raw markdown, citations, references, and optionally fitmarkdown. markdownv2 (Optional[MarkdownGenerationResult]) | Legacy field for detailed markdown output. This will be replaced by markdown soon. extractedcontent (Optional[str]) | The output of a structured extraction (CSS/LLM-based) stored as JSON string or other text. metadata (Optional[dict]) | Additional info about the crawl or extracted data. errormessage (Optional[str]) | If success=False, contains a short description of what went wrong. sessionid (Optional[str]) | The ID of the session used for multi-page or persistent crawling. responseheaders (Optional[dict]) | HTTP response headers, if captured. statuscode (Optional[int]) | HTTP status code (e.g., 200 for OK). sslcertificate (Optional[SSLCertificate]) | SSL certificate info if fetchsslcertificate=True. 2. HTML Variants html: Raw HTML Crawl4AI preserves the exact HTML as result.html. Useful for: Debugging page issues or checking the original content. Performing your own specialized parse if needed. cleanedhtml: Sanitized If you specify any cleanup or exclusion parameters in CrawlerRunConfig (like excludedtags, removeforms, etc.), you\u2019ll see the result here: config = CrawlerRunConfig( excludedtags=[\"form\", \"header\", \"footer\"], keepdataattributes=False ) result = await crawler.arun(\"https://example.com\", config=config) print(result.cleanedhtml) Freed of forms, header, footer, data- attributes 3. Markdown Generation 3.1 markdownv2 (Legacy) vs markdown markdownv2 : The current location for detailed markdown output, returning a MarkdownGenerationResult object. markdown : Eventually, we\u2019re merging these fields. For now, you might see result.markdownv2 used widely in code examples. MarkdownGenerationResult Fields: Field | Description ---|--- rawmarkdown | The basic HTML\u2192Markdown conversion. markdownwithcitations | Markdown including inline citations that reference links at the end. referencesmarkdown | The references/citations themselves (if citations=True). fitmarkdown | The filtered/\u201cfit\u201d markdown if a content filter was used. fithtml | The filtered HTML that generated fitmarkdown. 3.2 Basic Example with a Markdown Generator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator config = CrawlerRunConfig( markdowngenerator=DefaultMarkdownGenerator( options={\"citations\": True, \"bodywidth\": 80} e.g. pass html2text style options ) ) result = await crawler.arun(url=\"https://example.com\", config=config) mdres = result.markdownv2 or eventually 'result.markdown' print(mdres.rawmarkdown[:500]) print(mdres.markdownwithcitations) print(mdres.referencesmarkdown) Note : If you use a filter like PruningContentFilter, you\u2019ll get fitmarkdown and fithtml as well. 4. Structured Extraction: extractedcontent If you run a JSON-based extraction strategy (CSS, XPath, LLM, etc.), the structured data is not stored in markdown\u2014it\u2019s placed in result.extractedcontent as a JSON string (or sometimes plain text). Example: CSS Extraction with raw:// HTML import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): schema = { \"name\": \"Example Items\", \"baseSelector\": \"div.item\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } rawhtml = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"raw://\" + rawhtml, config=CrawlerRunConfig( cachemode=CacheMode.BYPASS, extractionstrategy=JsonCssExtractionStrategy(schema) ) ) data = json.loads(result.extractedcontent) print(data) if name == \"main\": asyncio.run(main()) Here: - url=\"raw://...\" passes the HTML content directly, no network requests. - The CSS extraction strategy populates result.extractedcontent with the JSON array [{\"title\": \"...\", \"link\": \"...\"}]. 5. More Fields: Links, Media, and More 5.1 links A dictionary, typically with \"internal\" and \"external\" lists. Each entry might have href, text, title, etc. This is automatically captured if you haven\u2019t disabled link extraction. print(result.links[\"internal\"][:3]) Show first 3 internal links 5.2 media Similarly, a dictionary with \"images\", \"audio\", \"video\", etc. Each item could include src, alt, score, and more, if your crawler is set to gather them. images = result.media.get(\"images\", []) for img in images: print(\"Image URL:\", img[\"src\"], \"Alt:\", img.get(\"alt\")) 5.3 screenshot and pdf If you set screenshot=True or pdf=True in CrawlerRunConfig , then: result.screenshot contains a base64-encoded PNG string. result.pdf contains raw PDF bytes (you can write them to a file). with open(\"page.pdf\", \"wb\") as f: f.write(result.pdf) 5.4 sslcertificate If fetchsslcertificate=True, result.sslcertificate holds details about the site\u2019s SSL cert, such as issuer, validity dates, etc. 6. Accessing These Fields After you run: result = await crawler.arun(url=\"https://example.com\", config=someconfig) Check any field: if result.success: print(result.statuscode, result.responseheaders) print(\"Links found:\", len(result.links.get(\"internal\", []))) if result.markdownv2: print(\"Markdown snippet:\", result.markdownv2.rawmarkdown[:200]) if result.extractedcontent: print(\"Structured JSON:\", result.extractedcontent) else: print(\"Error:\", result.errormessage) Remember : Use result.markdownv2 for now. It will eventually become result.markdown. 7. Next Steps Markdown Generation : Dive deeper into how to configure DefaultMarkdownGenerator and various filters. Content Filtering : Learn how to use BM25ContentFilter and PruningContentFilter. Session & Hooks: If you want to manipulate the page or preserve state across multiple arun() calls, see the hooking or session docs. LLM Extraction : For complex or unstructured content requiring AI-driven parsing, check the LLM-based strategies doc. Enjoy exploring all that CrawlResult offers\u2014whether you need raw HTML, sanitized output, markdown, or fully structured data, Crawl4AI has you covered! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/browser-crawler-config/\n\nTitle: Browser & Crawler Config - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Browser & Crawler Configuration (Quick Overview) 1. BrowserConfig Essentials 2. CrawlerRunConfig Essentials 3. Putting It All Together 4. Next Steps 5. Conclusion Browser & Crawler Configuration (Quick Overview) Crawl4AI\u2019s flexibility stems from two key classes: 1. BrowserConfig \u2013 Dictates how the browser is launched and behaves (e.g., headless or visible, proxy, user agent). 2. CrawlerRunConfig \u2013 Dictates how each crawl operates (e.g., caching, extraction, timeouts, JavaScript code to run, etc.). In most examples, you create one BrowserConfig for the entire crawler session, then pass a fresh or re-used CrawlerRunConfig whenever you call arun(). This tutorial shows the most commonly used parameters. If you need advanced or rarely used fields, see the Configuration Parameters. 1. BrowserConfig Essentials class BrowserConfig: def init( browsertype=\"chromium\", headless=True, proxyconfig=None, viewportwidth=1080, viewportheight=600, verbose=True, usepersistentcontext=False, userdatadir=None, cookies=None, headers=None, useragent=None, textmode=False, lightmode=False, extraargs=None, ... other advanced parameters omitted here ): ... Key Fields to Note 1. browsertype - Options: \"chromium\", \"firefox\", or \"webkit\". - Defaults to \"chromium\". - If you need a different engine, specify it here. 2. headless - True: Runs the browser in headless mode (invisible browser). - False: Runs the browser in visible mode, which helps with debugging. 3. proxyconfig - A dictionary with fields like: { \"server\": \"http://proxy.example.com:8080\", \"username\": \"...\", \"password\": \"...\" } Leave as None if a proxy is not required. 4. viewportwidth & viewportheight: - The initial window size. - Some sites behave differently with smaller or bigger viewports. 5. verbose : - If True, prints extra logs. - Handy for debugging. 6. usepersistentcontext : - If True, uses a persistent browser profile, storing cookies/local storage across runs. - Typically also set userdatadir to point to a folder. 7. cookies & headers : - If you want to start with specific cookies or add universal HTTP headers, set them here. - E.g. cookies=[{\"name\": \"session\", \"value\": \"abc123\", \"domain\": \"example.com\"}]. 8. useragent : - Custom User-Agent string. If None, a default is used. - You can also set useragentmode=\"random\" for randomization (if you want to fight bot detection). 9. textmode & lightmode : - textmode=True disables images, possibly speeding up text-only crawls. - lightmode=True turns off certain background features for performance. 10. extraargs : - Additional flags for the underlying browser. - E.g. [\"--disable-extensions\"]. Helper Methods Both configuration classes provide a clone() method to create modified copies: Create a base browser config basebrowser = BrowserConfig( browsertype=\"chromium\", headless=True, textmode=True ) Create a visible browser config for debugging debugbrowser = basebrowser.clone( headless=False, verbose=True ) Minimal Example : from crawl4ai import AsyncWebCrawler, BrowserConfig browserconf = BrowserConfig( browsertype=\"firefox\", headless=False, textmode=True ) async with AsyncWebCrawler(config=browserconf) as crawler: result = await crawler.arun(\"https://example.com\") print(result.markdown[:300]) 2. CrawlerRunConfig Essentials class CrawlerRunConfig: def init( wordcountthreshold=200, extractionstrategy=None, markdowngenerator=None, cachemode=None, jscode=None, waitfor=None, screenshot=False, pdf=False, enableratelimiting=False, ratelimitconfig=None, memorythresholdpercent=70.0, checkinterval=1.0, maxsessionpermit=20, displaymode=None, verbose=True, stream=False, Enable streaming for arunmany() ... other advanced parameters omitted ): ... Key Fields to Note 1. wordcountthreshold : - The minimum word count before a block is considered. - If your site has lots of short paragraphs or items, you can lower it. 2. extractionstrategy : - Where you plug in JSON-based extraction (CSS, LLM, etc.). - If None, no structured extraction is done (only raw/cleaned HTML + markdown). 3. markdowngenerator : - E.g., DefaultMarkdownGenerator(...), controlling how HTML\u2192Markdown conversion is done. - If None, a default approach is used. 4. cachemode : - Controls caching behavior (ENABLED, BYPASS, DISABLED, etc.). - If None, defaults to some level of caching or you can specify CacheMode.ENABLED. 5. jscode : - A string or list of JS strings to execute. - Great for \u201cLoad More\u201d buttons or user interactions. 6. waitfor : - A CSS or JS expression to wait for before extracting content. - Common usage: waitfor=\"css:.main-loaded\" or waitfor=\"js:() => window.loaded === true\". 7. screenshot & pdf : - If True, captures a screenshot or PDF after the page is fully loaded. - The results go to result.screenshot (base64) or result.pdf (bytes). 8. verbose : - Logs additional runtime details. - Overlaps with the browser\u2019s verbosity if also set to True in BrowserConfig. 9. enableratelimiting : - If True, enables rate limiting for batch processing. - Requires ratelimitconfig to be set. 10. ratelimitconfig : - A RateLimitConfig object controlling rate limiting behavior. - See below for details. 11. memorythresholdpercent : - The memory threshold (as a percentage) to monitor. - If exceeded, the crawler will pause or slow down. 12. checkinterval : - The interval (in seconds) to check system resources. - Affects how often memory and CPU usage are monitored. 13. maxsessionpermit : - The maximum number of concurrent crawl sessions. - Helps prevent overwhelming the system. 14. displaymode : - The display mode for progress information (DETAILED, BRIEF, etc.). - Affects how much information is printed during the crawl. Helper Methods The clone() method is particularly useful for creating variations of your crawler configuration: Create a base configuration baseconfig = CrawlerRunConfig( cachemode=CacheMode.ENABLED, wordcountthreshold=200, waituntil=\"networkidle\" ) Create variations for different use cases streamconfig = baseconfig.clone( stream=True, Enable streaming mode cachemode=CacheMode.BYPASS ) debugconfig = baseconfig.clone( pagetimeout=120000, Longer timeout for debugging verbose=True ) The clone() method: - Creates a new instance with all the same settings - Updates only the specified parameters - Leaves the original configuration unchanged - Perfect for creating variations without repeating all parameters Rate Limiting & Resource Management For batch processing with arunmany(), you can enable intelligent rate limiting: from crawl4ai import RateLimitConfig config = CrawlerRunConfig( enableratelimiting=True, ratelimitconfig=RateLimitConfig( basedelay=(1.0, 3.0), Random delay range maxdelay=60.0, Max delay after rate limits maxretries=3, Retries before giving up ratelimitcodes=[429, 503] Status codes to watch ), memorythresholdpercent=70.0, Memory threshold checkinterval=1.0, Resource check interval maxsessionpermit=20, Max concurrent crawls displaymode=\"DETAILED\" Progress display mode ) This configuration: - Implements intelligent rate limiting per domain - Monitors system resources - Provides detailed progress information - Manages concurrent crawls efficiently Minimal Example : from crawl4ai import AsyncWebCrawler, CrawlerRunConfig crawlconf = CrawlerRunConfig( jscode=\"document.querySelector('button#loadMore')?.click()\", waitfor=\"css:.loaded-content\", screenshot=True, enableratelimiting=True, ratelimitconfig=RateLimitConfig( basedelay=(1.0, 3.0), maxdelay=60.0, maxretries=3, ratelimitcodes=[429, 503] ), stream=True Enable streaming ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=\"https://example.com\", config=crawlconf) print(result.screenshot[:100]) Base64-encoded PNG snippet 3. Putting It All Together In a typical scenario, you define one BrowserConfig for your crawler session, then create one or more CrawlerRunConfig depending on each call\u2019s needs: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): 1) Browser config: headless, bigger viewport, no proxy browserconf = BrowserConfig( headless=True, viewportwidth=1280, viewportheight=720 ) 2) Example extraction strategy schema = { \"name\": \"Articles\", \"baseSelector\": \"div.article\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } extraction = JsonCssExtractionStrategy(schema) 3) Crawler run config: skip cache, use extraction runconf = CrawlerRunConfig( extractionstrategy=extraction, cachemode=CacheMode.BYPASS, enableratelimiting=True, ratelimitconfig=RateLimitConfig( basedelay=(1.0, 3.0), maxdelay=60.0, maxretries=3, ratelimitcodes=[429, 503] ) ) async with AsyncWebCrawler(config=browserconf) as crawler: 4) Execute the crawl result = await crawler.arun(url=\"https://example.com/news\", config=runconf) if result.success: print(\"Extracted content:\", result.extractedcontent) else: print(\"Error:\", result.errormessage) if name == \"main\": asyncio.run(main()) 4. Next Steps For a detailed list of available parameters (including advanced ones), see: BrowserConfig and CrawlerRunConfig Reference You can explore topics like: Custom Hooks & Auth (Inject JavaScript or handle login forms). Session Management (Re-use pages, preserve state across multiple calls). Magic Mode or Identity-based Crawling (Fight bot detection by simulating user behavior). Advanced Caching (Fine-tune read/write cache modes). 5. Conclusion BrowserConfig and CrawlerRunConfig give you straightforward ways to define: Which browser to launch, how it should run, and any proxy or user agent needs. How each crawl should behave\u2014caching, timeouts, JavaScript code, extraction strategies, etc. Use them together for clear, maintainable code, and when you need more specialized behavior, check out the advanced parameters in the reference docs. Happy crawling! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/markdown-generation/\n\nTitle: Markdown Generation - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Markdown Generation Basics 1. Quick Example 2. How Markdown Generation Works 3. Configuring the Default Markdown Generator 4. Content Filters 5. Using Fit Markdown 6. The MarkdownGenerationResult Object 7. Combining Filters (BM25 + Pruning) in Two Passes 8. Common Pitfalls & Tips 9. Summary & Next Steps Markdown Generation Basics One of Crawl4AI\u2019s core features is generating clean, structured markdown from web pages. Originally built to solve the problem of extracting only the \u201cactual\u201d content and discarding boilerplate or noise, Crawl4AI\u2019s markdown system remains one of its biggest draws for AI workflows. In this tutorial, you\u2019ll learn: 1. How to configure the Default Markdown Generator 2. How content filters (BM25 or Pruning) help you refine markdown and discard junk 3. The difference between raw markdown (result.markdown) and filtered markdown (fitmarkdown) > Prerequisites - You\u2019ve completed or read AsyncWebCrawler Basics to understand how to run a simple crawl. - You know how to configure CrawlerRunConfig. 1. Quick Example Here\u2019s a minimal code snippet that uses the DefaultMarkdownGenerator with no additional filtering: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator async def main(): config = CrawlerRunConfig( markdowngenerator=DefaultMarkdownGenerator() ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\", config=config) if result.success: print(\"Raw Markdown Output:\\n\") print(result.markdown) The unfiltered markdown from the page else: print(\"Crawl failed:\", result.errormessage) if name == \"main\": asyncio.run(main()) What\u2019s happening? - CrawlerRunConfig( markdowngenerator = DefaultMarkdownGenerator() ) instructs Crawl4AI to convert the final HTML into markdown at the end of each crawl. - The resulting markdown is accessible via result.markdown. 2. How Markdown Generation Works 2.1 HTML-to-Text Conversion (Forked & Modified) Under the hood, DefaultMarkdownGenerator uses a specialized HTML-to-text approach that: Preserves headings, code blocks, bullet points, etc. Removes extraneous tags (scripts, styles) that don\u2019t add meaningful content. Can optionally generate references for links or skip them altogether. A set of options (passed as a dict) allows you to customize precisely how HTML converts to markdown. These map to standard html2text-like configuration plus your own enhancements (e.g., ignoring internal links, preserving certain tags verbatim, or adjusting line widths). 2.2 Link Citations & References By default, the generator can convert <a href=\"...\"> elements into [text][1] citations, then place the actual links at the bottom of the document. This is handy for research workflows that demand references in a structured manner. 2.3 Optional Content Filters Before or after the HTML-to-Markdown step, you can apply a content filter (like BM25 or Pruning) to reduce noise and produce a \u201cfitmarkdown\u201d\u2014a heavily pruned version focusing on the page\u2019s main text. We\u2019ll cover these filters shortly. 3. Configuring the Default Markdown Generator You can tweak the output by passing an options dict to DefaultMarkdownGenerator. For example: from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): Example: ignore all links, don't escape HTML, and wrap text at 80 characters mdgenerator = DefaultMarkdownGenerator( options={ \"ignorelinks\": True, \"escapehtml\": False, \"bodywidth\": 80 } ) config = CrawlerRunConfig( markdowngenerator=mdgenerator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com/docs\", config=config) if result.success: print(\"Markdown:\\n\", result.markdown[:500]) Just a snippet else: print(\"Crawl failed:\", result.errormessage) if name == \"main\": import asyncio asyncio.run(main()) Some commonly used options: ignorelinks (bool): Whether to remove all hyperlinks in the final markdown. ignoreimages (bool): Remove all ![image]() references. escapehtml (bool): Turn HTML entities into text (default is often True). bodywidth (int): Wrap text at N characters. 0 or None means no wrapping. skipinternallinks (bool): If True, omit #localAnchors or internal links referencing the same page. includesupsub (bool): Attempt to handle <sup> / <sub> in a more readable way. 4. Content Filters Content filters selectively remove or rank sections of text before turning them into Markdown. This is especially helpful if your page has ads, nav bars, or other clutter you don\u2019t want. 4.1 BM25ContentFilter If you have a search query , BM25 is a good choice: from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator from crawl4ai.contentfilterstrategy import BM25ContentFilter from crawl4ai import CrawlerRunConfig bm25filter = BM25ContentFilter( userquery=\"machine learning\", bm25threshold=1.2, usestemming=True ) mdgenerator = DefaultMarkdownGenerator( contentfilter=bm25filter, options={\"ignorelinks\": True} ) config = CrawlerRunConfig(markdowngenerator=mdgenerator) userquery : The term you want to focus on. BM25 tries to keep only content blocks relevant to that query. bm25threshold : Raise it to keep fewer blocks; lower it to keep more. usestemming : If True, variations of words match (e.g., \u201clearn,\u201d \u201clearning,\u201d \u201clearnt\u201d). No query provided? BM25 tries to glean a context from page metadata, or you can simply treat it as a scorched-earth approach that discards text with low generic score. Realistically, you want to supply a query for best results. 4.2 PruningContentFilter If you don\u2019t have a specific query, or if you just want a robust \u201cjunk remover,\u201d use PruningContentFilter. It analyzes text density, link density, HTML structure, and known patterns (like \u201cnav,\u201d \u201cfooter\u201d) to systematically prune extraneous or repetitive sections. from crawl4ai.contentfilterstrategy import PruningContentFilter prunefilter = PruningContentFilter( threshold=0.5, thresholdtype=\"fixed\", or \"dynamic\" minwordthreshold=50 ) threshold : Score boundary. Blocks below this score get removed. thresholdtype : \"fixed\": Straight comparison (score >= threshold keeps the block). \"dynamic\": The filter adjusts threshold in a data-driven manner. minwordthreshold : Discard blocks under N words as likely too short or unhelpful. When to Use PruningContentFilter - You want a broad cleanup without a user query. - The page has lots of repeated sidebars, footers, or disclaimers that hamper text extraction. 4.3 LLMContentFilter For intelligent content filtering and high-quality markdown generation, you can use the LLMContentFilter. This filter leverages LLMs to generate relevant markdown while preserving the original content's meaning and structure: from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig from crawl4ai.contentfilterstrategy import LLMContentFilter async def main(): Initialize LLM filter with specific instruction filter = LLMContentFilter( provider=\"openai/gpt-4o\", or your preferred provider apitoken=\"your-api-token\", or use environment variable instruction=\"\"\" Focus on extracting the core educational content. Include: - Key concepts and explanations - Important code examples - Essential technical details Exclude: - Navigation elements - Sidebars - Footer content Format the output as clean markdown with proper code blocks and headers. \"\"\", chunktokenthreshold=4096, Adjust based on your needs verbose=True ) config = CrawlerRunConfig( contentfilter=filter ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\", config=config) print(result.fitmarkdown) Filtered markdown content Key Features: - Intelligent Filtering : Uses LLMs to understand and extract relevant content while maintaining context - Customizable Instructions : Tailor the filtering process with specific instructions - Chunk Processing : Handles large documents by processing them in chunks (controlled by chunktokenthreshold) - Parallel Processing : For better performance, use smaller chunktokenthreshold (e.g., 2048 or 4096) to enable parallel processing of content chunks Two Common Use Cases: 1. Exact Content Preservation : filter = LLMContentFilter( instruction=\"\"\" Extract the main educational content while preserving its original wording and substance completely. 1. Maintain the exact language and terminology 2. Keep all technical explanations and examples intact 3. Preserve the original flow and structure 4. Remove only clearly irrelevant elements like navigation menus and ads \"\"\", chunktokenthreshold=4096 ) 2. Focused Content Extraction : filter = LLMContentFilter( instruction=\"\"\" Focus on extracting specific types of content: - Technical documentation - Code examples - API references Reformat the content into clear, well-structured markdown \"\"\", chunktokenthreshold=4096 ) > Performance Tip : Set a smaller chunktokenthreshold (e.g., 2048 or 4096) to enable parallel processing of content chunks. The default value is infinity, which processes the entire content as a single chunk. 5. Using Fit Markdown When a content filter is active, the library produces two forms of markdown inside result.markdownv2 or (if using the simplified field) result.markdown: 1. rawmarkdown : The full unfiltered markdown. 2. fitmarkdown : A \u201cfit\u201d version where the filter has removed or trimmed noisy segments. Note : > In earlier examples, you may see references to result.markdownv2. Depending on your library version, you might access result.markdown, result.markdownv2, or an object named MarkdownGenerationResult. The idea is the same: you\u2019ll have a raw version and a filtered (\u201cfit\u201d) version if a filter is used. import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator from crawl4ai.contentfilterstrategy import PruningContentFilter async def main(): config = CrawlerRunConfig( markdowngenerator=DefaultMarkdownGenerator( contentfilter=PruningContentFilter(threshold=0.6), options={\"ignorelinks\": True} ) ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://news.example.com/tech\", config=config) if result.success: print(\"Raw markdown:\\n\", result.markdown) If a filter is used, we also have .fitmarkdown: mdobject = result.markdownv2 or your equivalent print(\"Filtered markdown:\\n\", mdobject.fitmarkdown) else: print(\"Crawl failed:\", result.errormessage) if name == \"main\": asyncio.run(main()) 6. The MarkdownGenerationResult Object If your library stores detailed markdown output in an object like MarkdownGenerationResult, you\u2019ll see fields such as: rawmarkdown : The direct HTML-to-markdown transformation (no filtering). markdownwithcitations : A version that moves links to reference-style footnotes. referencesmarkdown : A separate string or section containing the gathered references. fitmarkdown : The filtered markdown if you used a content filter. fithtml : The corresponding HTML snippet used to generate fitmarkdown (helpful for debugging or advanced usage). Example : mdobj = result.markdownv2 your library\u2019s naming may vary print(\"RAW:\\n\", mdobj.rawmarkdown) print(\"CITED:\\n\", mdobj.markdownwithcitations) print(\"REFERENCES:\\n\", mdobj.referencesmarkdown) print(\"FIT:\\n\", mdobj.fitmarkdown) Why Does This Matter? - You can supply rawmarkdown to an LLM if you want the entire text. - Or feed fitmarkdown into a vector database to reduce token usage. - referencesmarkdown can help you keep track of link provenance. Below is a revised section under \u201cCombining Filters (BM25 + Pruning)\u201d that demonstrates how you can run two passes of content filtering without re-crawling, by taking the HTML (or text) from a first pass and feeding it into the second filter. It uses real code patterns from the snippet you provided for BM25ContentFilter , which directly accepts HTML strings (and can also handle plain text with minimal adaptation). 7. Combining Filters (BM25 + Pruning) in Two Passes You might want to prune out noisy boilerplate first (with PruningContentFilter), and then rank what\u2019s left against a user query (with BM25ContentFilter). You don\u2019t have to crawl the page twice. Instead: 1. First pass : Apply PruningContentFilter directly to the raw HTML from result.html (the crawler\u2019s downloaded HTML). 2. Second pass : Take the pruned HTML (or text) from step 1, and feed it into BM25ContentFilter, focusing on a user query. Two-Pass Example import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter, BM25ContentFilter from bs4 import BeautifulSoup async def main(): 1. Crawl with minimal or no markdown generator, just get raw HTML config = CrawlerRunConfig( If you only want raw HTML, you can skip passing a markdowngenerator or provide one but focus on .html in this example ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com/tech-article\", config=config) if not result.success or not result.html: print(\"Crawl failed or no HTML content.\") return rawhtml = result.html 2. First pass: PruningContentFilter on raw HTML pruningfilter = PruningContentFilter(threshold=0.5, minwordthreshold=50) filtercontent returns a list of \"text chunks\" or cleaned HTML sections prunedchunks = pruningfilter.filtercontent(rawhtml) This list is basically pruned content blocks, presumably in HTML or text form For demonstration, let's combine these chunks back into a single HTML-like string or you could do further processing. It's up to your pipeline design. prunedhtml = \"\\n\".join(prunedchunks) 3. Second pass: BM25ContentFilter with a user query bm25filter = BM25ContentFilter( userquery=\"machine learning\", bm25threshold=1.2, language=\"english\" ) returns a list of text chunks bm25chunks = bm25filter.filtercontent(prunedhtml) if not bm25chunks: print(\"Nothing matched the BM25 query after pruning.\") return 4. Combine or display final results finaltext = \"\\n---\\n\".join(bm25chunks) print(\"==== PRUNED OUTPUT (first pass) ====\") print(prunedhtml[:500], \"... (truncated)\") preview print(\"\\n==== BM25 OUTPUT (second pass) ====\") print(finaltext[:500], \"... (truncated)\") if name == \"main\": asyncio.run(main()) What\u2019s Happening? 1. Raw HTML : We crawl once and store the raw HTML in result.html. 2. PruningContentFilter : Takes HTML + optional parameters. It extracts blocks of text or partial HTML, removing headings/sections deemed \u201cnoise.\u201d It returns a list of text chunks. 3. Combine or Transform : We join these pruned chunks back into a single HTML-like string. (Alternatively, you could store them in a list for further logic\u2014whatever suits your pipeline.) 4. BM25ContentFilter : We feed the pruned string into BM25ContentFilter with a user query. This second pass further narrows the content to chunks relevant to \u201cmachine learning.\u201d No Re-Crawling : We used rawhtml from the first pass, so there\u2019s no need to run arun() again\u2014no second network request. Tips & Variations Plain Text vs. HTML : If your pruned output is mostly text, BM25 can still handle it; just keep in mind it expects a valid string input. If you supply partial HTML (like \"<p>some text</p>\"), it will parse it as HTML. Chaining in a Single Pipeline : If your code supports it, you can chain multiple filters automatically. Otherwise, manual two-pass filtering (as shown) is straightforward. Adjust Thresholds : If you see too much or too little text in step one, tweak threshold=0.5 or minwordthreshold=50. Similarly, bm25threshold=1.2 can be raised/lowered for more or fewer chunks in step two. One-Pass Combination? If your codebase or pipeline design allows applying multiple filters in one pass, you could do so. But often it\u2019s simpler\u2014and more transparent\u2014to run them sequentially, analyzing each step\u2019s result. Bottom Line : By manually chaining your filtering logic in two passes, you get powerful incremental control over the final content. First, remove \u201cglobal\u201d clutter with Pruning, then refine further with BM25-based query relevance\u2014without incurring a second network crawl. 8. Common Pitfalls & Tips 1. No Markdown Output? - Make sure the crawler actually retrieved HTML. If the site is heavily JS-based, you may need to enable dynamic rendering or wait for elements. - Check if your content filter is too aggressive. Lower thresholds or disable the filter to see if content reappears. 2. Performance Considerations - Very large pages with multiple filters can be slower. Consider cachemode to avoid re-downloading. - If your final use case is LLM ingestion, consider summarizing further or chunking big texts. 3. Take Advantage offitmarkdown - Great for RAG pipelines, semantic search, or any scenario where extraneous boilerplate is unwanted. - Still verify the textual quality\u2014some sites have crucial data in footers or sidebars. 4. Adjustinghtml2text Options - If you see lots of raw HTML slipping into the text, turn on escapehtml. - If code blocks look messy, experiment with markcode or handlecodeinpre. 9. Summary & Next Steps In this Markdown Generation Basics tutorial, you learned to: Configure the DefaultMarkdownGenerator with HTML-to-text options. Use BM25ContentFilter for query-specific extraction or PruningContentFilter for general noise removal. Distinguish between raw and filtered markdown (fitmarkdown). Leverage the MarkdownGenerationResult object to handle different forms of output (citations, references, etc.). Now you can produce high-quality Markdown from any website, focusing on exactly the content you need\u2014an essential step for powering AI models, summarization pipelines, or knowledge-base queries. Last Updated : 2025-01-01 Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/fit-markdown/\n\nTitle: Fit Markdown - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Fit Markdown with Pruning & BM25 1. How \u201cFit Markdown\u201d Works 2. PruningContentFilter 3. BM25ContentFilter 4. Accessing the \u201cFit\u201d Output 5. Code Patterns Recap 6. Combining with \u201cwordcountthreshold\u201d & Exclusions 7. Custom Filters 8. Final Thoughts Fit Markdown with Pruning & BM25 Fit Markdown is a specialized filtered version of your page\u2019s markdown, focusing on the most relevant content. By default, Crawl4AI converts the entire HTML into a broad rawmarkdown. With fit markdown, we apply a content filter algorithm (e.g., Pruning or BM25) to remove or rank low-value sections\u2014such as repetitive sidebars, shallow text blocks, or irrelevancies\u2014leaving a concise textual \u201ccore.\u201d 1. How \u201cFit Markdown\u201d Works 1.1 The contentfilter In CrawlerRunConfig , you can specify a contentfilter to shape how content is pruned or ranked before final markdown generation. A filter\u2019s logic is applied before or during the HTML\u2192Markdown process, producing: result.markdownv2.rawmarkdown (unfiltered) result.markdownv2.fitmarkdown (filtered or \u201cfit\u201d version) result.markdownv2.fithtml (the corresponding HTML snippet that produced fitmarkdown) > Note : We\u2019re currently storing the result in markdownv2, but eventually we\u2019ll unify it as result.markdown. 1.2 Common Filters 1. PruningContentFilter \u2013 Scores each node by text density, link density, and tag importance, discarding those below a threshold. 2. BM25ContentFilter \u2013 Focuses on textual relevance using BM25 ranking, especially useful if you have a specific user query (e.g., \u201cmachine learning\u201d or \u201cfood nutrition\u201d). 2. PruningContentFilter Pruning discards less relevant nodes based on text density, link density, and tag importance. It\u2019s a heuristic-based approach\u2014if certain sections appear too \u201cthin\u201d or too \u201cspammy,\u201d they\u2019re pruned. 2.1 Usage Example import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import PruningContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator async def main(): Step 1: Create a pruning filter prunefilter = PruningContentFilter( Lower \u2192 more content retained, higher \u2192 more content pruned threshold=0.45, \"fixed\" or \"dynamic\" thresholdtype=\"dynamic\", Ignore nodes with <5 words minwordthreshold=5 ) Step 2: Insert it into a Markdown Generator mdgenerator = DefaultMarkdownGenerator(contentfilter=prunefilter) Step 3: Pass it to CrawlerRunConfig config = CrawlerRunConfig( markdowngenerator=mdgenerator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com\", config=config ) if result.success: 'fitmarkdown' is your pruned content, focusing on \"denser\" text print(\"Raw Markdown length:\", len(result.markdownv2.rawmarkdown)) print(\"Fit Markdown length:\", len(result.markdownv2.fitmarkdown)) else: print(\"Error:\", result.errormessage) if name == \"main\": asyncio.run(main()) 2.2 Key Parameters minwordthreshold (int): If a block has fewer words than this, it\u2019s pruned. thresholdtype (str): \"fixed\" \u2192 each node must exceed threshold (0\u20131). \"dynamic\" \u2192 node scoring adjusts according to tag type, text/link density, etc. threshold (float, default 0.48): The base or \u201canchor\u201d cutoff. Algorithmic Factors : Text density \u2013 Encourages blocks that have a higher ratio of text to overall content. Link density \u2013 Penalizes sections that are mostly links. Tag importance \u2013 e.g., an <article> or <p> might be more important than a <div>. Structural context \u2013 If a node is deeply nested or in a suspected sidebar, it might be deprioritized. 3. BM25ContentFilter BM25 is a classical text ranking algorithm often used in search engines. If you have a user query or rely on page metadata to derive a query, BM25 can identify which text chunks best match that query. 3.1 Usage Example import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.contentfilterstrategy import BM25ContentFilter from crawl4ai.markdowngenerationstrategy import DefaultMarkdownGenerator async def main(): 1) A BM25 filter with a user query bm25filter = BM25ContentFilter( userquery=\"startup fundraising tips\", Adjust for stricter or looser results bm25threshold=1.2 ) 2) Insert into a Markdown Generator mdgenerator = DefaultMarkdownGenerator(contentfilter=bm25filter) 3) Pass to crawler config config = CrawlerRunConfig( markdowngenerator=mdgenerator ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com\", config=config ) if result.success: print(\"Fit Markdown (BM25 query-based):\") print(result.markdownv2.fitmarkdown) else: print(\"Error:\", result.errormessage) if name == \"main\": asyncio.run(main()) 3.2 Parameters userquery (str, optional): E.g. \"machine learning\". If blank, the filter tries to glean a query from page metadata. bm25threshold (float, default 1.0): Higher \u2192 fewer chunks but more relevant. Lower \u2192 more inclusive. > In more advanced scenarios, you might see parameters like usestemming, casesensitive, or prioritytags to refine how text is tokenized or weighted. 4. Accessing the \u201cFit\u201d Output After the crawl, your \u201cfit\u201d content is found in result.markdownv2.fitmarkdown. In future versions, it will be result.markdown.fitmarkdown. Meanwhile: fitmd = result.markdownv2.fitmarkdown fithtml = result.markdownv2.fithtml If the content filter is BM25 , you might see additional logic or references in fitmarkdown that highlight relevant segments. If it\u2019s Pruning , the text is typically well-cleaned but not necessarily matched to a query. 5. Code Patterns Recap 5.1 Pruning prunefilter = PruningContentFilter( threshold=0.5, thresholdtype=\"fixed\", minwordthreshold=10 ) mdgenerator = DefaultMarkdownGenerator(contentfilter=prunefilter) config = CrawlerRunConfig(markdowngenerator=mdgenerator) => result.markdownv2.fitmarkdown 5.2 BM25 bm25filter = BM25ContentFilter( userquery=\"health benefits fruit\", bm25threshold=1.2 ) mdgenerator = DefaultMarkdownGenerator(contentfilter=bm25filter) config = CrawlerRunConfig(markdowngenerator=mdgenerator) => result.markdownv2.fitmarkdown 6. Combining with \u201cwordcountthreshold\u201d & Exclusions Remember you can also specify: config = CrawlerRunConfig( wordcountthreshold=10, excludedtags=[\"nav\", \"footer\", \"header\"], excludeexternallinks=True, markdowngenerator=DefaultMarkdownGenerator( contentfilter=PruningContentFilter(threshold=0.5) ) ) Thus, multi-level filtering occurs: 1. The crawler\u2019s excludedtags are removed from the HTML first. 2. The content filter (Pruning, BM25, or custom) prunes or ranks the remaining text blocks. 3. The final \u201cfit\u201d content is generated in result.markdownv2.fitmarkdown. 7. Custom Filters If you need a different approach (like a specialized ML model or site-specific heuristics), you can create a new class inheriting from RelevantContentFilter and implement filtercontent(html). Then inject it into your markdown generator : from crawl4ai.contentfilterstrategy import RelevantContentFilter class MyCustomFilter(RelevantContentFilter): def filtercontent(self, html, minwordthreshold=None): parse HTML, implement custom logic return [block for block in ... if ... some condition...] Steps : 1. Subclass RelevantContentFilter. 2. Implement filtercontent(...). 3. Use it in your DefaultMarkdownGenerator(contentfilter=MyCustomFilter(...)). 8. Final Thoughts Fit Markdown is a crucial feature for: Summaries : Quickly get the important text from a cluttered page. Search : Combine with BM25 to produce content relevant to a query. AI Pipelines : Filter out boilerplate so LLM-based extraction or summarization runs on denser text. Key Points : - PruningContentFilter : Great if you just want the \u201cmeatiest\u201d text without a user query. - BM25ContentFilter : Perfect for query-based extraction or searching. - Combine with excludedtags, excludeexternallinks, wordcountthreshold to refine your final \u201cfit\u201d text. - Fit markdown ends up in result.markdownv2.fitmarkdown ; eventually result.markdown.fitmarkdown in future versions. With these tools, you can zero in on the text that truly matters, ignoring spammy or boilerplate content, and produce a concise, relevant \u201cfit markdown\u201d for your AI or data pipelines. Happy pruning and searching! Last Updated: 2025-01-01 Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/page-interaction/\n\nTitle: Page Interaction - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Page Interaction 1. JavaScript Execution 2. Wait Conditions 3. Handling Dynamic Content 4. Timing Control 5. Multi-Step Interaction Example 6. Combine Interaction with Extraction 7. Relevant CrawlerRunConfig Parameters 8. Conclusion Page Interaction Crawl4AI provides powerful features for interacting with dynamic webpages, handling JavaScript execution, waiting for conditions, and managing multi-step flows. By combining jscode , waitfor , and certain CrawlerRunConfig parameters, you can: 1. Click \u201cLoad More\u201d buttons 2. Fill forms and submit them 3. Wait for elements or data to appear 4. Reuse sessions across multiple steps Below is a quick overview of how to do it. 1. JavaScript Execution Basic Execution jscode in CrawlerRunConfig accepts either a single JS string or a list of JS snippets. Example : We\u2019ll scroll to the bottom of the page, then optionally click a \u201cLoad More\u201d button. import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): Single JS command config = CrawlerRunConfig( jscode=\"window.scrollTo(0, document.body.scrollHeight);\" ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com\", Example site config=config ) print(\"Crawled length:\", len(result.cleanedhtml)) Multiple commands jscommands = [ \"window.scrollTo(0, document.body.scrollHeight);\", 'More' link on Hacker News \"document.querySelector('a.morelink')?.click();\", ] config = CrawlerRunConfig(jscode=jscommands) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com\", Another pass config=config ) print(\"After scroll+click, length:\", len(result.cleanedhtml)) if name == \"main\": asyncio.run(main()) RelevantCrawlerRunConfig params: - jscode : A string or list of strings with JavaScript to run after the page loads. - jsonly : If set to True on subsequent calls, indicates we\u2019re continuing an existing session without a new full navigation. - sessionid : If you want to keep the same page across multiple calls, specify an ID. 2. Wait Conditions 2.1 CSS-Based Waiting Sometimes, you just want to wait for a specific element to appear. For example: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): config = CrawlerRunConfig( Wait for at least 30 items on Hacker News waitfor=\"css:.athing:nth-child(30)\" ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com\", config=config ) print(\"We have at least 30 items loaded!\") Rough check print(\"Total items in HTML:\", result.cleanedhtml.count(\"athing\")) if name == \"main\": asyncio.run(main()) Key param : - waitfor=\"css:...\" : Tells the crawler to wait until that CSS selector is present. 2.2 JavaScript-Based Waiting For more complex conditions (e.g., waiting for content length to exceed a threshold), prefix js:: waitcondition = \"\"\"() => { const items = document.querySelectorAll('.athing'); return items.length > 50; // Wait for at least 51 items }\"\"\" config = CrawlerRunConfig(waitfor=f\"js:{waitcondition}\") Behind the Scenes : Crawl4AI keeps polling the JS function until it returns true or a timeout occurs. 3. Handling Dynamic Content Many modern sites require multiple steps : scrolling, clicking \u201cLoad More,\u201d or updating via JavaScript. Below are typical patterns. 3.1 Load More Example (Hacker News \u201cMore\u201d Link) import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): Step 1: Load initial Hacker News page config = CrawlerRunConfig( waitfor=\"css:.athing:nth-child(30)\" Wait for 30 items ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com\", config=config ) print(\"Initial items loaded.\") Step 2: Let's scroll and click the \"More\" link loadmorejs = [ \"window.scrollTo(0, document.body.scrollHeight);\", The \"More\" link at page bottom \"document.querySelector('a.morelink')?.click();\" ] nextpageconf = CrawlerRunConfig( jscode=loadmorejs, waitfor=\"\"\"js:() => { return document.querySelectorAll('.athing').length > 30; }\"\"\", Mark that we do not re-navigate, but run JS in the same session: jsonly=True, sessionid=\"hnsession\" ) Re-use the same crawler session result2 = await crawler.arun( url=\"https://news.ycombinator.com\", same URL but continuing session config=nextpageconf ) totalitems = result2.cleanedhtml.count(\"athing\") print(\"Items after load-more:\", totalitems) if name == \"main\": asyncio.run(main()) Key params : - sessionid=\"hnsession\" : Keep the same page across multiple calls to arun(). - jsonly=True : We\u2019re not performing a full reload, just applying JS in the existing page. - waitfor with js:: Wait for item count to grow beyond 30. 3.2 Form Interaction If the site has a search or login form, you can fill fields and submit them with jscode. For instance, if GitHub had a local search form: jsforminteraction = \"\"\" document.querySelector('#your-search').value = 'TypeScript commits'; document.querySelector('form').submit(); \"\"\" config = CrawlerRunConfig( jscode=jsforminteraction, waitfor=\"css:.commit\" ) result = await crawler.arun(url=\"https://github.com/search\", config=config) In reality : Replace IDs or classes with the real site\u2019s form selectors. 4. Timing Control 1. pagetimeout (ms): Overall page load or script execution time limit. 2. delaybeforereturnhtml (seconds): Wait an extra moment before capturing the final HTML. 3. meandelay & maxrange : If you call arunmany() with multiple URLs, these add a random pause between each request. Example : config = CrawlerRunConfig( pagetimeout=60000, 60s limit delaybeforereturnhtml=2.5 ) 5. Multi-Step Interaction Example Below is a simplified script that does multiple \u201cLoad More\u201d clicks on GitHub\u2019s TypeScript commits page. It re-uses the same session to accumulate new commits each time. The code includes the relevant CrawlerRunConfig parameters you\u2019d rely on. import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def multipagecommits(): browsercfg = BrowserConfig( headless=False, Visible for demonstration verbose=True ) sessionid = \"githubtscommits\" basewait = \"\"\"js:() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); return commits.length > 0; }\"\"\" Step 1: Load initial commits config1 = CrawlerRunConfig( waitfor=basewait, sessionid=sessionid, cachemode=CacheMode.BYPASS, Not using jsonly yet since it's our first load ) async with AsyncWebCrawler(config=browsercfg) as crawler: result = await crawler.arun( url=\"https://github.com/microsoft/TypeScript/commits/main\", config=config1 ) print(\"Initial commits loaded. Count:\", result.cleanedhtml.count(\"commit\")) Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists jsnextpage = \"\"\" const selector = 'a[data-testid=\"pagination-next-button\"]'; const button = document.querySelector(selector); if (button) button.click(); \"\"\" Wait until new commits appear waitformore = \"\"\"js:() => { const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4'); if (!window.firstCommit && commits.length>0) { window.firstCommit = commits[0].textContent; return false; } // If top commit changes, we have new commits const topNow = commits[0]?.textContent.trim(); return topNow && topNow !== window.firstCommit; }\"\"\" for page in range(2): let's do 2 more \"Next\" pages confignext = CrawlerRunConfig( sessionid=sessionid, jscode=jsnextpage, waitfor=waitformore, jsonly=True, We're continuing from the open tab cachemode=CacheMode.BYPASS ) result2 = await crawler.arun( url=\"https://github.com/microsoft/TypeScript/commits/main\", config=confignext ) print(f\"Page {page+2} commits count:\", result2.cleanedhtml.count(\"commit\")) Optionally kill session await crawler.crawlerstrategy.killsession(sessionid) async def main(): await multipagecommits() if name == \"main\": asyncio.run(main()) Key Points : sessionid : Keep the same page open. jscode + waitfor + jsonly=True : We do partial refreshes, waiting for new commits to appear. cachemode=CacheMode.BYPASS ensures we always see fresh data each step. 6. Combine Interaction with Extraction Once dynamic content is loaded, you can attach an extractionstrategy (like JsonCssExtractionStrategy or LLMExtractionStrategy). For example: from crawl4ai.extractionstrategy import JsonCssExtractionStrategy schema = { \"name\": \"Commits\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"} ] } config = CrawlerRunConfig( sessionid=\"tscommitssession\", jscode=jsnextpage, waitfor=waitformore, extractionstrategy=JsonCssExtractionStrategy(schema) ) When done, check result.extractedcontent for the JSON. 7. Relevant CrawlerRunConfig Parameters Below are the key interaction-related parameters in CrawlerRunConfig. For a full list, see Configuration Parameters. jscode : JavaScript to run after initial load. jsonly : If True, no new page navigation\u2014only JS in the existing session. waitfor : CSS (\"css:...\") or JS (\"js:...\") expression to wait for. sessionid : Reuse the same page across calls. cachemode : Whether to read/write from the cache or bypass. removeoverlayelements : Remove certain popups automatically. simulateuser, overridenavigator, magic: Anti-bot or \u201chuman-like\u201d interactions. 8. Conclusion Crawl4AI\u2019s page interaction features let you: 1. Execute JavaScript for scrolling, clicks, or form filling. 2. Wait for CSS or custom JS conditions before capturing data. 3. Handle multi-step flows (like \u201cLoad More\u201d) with partial reloads or persistent sessions. 4. Combine with structured extraction for dynamic sites. With these tools, you can scrape modern, interactive webpages confidently. For advanced hooking, user simulation, or in-depth config, check the API reference or related advanced docs. Happy scripting! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/content-selection/\n\nTitle: Content Selection - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Content Selection 1. CSS-Based Selection 2. Content Filtering & Exclusions 3. Handling Iframes 4. Structured Extraction Examples 5. Comprehensive Example 6. Scraping Modes 7. Conclusion Content Selection Crawl4AI provides multiple ways to select , filter , and refine the content from your crawls. Whether you need to target a specific CSS region, exclude entire tags, filter out external links, or remove certain domains and images, CrawlerRunConfig offers a wide range of parameters. Below, we show how to configure these parameters and combine them for precise control. 1. CSS-Based Selection A straightforward way to limit your crawl results to a certain region of the page is cssselector in CrawlerRunConfig : import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): config = CrawlerRunConfig( e.g., first 30 items from Hacker News cssselector=\".athing:nth-child(-n+30)\" ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com/newest\", config=config ) print(\"Partial HTML length:\", len(result.cleanedhtml)) if name == \"main\": asyncio.run(main()) Result : Only elements matching that selector remain in result.cleanedhtml. 2. Content Filtering & Exclusions 2.1 Basic Overview config = CrawlerRunConfig( Content thresholds wordcountthreshold=10, Minimum words per block Tag exclusions excludedtags=['form', 'header', 'footer', 'nav'], Link filtering excludeexternallinks=True, excludesocialmedialinks=True, Block entire domains excludedomains=[\"adtrackers.com\", \"spammynews.org\"], excludesocialmediadomains=[\"facebook.com\", \"twitter.com\"], Media filtering excludeexternalimages=True ) Explanation : wordcountthreshold : Ignores text blocks under X words. Helps skip trivial blocks like short nav or disclaimers. excludedtags : Removes entire tags (<form>, <header>, <footer>, etc.). Link Filtering : excludeexternallinks: Strips out external links and may remove them from result.links. excludesocialmedialinks: Removes links pointing to known social media domains. excludedomains: A custom list of domains to block if discovered in links. excludesocialmediadomains: A curated list (override or add to it) for social media sites. Media Filtering : excludeexternalimages: Discards images not hosted on the same domain as the main page (or its subdomains). By default in case you set excludesocialmedialinks=True, the following social media domains are excluded: [ 'facebook.com', 'twitter.com', 'x.com', 'linkedin.com', 'instagram.com', 'pinterest.com', 'tiktok.com', 'snapchat.com', 'reddit.com', ] 2.2 Example Usage import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): config = CrawlerRunConfig( cssselector=\"main.content\", wordcountthreshold=10, excludedtags=[\"nav\", \"footer\"], excludeexternallinks=True, excludesocialmedialinks=True, excludedomains=[\"ads.com\", \"spammytrackers.net\"], excludeexternalimages=True, cachemode=CacheMode.BYPASS ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config) print(\"Cleaned HTML length:\", len(result.cleanedhtml)) if name == \"main\": asyncio.run(main()) Note : If these parameters remove too much, reduce or disable them accordingly. 3. Handling Iframes Some sites embed content in <iframe> tags. If you want that inline: config = CrawlerRunConfig( Merge iframe content into the final output processiframes=True, removeoverlayelements=True ) Usage : import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): config = CrawlerRunConfig( processiframes=True, removeoverlayelements=True ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.org/iframe-demo\", config=config ) print(\"Iframe-merged length:\", len(result.cleanedhtml)) if name == \"main\": asyncio.run(main()) 4. Structured Extraction Examples You can combine content selection with a more advanced extraction strategy. For instance, a CSS-based or LLM-based extraction strategy can run on the filtered HTML. 4.1 Pattern-Based with JsonCssExtractionStrategy import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): Minimal schema for repeated items schema = { \"name\": \"News Items\", \"baseSelector\": \"tr.athing\", \"fields\": [ {\"name\": \"title\", \"selector\": \"a.storylink\", \"type\": \"text\"}, { \"name\": \"link\", \"selector\": \"a.storylink\", \"type\": \"attribute\", \"attribute\": \"href\" } ] } config = CrawlerRunConfig( Content filtering excludedtags=[\"form\", \"header\"], excludedomains=[\"adsite.com\"], CSS selection or entire page cssselector=\"table.itemlist\", No caching for demonstration cachemode=CacheMode.BYPASS, Extraction strategy extractionstrategy=JsonCssExtractionStrategy(schema) ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://news.ycombinator.com/newest\", config=config ) data = json.loads(result.extractedcontent) print(\"Sample extracted item:\", data[:1]) Show first item if name == \"main\": asyncio.run(main()) 4.2 LLM-Based Extraction import asyncio import json from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import LLMExtractionStrategy class ArticleData(BaseModel): headline: str summary: str async def main(): llmstrategy = LLMExtractionStrategy( provider=\"openai/gpt-4\", apitoken=\"sk-YOURAPIKEY\", schema=ArticleData.schema(), extractiontype=\"schema\", instruction=\"Extract 'headline' and a short 'summary' from the content.\" ) config = CrawlerRunConfig( excludeexternallinks=True, wordcountthreshold=20, extractionstrategy=llmstrategy ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config) article = json.loads(result.extractedcontent) print(article) if name == \"main\": asyncio.run(main()) Here, the crawler: Filters out external links (excludeexternallinks=True). Ignores very short text blocks (wordcountthreshold=20). Passes the final HTML to your LLM strategy for an AI-driven parse. 5. Comprehensive Example Below is a short function that unifies CSS selection , exclusion logic, and a pattern-based extraction, demonstrating how you can fine-tune your final data: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractmainarticles(url: str): schema = { \"name\": \"ArticleBlock\", \"baseSelector\": \"div.article-block\", \"fields\": [ {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"}, { \"name\": \"metadata\", \"type\": \"nested\", \"fields\": [ {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"}, {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"} ] } ] } config = CrawlerRunConfig( Keep only #main-content cssselector=\"#main-content\", Filtering wordcountthreshold=10, excludedtags=[\"nav\", \"footer\"], excludeexternallinks=True, excludedomains=[\"somebadsite.com\"], excludeexternalimages=True, Extraction extractionstrategy=JsonCssExtractionStrategy(schema), cachemode=CacheMode.BYPASS ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=url, config=config) if not result.success: print(f\"Error: {result.errormessage}\") return None return json.loads(result.extractedcontent) async def main(): articles = await extractmainarticles(\"https://news.ycombinator.com/newest\") if articles: print(\"Extracted Articles:\", articles[:2]) Show first 2 if name == \"main\": asyncio.run(main()) Why This Works : - CSS scoping with #main-content. - Multiple exclude parameters to remove domains, external images, etc. - A JsonCssExtractionStrategy to parse repeated article blocks. 6. Scraping Modes Crawl4AI provides two different scraping strategies for HTML content processing: WebScrapingStrategy (BeautifulSoup-based, default) and LXMLWebScrapingStrategy (LXML-based). The LXML strategy offers significantly better performance, especially for large HTML documents. from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy async def main(): config = CrawlerRunConfig( scrapingstrategy=LXMLWebScrapingStrategy() Faster alternative to default BeautifulSoup ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com\", config=config ) You can also create your own custom scraping strategy by inheriting from ContentScrapingStrategy. The strategy must return a ScrapingResult object with the following structure: from crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links class CustomScrapingStrategy(ContentScrapingStrategy): def scrap(self, url: str, html: str, kwargs) -> ScrapingResult: Implement your custom scraping logic here return ScrapingResult( cleanedhtml=\"<html>...</html>\", Cleaned HTML content success=True, Whether scraping was successful media=Media( images=[ List of images found MediaItem( src=\"https://example.com/image.jpg\", alt=\"Image description\", desc=\"Surrounding text\", score=1, type=\"image\", groupid=1, format=\"jpg\", width=800 ) ], videos=[], List of videos (same structure as images) audios=[] List of audio files (same structure as images) ), links=Links( internal=[ List of internal links Link( href=\"https://example.com/page\", text=\"Link text\", title=\"Link title\", basedomain=\"example.com\" ) ], external=[] List of external links (same structure) ), metadata={ Additional metadata \"title\": \"Page Title\", \"description\": \"Page description\" } ) async def ascrap(self, url: str, html: str, kwargs) -> ScrapingResult: For simple cases, you can use the sync version return await asyncio.tothread(self.scrap, url, html, kwargs) Performance Considerations The LXML strategy can be up to 10-20x faster than BeautifulSoup strategy, particularly when processing large HTML documents. However, please note: 1. LXML strategy is currently experimental 2. In some edge cases, the parsing results might differ slightly from BeautifulSoup 3. If you encounter any inconsistencies between LXML and BeautifulSoup results, please raise an issue with a reproducible example Choose LXML strategy when: - Processing large HTML documents (recommended for >100KB) - Performance is critical - Working with well-formed HTML Stick to BeautifulSoup strategy (default) when: - Maximum compatibility is needed - Working with malformed HTML - Exact parsing behavior is critical 7. Conclusion By mixing cssselector scoping, content filtering parameters, and advanced extraction strategies , you can precisely choose which data to keep. Key parameters in CrawlerRunConfig for content selection include: 1. cssselector \u2013 Basic scoping to an element or region. 2. wordcountthreshold \u2013 Skip short blocks. 3. excludedtags \u2013 Remove entire HTML tags. 4. excludeexternallinks , excludesocialmedialinks , excludedomains \u2013 Filter out unwanted links or domains. 5. excludeexternalimages \u2013 Remove images from external sources. 6. processiframes \u2013 Merge iframe content if needed. Combine these with structured extraction (CSS, LLM-based, or others) to build powerful crawls that yield exactly the content you want, from raw or cleaned HTML up to sophisticated JSON structures. For more detail, see Configuration Reference. Enjoy curating your data to the max! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/cache-modes/\n\nTitle: Cache Modes - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Crawl4AI Cache System and Migration Guide Overview Old vs New Approach Migration Example Common Migration Patterns Crawl4AI Cache System and Migration Guide Overview Starting from version 0.5.0, Crawl4AI introduces a new caching system that replaces the old boolean flags with a more intuitive CacheMode enum. This change simplifies cache control and makes the behavior more predictable. Old vs New Approach Old Way (Deprecated) The old system used multiple boolean flags: - bypasscache: Skip cache entirely - disablecache: Disable all caching - nocacheread: Don't read from cache - nocachewrite: Don't write to cache New Way (Recommended) The new system uses a single CacheMode enum: - CacheMode.ENABLED: Normal caching (read/write) - CacheMode.DISABLED: No caching at all - CacheMode.READONLY: Only read from cache - CacheMode.WRITEONLY: Only write to cache - CacheMode.BYPASS: Skip cache for this operation Migration Example Old Code (Deprecated) import asyncio from crawl4ai import AsyncWebCrawler async def useproxy(): async with AsyncWebCrawler(verbose=True) as crawler: result = await crawler.arun( url=\"https://www.nbcnews.com/business\", bypasscache=True Old way ) print(len(result.markdown)) async def main(): await useproxy() if name == \"main\": asyncio.run(main()) New Code (Recommended) import asyncio from crawl4ai import AsyncWebCrawler, CacheMode from crawl4ai.asyncconfigs import CrawlerRunConfig async def useproxy(): Use CacheMode in CrawlerRunConfig config = CrawlerRunConfig(cachemode=CacheMode.BYPASS) async with AsyncWebCrawler(verbose=True) as crawler: result = await crawler.arun( url=\"https://www.nbcnews.com/business\", config=config Pass the configuration object ) print(len(result.markdown)) async def main(): await useproxy() if name == \"main\": asyncio.run(main()) Common Migration Patterns Old Flag | New Mode ---|--- bypasscache=True | cachemode=CacheMode.BYPASS disablecache=True | cachemode=CacheMode.DISABLED nocacheread=True | cachemode=CacheMode.WRITEONLY nocachewrite=True | cachemode=CacheMode.READONLY Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/local-files/\n\nTitle: Local Files & Raw HTML - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Prefix-Based Input Handling in Crawl4AI Crawling a Web URL Crawling a Local HTML File Crawling Raw HTML Content Complete Example Conclusion Prefix-Based Input Handling in Crawl4AI This guide will walk you through using the Crawl4AI library to crawl web pages, local HTML files, and raw HTML strings. We'll demonstrate these capabilities using a Wikipedia page as an example. Crawling a Web URL To crawl a live web page, provide the URL starting with http:// or https://, using a CrawlerRunConfig object: import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai.asyncconfigs import CrawlerRunConfig async def crawlweb(): config = CrawlerRunConfig(bypasscache=True) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://en.wikipedia.org/wiki/apple\", config=config ) if result.success: print(\"Markdown Content:\") print(result.markdown) else: print(f\"Failed to crawl: {result.errormessage}\") asyncio.run(crawlweb()) Crawling a Local HTML File To crawl a local HTML file, prefix the file path with file://. import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai.asyncconfigs import CrawlerRunConfig async def crawllocalfile(): localfilepath = \"/path/to/apple.html\" Replace with your file path fileurl = f\"file://{localfilepath}\" config = CrawlerRunConfig(bypasscache=True) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=fileurl, config=config) if result.success: print(\"Markdown Content from Local File:\") print(result.markdown) else: print(f\"Failed to crawl local file: {result.errormessage}\") asyncio.run(crawllocalfile()) Crawling Raw HTML Content To crawl raw HTML content, prefix the HTML string with raw:. import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai.asyncconfigs import CrawlerRunConfig async def crawlrawhtml(): rawhtml = \"<html><body><h1>Hello, World!</h1></body></html>\" rawhtmlurl = f\"raw:{rawhtml}\" config = CrawlerRunConfig(bypasscache=True) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=rawhtmlurl, config=config) if result.success: print(\"Markdown Content from Raw HTML:\") print(result.markdown) else: print(f\"Failed to crawl raw HTML: {result.errormessage}\") asyncio.run(crawlrawhtml()) Complete Example Below is a comprehensive script that: 1. Crawls the Wikipedia page for \"Apple.\" 2. Saves the HTML content to a local file (apple.html). 3. Crawls the local HTML file and verifies the markdown length matches the original crawl. 4. Crawls the raw HTML content from the saved file and verifies consistency. import os import sys import asyncio from pathlib import Path from crawl4ai import AsyncWebCrawler from crawl4ai.asyncconfigs import CrawlerRunConfig async def main(): wikipediaurl = \"https://en.wikipedia.org/wiki/apple\" scriptdir = Path(file).parent htmlfilepath = scriptdir / \"apple.html\" async with AsyncWebCrawler() as crawler: Step 1: Crawl the Web URL print(\"\\n=== Step 1: Crawling the Wikipedia URL ===\") webconfig = CrawlerRunConfig(bypasscache=True) result = await crawler.arun(url=wikipediaurl, config=webconfig) if not result.success: print(f\"Failed to crawl {wikipediaurl}: {result.errormessage}\") return with open(htmlfilepath, 'w', encoding='utf-8') as f: f.write(result.html) webcrawllength = len(result.markdown) print(f\"Length of markdown from web crawl: {webcrawllength}\\n\") Step 2: Crawl from the Local HTML File print(\"=== Step 2: Crawling from the Local HTML File ===\") fileurl = f\"file://{htmlfilepath.resolve()}\" fileconfig = CrawlerRunConfig(bypasscache=True) localresult = await crawler.arun(url=fileurl, config=fileconfig) if not localresult.success: print(f\"Failed to crawl local file {fileurl}: {localresult.errormessage}\") return localcrawllength = len(localresult.markdown) assert webcrawllength == localcrawllength, \"Markdown length mismatch\" print(\"\u2705 Markdown length matches between web and local file crawl.\\n\") Step 3: Crawl Using Raw HTML Content print(\"=== Step 3: Crawling Using Raw HTML Content ===\") with open(htmlfilepath, 'r', encoding='utf-8') as f: rawhtmlcontent = f.read() rawhtmlurl = f\"raw:{rawhtmlcontent}\" rawconfig = CrawlerRunConfig(bypasscache=True) rawresult = await crawler.arun(url=rawhtmlurl, config=rawconfig) if not rawresult.success: print(f\"Failed to crawl raw HTML content: {rawresult.errormessage}\") return rawcrawllength = len(rawresult.markdown) assert webcrawllength == rawcrawllength, \"Markdown length mismatch\" print(\"\u2705 Markdown length matches between web and raw HTML crawl.\\n\") print(\"All tests passed successfully!\") if htmlfilepath.exists(): os.remove(htmlfilepath) if name == \"main\": asyncio.run(main()) Conclusion With the unified url parameter and prefix-based handling in Crawl4AI , you can seamlessly handle web URLs, local HTML files, and raw HTML content. Use CrawlerRunConfig for flexible and consistent configuration in all scenarios. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/core/link-media/\n\nTitle: Link & Media - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Link & Media 1. Link Extraction 2. Domain Filtering 3. Media Extraction 4. Putting It All Together: Link & Media Filtering 5. Common Pitfalls & Tips Link & Media In this tutorial, you\u2019ll learn how to: 1. Extract links (internal, external) from crawled pages 2. Filter or exclude specific domains (e.g., social media or custom domains) 3. Access and manage media data (especially images) in the crawl result 4. Configure your crawler to exclude or prioritize certain images > Prerequisites - You have completed or are familiar with the AsyncWebCrawler Basics tutorial. - You can run Crawl4AI in your environment (Playwright, Python, etc.). Below is a revised version of the Link Extraction and Media Extraction sections that includes example data structures showing how links and media items are stored in CrawlResult. Feel free to adjust any field names or descriptions to match your actual output. 1. Link Extraction 1.1 result.links When you call arun() or arunmany() on a URL, Crawl4AI automatically extracts links and stores them in the links field of CrawlResult. By default, the crawler tries to distinguish internal links (same domain) from external links (different domains). Basic Example : from crawl4ai import AsyncWebCrawler async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://www.example.com\") if result.success: internallinks = result.links.get(\"internal\", []) externallinks = result.links.get(\"external\", []) print(f\"Found {len(internallinks)} internal links.\") print(f\"Found {len(internallinks)} external links.\") print(f\"Found {len(result.media)} media items.\") Each link is typically a dictionary with fields like: { \"href\": \"...\", \"text\": \"...\", \"title\": \"...\", \"basedomain\": \"...\" } if internallinks: print(\"Sample Internal Link:\", internallinks[0]) else: print(\"Crawl failed:\", result.errormessage) Structure Example : result.links = { \"internal\": [ { \"href\": \"https://kidocode.com/\", \"text\": \"\", \"title\": \"\", \"basedomain\": \"kidocode.com\" }, { \"href\": \"https://kidocode.com/degrees/technology\", \"text\": \"Technology Degree\", \"title\": \"KidoCode Tech Program\", \"basedomain\": \"kidocode.com\" }, ... ], \"external\": [ possibly other links leading to third-party sites ] } href : The raw hyperlink URL. text : The link text (if any) within the <a> tag. title : The title attribute of the link (if present). basedomain : The domain extracted from href. Helpful for filtering or grouping by domain. 2. Domain Filtering Some websites contain hundreds of third-party or affiliate links. You can filter out certain domains at crawl time by configuring the crawler. The most relevant parameters in CrawlerRunConfig are: excludeexternallinks : If True, discard any link pointing outside the root domain. excludesocialmediadomains : Provide a list of social media platforms (e.g., [\"facebook.com\", \"twitter.com\"]) to exclude from your crawl. excludesocialmedialinks : If True, automatically skip known social platforms. excludedomains : Provide a list of custom domains you want to exclude (e.g., [\"spammyads.com\", \"tracker.net\"]). 2.1 Example: Excluding External & Social Media Links import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): crawlercfg = CrawlerRunConfig( excludeexternallinks=True, No links outside primary domain excludesocialmedialinks=True Skip recognized social media domains ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( \"https://www.example.com\", config=crawlercfg ) if result.success: print(\"[OK] Crawled:\", result.url) print(\"Internal links count:\", len(result.links.get(\"internal\", []))) print(\"External links count:\", len(result.links.get(\"external\", []))) Likely zero external links in this scenario else: print(\"[ERROR]\", result.errormessage) if name == \"main\": asyncio.run(main()) 2.2 Example: Excluding Specific Domains If you want to let external links in, but specifically exclude a domain (e.g., suspiciousads.com), do this: crawlercfg = CrawlerRunConfig( excludedomains=[\"suspiciousads.com\"] ) This approach is handy when you still want external links but need to block certain sites you consider spammy. 3. Media Extraction 3.1 Accessing result.media By default, Crawl4AI collects images, audio, and video URLs it finds on the page. These are stored in result.media, a dictionary keyed by media type (e.g., images, videos, audio). Basic Example : if result.success: imagesinfo = result.media.get(\"images\", []) print(f\"Found {len(imagesinfo)} images in total.\") for i, img in enumerate(imagesinfo[:5]): Inspect just the first 5 print(f\"[Image {i}] URL: {img['src']}\") print(f\" Alt text: {img.get('alt', '')}\") print(f\" Score: {img.get('score')}\") print(f\" Description: {img.get('desc', '')}\\n\") Structure Example : result.media = { \"images\": [ { \"src\": \"https://cdn.prod.website-files.com/.../Group%2089.svg\", \"alt\": \"coding school for kids\", \"desc\": \"Trial Class Degrees degrees All Degrees AI Degree Technology ...\", \"score\": 3, \"type\": \"image\", \"groupid\": 0, \"format\": None, \"width\": None, \"height\": None }, ... ], \"videos\": [ Similar structure but with video-specific fields ], \"audio\": [ Similar structure but with audio-specific fields ] } Depending on your Crawl4AI version or scraping strategy, these dictionaries can include fields like: src : The media URL (e.g., image source) alt : The alt text for images (if present) desc : A snippet of nearby text or a short description (optional) score : A heuristic relevance score if you\u2019re using content-scoring features width , height : If the crawler detects dimensions for the image/video type : Usually \"image\", \"video\", or \"audio\" groupid : If you\u2019re grouping related media items, the crawler might assign an ID With these details, you can easily filter out or focus on certain images (for instance, ignoring images with very low scores or a different domain), or gather metadata for analytics. 3.2 Excluding External Images If you\u2019re dealing with heavy pages or want to skip third-party images (advertisements, for example), you can turn on: crawlercfg = CrawlerRunConfig( excludeexternalimages=True ) This setting attempts to discard images from outside the primary domain, keeping only those from the site you\u2019re crawling. 3.3 Additional Media Config screenshot : Set to True if you want a full-page screenshot stored as base64 in result.screenshot. pdf : Set to True if you want a PDF version of the page in result.pdf. waitforimages : If True, attempts to wait until images are fully loaded before final extraction. 4. Putting It All Together: Link & Media Filtering Here\u2019s a combined example demonstrating how to filter out external links, skip certain domains, and exclude external images: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): Suppose we want to keep only internal links, remove certain domains, and discard external images from the final crawl data. crawlercfg = CrawlerRunConfig( excludeexternallinks=True, excludedomains=[\"spammyads.com\"], excludesocialmedialinks=True, skip Twitter, Facebook, etc. excludeexternalimages=True, keep only images from main domain waitforimages=True, ensure images are loaded verbose=True ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://www.example.com\", config=crawlercfg) if result.success: print(\"[OK] Crawled:\", result.url) 1. Links inlinks = result.links.get(\"internal\", []) extlinks = result.links.get(\"external\", []) print(\"Internal link count:\", len(inlinks)) print(\"External link count:\", len(extlinks)) should be zero with excludeexternallinks=True 2. Images images = result.media.get(\"images\", []) print(\"Images found:\", len(images)) Let's see a snippet of these images for i, img in enumerate(images[:3]): print(f\" - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})\") else: print(\"[ERROR] Failed to crawl. Reason:\", result.errormessage) if name == \"main\": asyncio.run(main()) 5. Common Pitfalls & Tips 1. Conflicting Flags : - excludeexternallinks=True but then also specifying excludesocialmedialinks=True is typically fine, but understand that the first setting already discards all external links. The second becomes somewhat redundant. - excludeexternalimages=True but want to keep some external images? Currently no partial domain-based setting for images, so you might need a custom approach or hook logic. 2. Relevancy Scores : - If your version of Crawl4AI or your scraping strategy includes an img[\"score\"], it\u2019s typically a heuristic based on size, position, or content analysis. Evaluate carefully if you rely on it. 3. Performance : - Excluding certain domains or external images can speed up your crawl, especially for large, media-heavy pages. - If you want a \u201cfull\u201d link map, do not exclude them. Instead, you can post-filter in your own code. 4. Social Media Lists : - excludesocialmedialinks=True typically references an internal list of known social domains like Facebook, Twitter, LinkedIn, etc. If you need to add or remove from that list, look for library settings or a local config file (depending on your version). That\u2019s it for Link & Media Analysis! You\u2019re now equipped to filter out unwanted sites and zero in on the images and videos that matter for your project. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/advanced-features/\n\nTitle: Overview - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Overview of Some Important Advanced Features 1. Proxy Usage 2. Capturing PDFs & Screenshots 3. Handling SSL Certificates 4. Custom Headers 5. Session Persistence & Local Storage 6. Robots.txt Compliance Putting It All Together Conclusion & Next Steps Overview of Some Important Advanced Features (Proxy, PDF, Screenshot, SSL, Headers, & Storage State) Crawl4AI offers multiple power-user features that go beyond simple crawling. This tutorial covers: 1. Proxy Usage 2. Capturing PDFs & Screenshots 3. Handling SSL Certificates 4. Custom Headers 5. Session Persistence & Local Storage 6. Robots.txt Compliance > Prerequisites - You have a basic grasp of AsyncWebCrawler Basics - You know how to run or configure your Python environment with Playwright installed 1. Proxy Usage If you need to route your crawl traffic through a proxy\u2014whether for IP rotation, geo-testing, or privacy\u2014Crawl4AI supports it via BrowserConfig.proxyconfig. import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): browsercfg = BrowserConfig( proxyconfig={ \"server\": \"http://proxy.example.com:8080\", \"username\": \"myuser\", \"password\": \"mypass\", }, headless=True ) crawlercfg = CrawlerRunConfig( verbose=True ) async with AsyncWebCrawler(config=browsercfg) as crawler: result = await crawler.arun( url=\"https://www.whatismyip.com/\", config=crawlercfg ) if result.success: print(\"[OK] Page fetched via proxy.\") print(\"Page HTML snippet:\", result.html[:200]) else: print(\"[ERROR]\", result.errormessage) if name == \"main\": asyncio.run(main()) Key Points - proxyconfig expects a dict with server and optional auth credentials. - Many commercial proxies provide an HTTP/HTTPS \u201cgateway\u201d server that you specify in server. - If your proxy doesn\u2019t need auth, omit username/password. 2. Capturing PDFs & Screenshots Sometimes you need a visual record of a page or a PDF \u201cprintout.\u201d Crawl4AI can do both in one pass: import os, asyncio from base64 import b64decode from crawl4ai import AsyncWebCrawler, CacheMode async def main(): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://en.wikipedia.org/wiki/Listofcommonmisconceptions\", cachemode=CacheMode.BYPASS, pdf=True, screenshot=True ) if result.success: Save screenshot if result.screenshot: with open(\"wikipediascreenshot.png\", \"wb\") as f: f.write(b64decode(result.screenshot)) Save PDF if result.pdf: with open(\"wikipediapage.pdf\", \"wb\") as f: f.write(result.pdf) print(\"[OK] PDF & screenshot captured.\") else: print(\"[ERROR]\", result.errormessage) if name == \"main\": asyncio.run(main()) Why PDF + Screenshot? - Large or complex pages can be slow or error-prone with \u201ctraditional\u201d full-page screenshots. - Exporting a PDF is more reliable for very long pages. Crawl4AI automatically converts the first PDF page into an image if you request both. Relevant Parameters - pdf=True : Exports the current page as a PDF (base64-encoded in result.pdf). - screenshot=True : Creates a screenshot (base64-encoded in result.screenshot). - scanfullpage or advanced hooking can further refine how the crawler captures content. 3. Handling SSL Certificates If you need to verify or export a site\u2019s SSL certificate\u2014for compliance, debugging, or data analysis\u2014Crawl4AI can fetch it during the crawl: import asyncio, os from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): tmpdir = os.path.join(os.getcwd(), \"tmp\") os.makedirs(tmpdir, existok=True) config = CrawlerRunConfig( fetchsslcertificate=True, cachemode=CacheMode.BYPASS ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=\"https://example.com\", config=config) if result.success and result.sslcertificate: cert = result.sslcertificate print(\"\\nCertificate Information:\") print(f\"Issuer (CN): {cert.issuer.get('CN', '')}\") print(f\"Valid until: {cert.validuntil}\") print(f\"Fingerprint: {cert.fingerprint}\") Export in multiple formats: cert.tojson(os.path.join(tmpdir, \"certificate.json\")) cert.topem(os.path.join(tmpdir, \"certificate.pem\")) cert.toder(os.path.join(tmpdir, \"certificate.der\")) print(\"\\nCertificate exported to JSON/PEM/DER in 'tmp' folder.\") else: print(\"[ERROR] No certificate or crawl failed.\") if name == \"main\": asyncio.run(main()) Key Points - fetchsslcertificate=True triggers certificate retrieval. - result.sslcertificate includes methods (tojson, topem, toder) for saving in various formats (handy for server config, Java keystores, etc.). 4. Custom Headers Sometimes you need to set custom headers (e.g., language preferences, authentication tokens, or specialized user-agent strings). You can do this in multiple ways: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Option 1: Set headers at the crawler strategy level crawler1 = AsyncWebCrawler( The underlying strategy can accept headers in its constructor crawlerstrategy=None We'll override below for clarity ) crawler1.crawlerstrategy.updateuseragent(\"MyCustomUA/1.0\") crawler1.crawlerstrategy.setcustomheaders({ \"Accept-Language\": \"fr-FR,fr;q=0.9\" }) result1 = await crawler1.arun(\"https://www.example.com\") print(\"Example 1 result success:\", result1.success) Option 2: Pass headers directly to arun() crawler2 = AsyncWebCrawler() result2 = await crawler2.arun( url=\"https://www.example.com\", headers={\"Accept-Language\": \"es-ES,es;q=0.9\"} ) print(\"Example 2 result success:\", result2.success) if name == \"main\": asyncio.run(main()) Notes - Some sites may react differently to certain headers (e.g., Accept-Language). - If you need advanced user-agent randomization or client hints, see Identity-Based Crawling (Anti-Bot) or use UserAgentGenerator. 5. Session Persistence & Local Storage Crawl4AI can preserve cookies and localStorage so you can continue where you left off\u2014ideal for logging into sites or skipping repeated auth flows. 5.1 storagestate import asyncio from crawl4ai import AsyncWebCrawler async def main(): storagedict = { \"cookies\": [ { \"name\": \"session\", \"value\": \"abcd1234\", \"domain\": \"example.com\", \"path\": \"/\", \"expires\": 1699999999.0, \"httpOnly\": False, \"secure\": False, \"sameSite\": \"None\" } ], \"origins\": [ { \"origin\": \"https://example.com\", \"localStorage\": [ {\"name\": \"token\", \"value\": \"myauthtoken\"} ] } ] } Provide the storage state as a dictionary to start \"already logged in\" async with AsyncWebCrawler( headless=True, storagestate=storagedict ) as crawler: result = await crawler.arun(\"https://example.com/protected\") if result.success: print(\"Protected page content length:\", len(result.html)) else: print(\"Failed to crawl protected page\") if name == \"main\": asyncio.run(main()) 5.2 Exporting & Reusing State You can sign in once, export the browser context, and reuse it later\u2014without re-entering credentials. await context.storagestate(path=\"mystorage.json\") : Exports cookies, localStorage, etc. to a file. Provide storagestate=\"mystorage.json\" on subsequent runs to skip the login step. See : Detailed session management tutorial or Explanations \u2192 Browser Context & Managed Browser for more advanced scenarios (like multi-step logins, or capturing after interactive pages). 6. Robots.txt Compliance Crawl4AI supports respecting robots.txt rules with efficient caching: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): Enable robots.txt checking in config config = CrawlerRunConfig( checkrobotstxt=True Will check and respect robots.txt rules ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( \"https://example.com\", config=config ) if not result.success and result.statuscode == 403: print(\"Access denied by robots.txt\") if name == \"main\": asyncio.run(main()) Key Points - Robots.txt files are cached locally for efficiency - Cache is stored in /.crawl4ai/robots/robotscache.db - Cache has a default TTL of 7 days - If robots.txt can't be fetched, crawling is allowed - Returns 403 status code if URL is disallowed Putting It All Together Here\u2019s a snippet that combines multiple \u201cadvanced\u201d features (proxy, PDF, screenshot, SSL, custom headers, and session reuse) into one run. Normally, you\u2019d tailor each setting to your project\u2019s needs. import os, asyncio from base64 import b64decode from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main(): 1. Browser config with proxy + headless browsercfg = BrowserConfig( proxyconfig={ \"server\": \"http://proxy.example.com:8080\", \"username\": \"myuser\", \"password\": \"mypass\", }, headless=True, ) 2. Crawler config with PDF, screenshot, SSL, custom headers, and ignoring caches crawlercfg = CrawlerRunConfig( pdf=True, screenshot=True, fetchsslcertificate=True, cachemode=CacheMode.BYPASS, headers={\"Accept-Language\": \"en-US,en;q=0.8\"}, storagestate=\"mystorage.json\", Reuse session from a previous sign-in verbose=True, ) 3. Crawl async with AsyncWebCrawler(config=browsercfg) as crawler: result = await crawler.arun( url = \"https://secure.example.com/protected\", config=crawlercfg ) if result.success: print(\"[OK] Crawled the secure page. Links found:\", len(result.links.get(\"internal\", []))) Save PDF & screenshot if result.pdf: with open(\"result.pdf\", \"wb\") as f: f.write(b64decode(result.pdf)) if result.screenshot: with open(\"result.png\", \"wb\") as f: f.write(b64decode(result.screenshot)) Check SSL cert if result.sslcertificate: print(\"SSL Issuer CN:\", result.sslcertificate.issuer.get(\"CN\", \"\")) else: print(\"[ERROR]\", result.errormessage) if name == \"main\": asyncio.run(main()) Conclusion & Next Steps You\u2019ve now explored several advanced features: Proxy Usage PDF & Screenshot capturing for large or critical pages SSL Certificate retrieval & exporting Custom Headers for language or specialized requests Session Persistence via storage state Robots.txt Compliance With these power tools, you can build robust scraping workflows that mimic real user behavior, handle secure sites, capture detailed snapshots, and manage sessions across multiple runs\u2014streamlining your entire data collection pipeline. Last Updated : 2025-01-01 Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/file-downloading/\n\nTitle: File Downloading - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Download Handling in Crawl4AI Enabling Downloads Specifying Download Location Triggering Downloads Accessing Downloaded Files Example: Downloading Multiple Files Important Considerations Download Handling in Crawl4AI This guide explains how to use Crawl4AI to handle file downloads during crawling. You'll learn how to trigger downloads, specify download locations, and access downloaded files. Enabling Downloads To enable downloads, set the acceptdownloads parameter in the BrowserConfig object and pass it to the crawler. from crawl4ai.asyncconfigs import BrowserConfig, AsyncWebCrawler async def main(): config = BrowserConfig(acceptdownloads=True) Enable downloads globally async with AsyncWebCrawler(config=config) as crawler: ... your crawling logic ... asyncio.run(main()) Specifying Download Location Specify the download directory using the downloadspath attribute in the BrowserConfig object. If not provided, Crawl4AI defaults to creating a \"downloads\" directory inside the .crawl4ai folder in your home directory. from crawl4ai.asyncconfigs import BrowserConfig import os downloadspath = os.path.join(os.getcwd(), \"mydownloads\") Custom download path os.makedirs(downloadspath, existok=True) config = BrowserConfig(acceptdownloads=True, downloadspath=downloadspath) async def main(): async with AsyncWebCrawler(config=config) as crawler: result = await crawler.arun(url=\"https://example.com\") ... Triggering Downloads Downloads are typically triggered by user interactions on a web page, such as clicking a download button. Use jscode in CrawlerRunConfig to simulate these actions and waitfor to allow sufficient time for downloads to start. from crawl4ai.asyncconfigs import CrawlerRunConfig config = CrawlerRunConfig( jscode=\"\"\" const downloadLink = document.querySelector('a[href$=\".exe\"]'); if (downloadLink) { downloadLink.click(); } \"\"\", waitfor=5 Wait 5 seconds for the download to start ) result = await crawler.arun(url=\"https://www.python.org/downloads/\", config=config) Accessing Downloaded Files The downloadedfiles attribute of the CrawlResult object contains paths to downloaded files. if result.downloadedfiles: print(\"Downloaded files:\") for filepath in result.downloadedfiles: print(f\"- {filepath}\") filesize = os.path.getsize(filepath) print(f\"- File size: {filesize} bytes\") else: print(\"No files downloaded.\") Example: Downloading Multiple Files from crawl4ai.asyncconfigs import BrowserConfig, CrawlerRunConfig import os from pathlib import Path async def downloadmultiplefiles(url: str, downloadpath: str): config = BrowserConfig(acceptdownloads=True, downloadspath=downloadpath) async with AsyncWebCrawler(config=config) as crawler: runconfig = CrawlerRunConfig( jscode=\"\"\" const downloadLinks = document.querySelectorAll('a[download]'); for (const link of downloadLinks) { link.click(); // Delay between clicks await new Promise(r => setTimeout(r, 2000)); } \"\"\", waitfor=10 Wait for all downloads to start ) result = await crawler.arun(url=url, config=runconfig) if result.downloadedfiles: print(\"Downloaded files:\") for file in result.downloadedfiles: print(f\"- {file}\") else: print(\"No files downloaded.\") Usage downloadpath = os.path.join(Path.home(), \".crawl4ai\", \"downloads\") os.makedirs(downloadpath, existok=True) asyncio.run(downloadmultiplefiles(\"https://www.python.org/downloads/windows/\", downloadpath)) Important Considerations Browser Context: Downloads are managed within the browser context. Ensure jscode correctly targets the download triggers on the webpage. Timing: Use waitfor in CrawlerRunConfig to manage download timing. Error Handling: Handle errors to manage failed downloads or incorrect paths gracefully. Security: Scan downloaded files for potential security threats before use. This revised guide ensures consistency with the Crawl4AI codebase by using BrowserConfig and CrawlerRunConfig for all download-related configurations. Let me know if further adjustments are needed! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/lazy-loading/\n\nTitle: Lazy Loading - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Handling Lazy-Loaded Images Example: Ensuring Lazy Images Appear Combining with Other Link & Media Filters Tips & Troubleshooting Handling Lazy-Loaded Images Many websites now load images lazily as you scroll. If you need to ensure they appear in your final crawl (and in result.media), consider: 1. waitforimages=True \u2013 Wait for images to fully load. 2. scanfullpage \u2013 Force the crawler to scroll the entire page, triggering lazy loads. 3. scrolldelay \u2013 Add small delays between scroll steps. Note : If the site requires multiple \u201cLoad More\u201d triggers or complex interactions, see the Page Interaction docs. Example: Ensuring Lazy Images Appear import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig from crawl4ai.asyncconfigs import CacheMode async def main(): config = CrawlerRunConfig( Force the crawler to wait until images are fully loaded waitforimages=True, Option 1: If you want to automatically scroll the page to load images scanfullpage=True, Tells the crawler to try scrolling the entire page scrolldelay=0.5, Delay (seconds) between scroll steps Option 2: If the site uses a 'Load More' or JS triggers for images, you can also specify jscode or waitfor logic here. cachemode=CacheMode.BYPASS, verbose=True ) async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler: result = await crawler.arun(\"https://www.example.com/gallery\", config=config) if result.success: images = result.media.get(\"images\", []) print(\"Images found:\", len(images)) for i, img in enumerate(images[:5]): print(f\"[Image {i}] URL: {img['src']}, Score: {img.get('score','N/A')}\") else: print(\"Error:\", result.errormessage) if name == \"main\": asyncio.run(main()) Explanation : waitforimages=True The crawler tries to ensure images have finished loading before finalizing the HTML. scanfullpage=True Tells the crawler to attempt scrolling from top to bottom. Each scroll step helps trigger lazy loading. scrolldelay=0.5 Pause half a second between each scroll step. Helps the site load images before continuing. When to Use : Lazy-Loading : If images appear only when the user scrolls into view, scanfullpage + scrolldelay helps the crawler see them. Heavier Pages : If a page is extremely long, be mindful that scanning the entire page can be slow. Adjust scrolldelay or the max scroll steps as needed. Combining with Other Link & Media Filters You can still combine lazy-load logic with the usual excludeexternalimages , excludedomains , or link filtration: config = CrawlerRunConfig( waitforimages=True, scanfullpage=True, scrolldelay=0.5, Filter out external images if you only want local ones excludeexternalimages=True, Exclude certain domains for links excludedomains=[\"spammycdn.com\"], ) This approach ensures you see all images from the main domain while ignoring external ones, and the crawler physically scrolls the entire page so that lazy-loading triggers. Tips & Troubleshooting 1. Long Pages - Setting scanfullpage=True on extremely long or infinite-scroll pages can be resource-intensive. - Consider using hooks or specialized logic to load specific sections or \u201cLoad More\u201d triggers repeatedly. 2. Mixed Image Behavior - Some sites load images in batches as you scroll. If you\u2019re missing images, increase your scrolldelay or call multiple partial scrolls in a loop with JS code or hooks. 3. Combining with Dynamic Wait - If the site has a placeholder that only changes to a real image after a certain event, you might do waitfor=\"css:img.loaded\" or a custom JS waitfor. 4. Caching - If cachemode is enabled, repeated crawls might skip some network fetches. If you suspect caching is missing new images, set cachemode=CacheMode.BYPASS for fresh fetches. With lazy-loading support, waitforimages , and scanfullpage settings, you can capture the entire gallery or feed of images you expect\u2014even if the site only loads them as the user scrolls. Combine these with the standard media filtering and domain exclusion for a complete link & media handling strategy. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/hooks-auth/\n\nTitle: Hooks & Auth - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Hooks & Auth in AsyncWebCrawler Example: Using Hooks in AsyncWebCrawler Hook Lifecycle Summary When to Handle Authentication Additional Considerations Conclusion Hooks & Auth in AsyncWebCrawler Crawl4AI\u2019s hooks let you customize the crawler at specific points in the pipeline: 1. onbrowsercreated \u2013 After browser creation. 2. onpagecontextcreated \u2013 After a new context & page are created. 3. beforegoto \u2013 Just before navigating to a page. 4. aftergoto \u2013 Right after navigation completes. 5. onuseragentupdated \u2013 Whenever the user agent changes. 6. onexecutionstarted \u2013 Once custom JavaScript execution begins. 7. beforeretrievehtml \u2013 Just before the crawler retrieves final HTML. 8. beforereturnhtml \u2013 Right before returning the HTML content. Important : Avoid heavy tasks in onbrowsercreated since you don\u2019t yet have a page context. If you need to log in , do so in onpagecontextcreated. > note \"Important Hook Usage Warning\" Avoid Misusing Hooks : Do not manipulate page objects in the wrong hook or at the wrong time, as it can crash the pipeline or produce incorrect results. A common mistake is attempting to handle authentication prematurely\u2014such as creating or closing pages in onbrowsercreated. > > Use the Right Hook for Auth : If you need to log in or set tokens, use onpagecontextcreated. This ensures you have a valid page/context to work with, without disrupting the main crawling flow. > > Identity-Based Crawling : For robust auth, consider identity-based crawling (or passing a session ID) to preserve state. Run your initial login steps in a separate, well-defined process, then feed that session to your main crawl\u2014rather than shoehorning complex authentication into early hooks. Check out Identity-Based Crawling for more details. > > Be Cautious : Overwriting or removing elements in the wrong hook can compromise the final crawl. Keep hooks focused on smaller tasks (like route filters, custom headers), and let your main logic (crawling, data extraction) proceed normally. Below is an example demonstration. Example: Using Hooks in AsyncWebCrawler import asyncio import json from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from playwright.asyncapi import Page, BrowserContext async def main(): print(\"\ud83d\udd17 Hooks Example: Demonstrating recommended usage\") 1) Configure the browser browserconfig = BrowserConfig( headless=True, verbose=True ) 2) Configure the crawler run crawlerrunconfig = CrawlerRunConfig( jscode=\"window.scrollTo(0, document.body.scrollHeight);\", waitfor=\"body\", cachemode=CacheMode.BYPASS ) 3) Create the crawler instance crawler = AsyncWebCrawler(config=browserconfig) Define Hook Functions async def onbrowsercreated(browser, kwargs): Called once the browser instance is created (but no pages or contexts yet) print(\"[HOOK] onbrowsercreated - Browser created successfully!\") Typically, do minimal setup here if needed return browser async def onpagecontextcreated(page: Page, context: BrowserContext, kwargs): Called right after a new page + context are created (ideal for auth or route config). print(\"[HOOK] onpagecontextcreated - Setting up page & context.\") Example 1: Route filtering (e.g., block images) async def routefilter(route): if route.request.resourcetype == \"image\": print(f\"[HOOK] Blocking image request: {route.request.url}\") await route.abort() else: await route.continue() await context.route(\"\", routefilter) Example 2: (Optional) Simulate a login scenario (We do NOT create or close pages here, just do quick steps if needed) e.g., await page.goto(\"https://example.com/login\") e.g., await page.fill(\"input[name='username']\", \"testuser\") e.g., await page.fill(\"input[name='password']\", \"password123\") e.g., await page.click(\"button[type='submit']\") e.g., await page.waitforselector(\"#welcome\") e.g., await context.addcookies([...]) Then continue Example 3: Adjust the viewport await page.setviewportsize({\"width\": 1080, \"height\": 600}) return page async def beforegoto( page: Page, context: BrowserContext, url: str, kwargs ): Called before navigating to each URL. print(f\"[HOOK] beforegoto - About to navigate: {url}\") e.g., inject custom headers await page.setextrahttpheaders({ \"Custom-Header\": \"my-value\" }) return page async def aftergoto( page: Page, context: BrowserContext, url: str, response, kwargs ): Called after navigation completes. print(f\"[HOOK] aftergoto - Successfully loaded: {url}\") e.g., wait for a certain element if we want to verify try: await page.waitforselector('.content', timeout=1000) print(\"[HOOK] Found .content element!\") except: print(\"[HOOK] .content not found, continuing anyway.\") return page async def onuseragentupdated( page: Page, context: BrowserContext, useragent: str, kwargs ): Called whenever the user agent updates. print(f\"[HOOK] onuseragentupdated - New user agent: {useragent}\") return page async def onexecutionstarted(page: Page, context: BrowserContext, kwargs): Called after custom JavaScript execution begins. print(\"[HOOK] onexecutionstarted - JS code is running!\") return page async def beforeretrievehtml(page: Page, context: BrowserContext, kwargs): Called before final HTML retrieval. print(\"[HOOK] beforeretrievehtml - We can do final actions\") Example: Scroll again await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\") return page async def beforereturnhtml( page: Page, context: BrowserContext, html: str, kwargs ): Called just before returning the HTML in the result. print(f\"[HOOK] beforereturnhtml - HTML length: {len(html)}\") return page Attach Hooks crawler.crawlerstrategy.sethook(\"onbrowsercreated\", onbrowsercreated) crawler.crawlerstrategy.sethook( \"onpagecontextcreated\", onpagecontextcreated ) crawler.crawlerstrategy.sethook(\"beforegoto\", beforegoto) crawler.crawlerstrategy.sethook(\"aftergoto\", aftergoto) crawler.crawlerstrategy.sethook( \"onuseragentupdated\", onuseragentupdated ) crawler.crawlerstrategy.sethook( \"onexecutionstarted\", onexecutionstarted ) crawler.crawlerstrategy.sethook( \"beforeretrievehtml\", beforeretrievehtml ) crawler.crawlerstrategy.sethook( \"beforereturnhtml\", beforereturnhtml ) await crawler.start() 4) Run the crawler on an example page url = \"https://example.com\" result = await crawler.arun(url, config=crawlerrunconfig) if result.success: print(\"\\nCrawled URL:\", result.url) print(\"HTML length:\", len(result.html)) else: print(\"Error:\", result.errormessage) await crawler.close() if name == \"main\": asyncio.run(main()) Hook Lifecycle Summary 1. onbrowsercreated : - Browser is up, but no pages or contexts yet. - Light setup only\u2014don\u2019t try to open or close pages here (that belongs in onpagecontextcreated). 2. onpagecontextcreated : - Perfect for advanced auth or route blocking. - You have a page + context ready but haven\u2019t navigated to the target URL yet. 3. beforegoto : - Right before navigation. Typically used for setting custom headers or logging the target URL. 4. aftergoto : - After page navigation is done. Good place for verifying content or waiting on essential elements. 5. onuseragentupdated : - Whenever the user agent changes (for stealth or different UA modes). 6. onexecutionstarted : - If you set jscode or run custom scripts, this runs once your JS is about to start. 7. beforeretrievehtml : - Just before the final HTML snapshot is taken. Often you do a final scroll or lazy-load triggers here. 8. beforereturnhtml : - The last hook before returning HTML to the CrawlResult. Good for logging HTML length or minor modifications. When to Handle Authentication Recommended : Use onpagecontextcreated if you need to: Navigate to a login page or fill forms Set cookies or localStorage tokens Block resource routes to avoid ads This ensures the newly created context is under your control before arun() navigates to the main URL. Additional Considerations Session Management : If you want multiple arun() calls to reuse a single session, pass sessionid= in your CrawlerRunConfig. Hooks remain the same. Performance : Hooks can slow down crawling if they do heavy tasks. Keep them concise. Error Handling : If a hook fails, the overall crawl might fail. Catch exceptions or handle them gracefully. Concurrency : If you run arunmany(), each URL triggers these hooks in parallel. Ensure your hooks are thread/async-safe. Conclusion Hooks provide fine-grained control over: Browser creation (light tasks only) Page and context creation (auth, route blocking) Navigation phases Final HTML retrieval Follow the recommended usage: - Login or advanced tasks in onpagecontextcreated - Custom headers or logs in beforegoto / aftergoto - Scrolling or final checks in beforeretrievehtml / beforereturnhtml Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/proxy-security/\n\nTitle: Proxy & Security - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Proxy Basic Proxy Setup Authenticated Proxy Rotating Proxies Proxy Basic Proxy Setup Simple proxy configuration with BrowserConfig: from crawl4ai.asyncconfigs import BrowserConfig Using proxy URL browserconfig = BrowserConfig(proxy=\"http://proxy.example.com:8080\") async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun(url=\"https://example.com\") Using SOCKS proxy browserconfig = BrowserConfig(proxy=\"socks5://proxy.example.com:1080\") async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun(url=\"https://example.com\") Authenticated Proxy Use an authenticated proxy with BrowserConfig: from crawl4ai.asyncconfigs import BrowserConfig proxyconfig = { \"server\": \"http://proxy.example.com:8080\", \"username\": \"user\", \"password\": \"pass\" } browserconfig = BrowserConfig(proxyconfig=proxyconfig) async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun(url=\"https://example.com\") Here's the corrected documentation: Rotating Proxies Example using a proxy rotation service dynamically: from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def getnextproxy(): Your proxy rotation logic here return {\"server\": \"http://next.proxy.com:8080\"} async def main(): browserconfig = BrowserConfig() runconfig = CrawlerRunConfig() async with AsyncWebCrawler(config=browserconfig) as crawler: For each URL, create a new run config with different proxy for url in urls: proxy = await getnextproxy() Clone the config and update proxy - this creates a new browser context currentconfig = runconfig.clone(proxyconfig=proxy) result = await crawler.arun(url=url, config=currentconfig) if name == \"main\": import asyncio asyncio.run(main()) Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/session-management/\n\nTitle: Session Management - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Session Management Basic Session Usage Dynamic Content with Sessions Example 1: Basic Session-Based Crawling Advanced Technique 1: Custom Execution Hooks Advanced Technique 2: Integrated JavaScript Execution and Waiting Session Management Session management in Crawl4AI is a powerful feature that allows you to maintain state across multiple requests, making it particularly suitable for handling complex multi-step crawling tasks. It enables you to reuse the same browser tab (or page object) across sequential actions and crawls, which is beneficial for: Performing JavaScript actions before and after crawling. Executing multiple sequential crawls faster without needing to reopen tabs or allocate memory repeatedly. Note: This feature is designed for sequential workflows and is not suitable for parallel operations. Basic Session Usage Use BrowserConfig and CrawlerRunConfig to maintain state with a sessionid: from crawl4ai.asyncconfigs import BrowserConfig, CrawlerRunConfig async with AsyncWebCrawler() as crawler: sessionid = \"mysession\" Define configurations config1 = CrawlerRunConfig( url=\"https://example.com/page1\", sessionid=sessionid ) config2 = CrawlerRunConfig( url=\"https://example.com/page2\", sessionid=sessionid ) First request result1 = await crawler.arun(config=config1) Subsequent request using the same session result2 = await crawler.arun(config=config2) Clean up when done await crawler.crawlerstrategy.killsession(sessionid) Dynamic Content with Sessions Here's an example of crawling GitHub commits across multiple pages while preserving session state: from crawl4ai.asyncconfigs import CrawlerRunConfig from crawl4ai.extractionstrategy import JsonCssExtractionStrategy from crawl4ai.cachecontext import CacheMode async def crawldynamiccontent(): async with AsyncWebCrawler() as crawler: sessionid = \"githubcommitssession\" url = \"https://github.com/microsoft/TypeScript/commits/main\" allcommits = [] Define extraction schema schema = { \"name\": \"Commit Extractor\", \"baseSelector\": \"li.Box-sc-g0xbh4-0\", \"fields\": [{ \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\" }], } extractionstrategy = JsonCssExtractionStrategy(schema) JavaScript and wait configurations jsnextpage = \"\"\"document.querySelector('a[data-testid=\"pagination-next-button\"]').click();\"\"\" waitfor = \"\"\"() => document.querySelectorAll('li.Box-sc-g0xbh4-0').length > 0\"\"\" Crawl multiple pages for page in range(3): config = CrawlerRunConfig( url=url, sessionid=sessionid, extractionstrategy=extractionstrategy, jscode=jsnextpage if page > 0 else None, waitfor=waitfor if page > 0 else None, jsonly=page > 0, cachemode=CacheMode.BYPASS ) result = await crawler.arun(config=config) if result.success: commits = json.loads(result.extractedcontent) allcommits.extend(commits) print(f\"Page {page + 1}: Found {len(commits)} commits\") Clean up session await crawler.crawlerstrategy.killsession(sessionid) return allcommits Example 1: Basic Session-Based Crawling A simple example using session-based crawling: import asyncio from crawl4ai.asyncconfigs import BrowserConfig, CrawlerRunConfig from crawl4ai.cachecontext import CacheMode async def basicsessioncrawl(): async with AsyncWebCrawler() as crawler: sessionid = \"dynamiccontentsession\" url = \"https://example.com/dynamic-content\" for page in range(3): config = CrawlerRunConfig( url=url, sessionid=sessionid, jscode=\"document.querySelector('.load-more-button').click();\" if page > 0 else None, cssselector=\".content-item\", cachemode=CacheMode.BYPASS ) result = await crawler.arun(config=config) print(f\"Page {page + 1}: Found {result.extractedcontent.count('.content-item')} items\") await crawler.crawlerstrategy.killsession(sessionid) asyncio.run(basicsessioncrawl()) This example shows: 1. Reusing the same sessionid across multiple requests. 2. Executing JavaScript to load more content dynamically. 3. Properly closing the session to free resources. Advanced Technique 1: Custom Execution Hooks > Warning: You might feel confused by the end of the next few examples \ud83d\ude05, so make sure you are comfortable with the order of the parts before you start this. Use custom hooks to handle complex scenarios, such as waiting for content to load dynamically: async def advancedsessioncrawlwithhooks(): firstcommit = \"\" async def onexecutionstarted(page): nonlocal firstcommit try: while True: await page.waitforselector(\"li.commit-item h4\") commit = await page.queryselector(\"li.commit-item h4\") commit = await commit.evaluate(\"(element) => element.textContent\").strip() if commit and commit != firstcommit: firstcommit = commit break await asyncio.sleep(0.5) except Exception as e: print(f\"Warning: New content didn't appear: {e}\") async with AsyncWebCrawler() as crawler: sessionid = \"commitsession\" url = \"https://github.com/example/repo/commits/main\" crawler.crawlerstrategy.sethook(\"onexecutionstarted\", onexecutionstarted) jsnextpage = \"\"\"document.querySelector('a.pagination-next').click();\"\"\" for page in range(3): config = CrawlerRunConfig( url=url, sessionid=sessionid, jscode=jsnextpage if page > 0 else None, cssselector=\"li.commit-item\", jsonly=page > 0, cachemode=CacheMode.BYPASS ) result = await crawler.arun(config=config) print(f\"Page {page + 1}: Found {len(result.extractedcontent)} commits\") await crawler.crawlerstrategy.killsession(sessionid) asyncio.run(advancedsessioncrawlwithhooks()) This technique ensures new content loads before the next action. Advanced Technique 2: Integrated JavaScript Execution and Waiting Combine JavaScript execution and waiting logic for concise handling of dynamic content: async def integratedjsandwaitcrawl(): async with AsyncWebCrawler() as crawler: sessionid = \"integratedsession\" url = \"https://github.com/example/repo/commits/main\" jsnextpageandwait = \"\"\" (async () => { const getCurrentCommit = () => document.querySelector('li.commit-item h4').textContent.trim(); const initialCommit = getCurrentCommit(); document.querySelector('a.pagination-next').click(); while (getCurrentCommit() === initialCommit) { await new Promise(resolve => setTimeout(resolve, 100)); } })(); \"\"\" for page in range(3): config = CrawlerRunConfig( url=url, sessionid=sessionid, jscode=jsnextpageandwait if page > 0 else None, cssselector=\"li.commit-item\", jsonly=page > 0, cachemode=CacheMode.BYPASS ) result = await crawler.arun(config=config) print(f\"Page {page + 1}: Found {len(result.extractedcontent)} commits\") await crawler.crawlerstrategy.killsession(sessionid) asyncio.run(integratedjsandwaitcrawl()) Common Use Cases for Sessions 1. Authentication Flows : Login and interact with secured pages. 2. Pagination Handling : Navigate through multiple pages. 3. Form Submissions : Fill forms, submit, and process results. 4. Multi-step Processes : Complete workflows that span multiple actions. 5. Dynamic Content Navigation : Handle JavaScript-rendered or event-triggered content. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/multi-url-crawling/\n\nTitle: Multi-URL Crawling - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Advanced Multi-URL Crawling with Dispatchers 1. Introduction 2. Core Components 3. Available Dispatchers 4. Usage Examples 5. Dispatch Results 6. Summary Advanced Multi-URL Crawling with Dispatchers > Heads Up : Crawl4AI supports advanced dispatchers for parallel or throttled crawling, providing dynamic rate limiting and memory usage checks. The built-in arunmany() function uses these dispatchers to handle concurrency efficiently. 1. Introduction When crawling many URLs: Basic : Use arun() in a loop (simple but less efficient) Better : Use arunmany(), which efficiently handles multiple URLs with proper concurrency control Best : Customize dispatcher behavior for your specific needs (memory management, rate limits, etc.) Why Dispatchers? Adaptive : Memory-based dispatchers can pause or slow down based on system resources Rate-limiting : Built-in rate limiting with exponential backoff for 429/503 responses Real-time Monitoring : Live dashboard of ongoing tasks, memory usage, and performance Flexibility : Choose between memory-adaptive or semaphore-based concurrency 2. Core Components 2.1 Rate Limiter class RateLimiter: def init( Random delay range between requests basedelay: Tuple[float, float] = (1.0, 3.0), Maximum backoff delay maxdelay: float = 60.0, Retries before giving up maxretries: int = 3, Status codes triggering backoff ratelimitcodes: List[int] = [429, 503] ) Here\u2019s the revised and simplified explanation of the RateLimiter , focusing on constructor parameters and adhering to your markdown style and mkDocs guidelines. RateLimiter Constructor Parameters The RateLimiter is a utility that helps manage the pace of requests to avoid overloading servers or getting blocked due to rate limits. It operates internally to delay requests and handle retries but can be configured using its constructor parameters. Parameters of theRateLimiter constructor: 1. basedelay (Tuple[float, float], default: (1.0, 3.0)) The range for a random delay (in seconds) between consecutive requests to the same domain. A random delay is chosen between basedelay[0] and basedelay[1] for each request. This prevents sending requests at a predictable frequency, reducing the chances of triggering rate limits. Example: If basedelay = (2.0, 5.0), delays could be randomly chosen as 2.3s, 4.1s, etc. 2. maxdelay (float, default: 60.0) The maximum allowable delay when rate-limiting errors occur. When servers return rate-limit responses (e.g., 429 or 503), the delay increases exponentially with jitter. The maxdelay ensures the delay doesn\u2019t grow unreasonably high, capping it at this value. Example: For a maxdelay = 30.0, even if backoff calculations suggest a delay of 45s, it will cap at 30s. 3. maxretries (int, default: 3) The maximum number of retries for a request if rate-limiting errors occur. After encountering a rate-limit response, the RateLimiter retries the request up to this number of times. If all retries fail, the request is marked as failed, and the process continues. Example: If maxretries = 3, the system retries a failed request three times before giving up. 4. ratelimitcodes (List[int], default: [429, 503]) A list of HTTP status codes that trigger the rate-limiting logic. These status codes indicate the server is overwhelmed or actively limiting requests. You can customize this list to include other codes based on specific server behavior. Example: If ratelimitcodes = [429, 503, 504], the crawler will back off on these three error codes. How to Use theRateLimiter: Here\u2019s an example of initializing and using a RateLimiter in your project: from crawl4ai import RateLimiter Create a RateLimiter with custom settings ratelimiter = RateLimiter( basedelay=(2.0, 4.0), Random delay between 2-4 seconds maxdelay=30.0, Cap delay at 30 seconds maxretries=5, Retry up to 5 times on rate-limiting errors ratelimitcodes=[429, 503] Handle these HTTP status codes ) RateLimiter will handle delays and retries internally No additional setup is required for its operation The RateLimiter integrates seamlessly with dispatchers like MemoryAdaptiveDispatcher and SemaphoreDispatcher, ensuring requests are paced correctly without user intervention. Its internal mechanisms manage delays and retries to avoid overwhelming servers while maximizing efficiency. 2.2 Crawler Monitor The CrawlerMonitor provides real-time visibility into crawling operations: from crawl4ai import CrawlerMonitor, DisplayMode monitor = CrawlerMonitor( Maximum rows in live display maxvisiblerows=15, DETAILED or AGGREGATED view displaymode=DisplayMode.DETAILED ) Display Modes : 1. DETAILED : Shows individual task status, memory usage, and timing 2. AGGREGATED : Displays summary statistics and overall progress 3. Available Dispatchers 3.1 MemoryAdaptiveDispatcher (Default) Automatically manages concurrency based on system memory usage: from crawl4ai.asyncdispatcher import MemoryAdaptiveDispatcher dispatcher = MemoryAdaptiveDispatcher( memorythresholdpercent=90.0, Pause if memory exceeds this checkinterval=1.0, How often to check memory maxsessionpermit=10, Maximum concurrent tasks ratelimiter=RateLimiter( Optional rate limiting basedelay=(1.0, 2.0), maxdelay=30.0, maxretries=2 ), monitor=CrawlerMonitor( Optional monitoring maxvisiblerows=15, displaymode=DisplayMode.DETAILED ) ) Constructor Parameters: 1. memorythresholdpercent (float, default: 90.0) Specifies the memory usage threshold (as a percentage). If system memory usage exceeds this value, the dispatcher pauses crawling to prevent system overload. 2. checkinterval (float, default: 1.0) The interval (in seconds) at which the dispatcher checks system memory usage. 3. maxsessionpermit (int, default: 10) The maximum number of concurrent crawling tasks allowed. This ensures resource limits are respected while maintaining concurrency. 4. memorywaittimeout (float, default: 300.0) Optional timeout (in seconds). If memory usage exceeds memorythresholdpercent for longer than this duration, a MemoryError is raised. 5. ratelimiter (RateLimiter, default: None) Optional rate-limiting logic to avoid server-side blocking (e.g., for handling 429 or 503 errors). See RateLimiter for details. 6. monitor (CrawlerMonitor, default: None) Optional monitoring for real-time task tracking and performance insights. See CrawlerMonitor for details. 3.2 SemaphoreDispatcher Provides simple concurrency control with a fixed limit: from crawl4ai.asyncdispatcher import SemaphoreDispatcher dispatcher = SemaphoreDispatcher( maxsessionpermit=20, Maximum concurrent tasks ratelimiter=RateLimiter( Optional rate limiting basedelay=(0.5, 1.0), maxdelay=10.0 ), monitor=CrawlerMonitor( Optional monitoring maxvisiblerows=15, displaymode=DisplayMode.DETAILED ) ) Constructor Parameters: 1. maxsessionpermit (int, default: 20) The maximum number of concurrent crawling tasks allowed, irrespective of semaphore slots. 2. ratelimiter (RateLimiter, default: None) Optional rate-limiting logic to avoid overwhelming servers. See RateLimiter for details. 3. monitor (CrawlerMonitor, default: None) Optional monitoring for tracking task progress and resource usage. See CrawlerMonitor for details. 4. Usage Examples 4.1 Batch Processing (Default) async def crawlbatch(): browserconfig = BrowserConfig(headless=True, verbose=False) runconfig = CrawlerRunConfig( cachemode=CacheMode.BYPASS, stream=False Default: get all results at once ) dispatcher = MemoryAdaptiveDispatcher( memorythresholdpercent=70.0, checkinterval=1.0, maxsessionpermit=10, monitor=CrawlerMonitor( displaymode=DisplayMode.DETAILED ) ) async with AsyncWebCrawler(config=browserconfig) as crawler: Get all results at once results = await crawler.arunmany( urls=urls, config=runconfig, dispatcher=dispatcher ) Process all results after completion for result in results: if result.success: await processresult(result) else: print(f\"Failed to crawl {result.url}: {result.errormessage}\") Review: - Purpose: Executes a batch crawl with all URLs processed together after crawling is complete. - Dispatcher: Uses MemoryAdaptiveDispatcher to manage concurrency and system memory. - Stream: Disabled (stream=False), so all results are collected at once for post-processing. - Best Use Case: When you need to analyze results in bulk rather than individually during the crawl. 4.2 Streaming Mode async def crawlstreaming(): browserconfig = BrowserConfig(headless=True, verbose=False) runconfig = CrawlerRunConfig( cachemode=CacheMode.BYPASS, stream=True Enable streaming mode ) dispatcher = MemoryAdaptiveDispatcher( memorythresholdpercent=70.0, checkinterval=1.0, maxsessionpermit=10, monitor=CrawlerMonitor( displaymode=DisplayMode.DETAILED ) ) async with AsyncWebCrawler(config=browserconfig) as crawler: Process results as they become available async for result in await crawler.arunmany( urls=urls, config=runconfig, dispatcher=dispatcher ): if result.success: Process each result immediately await processresult(result) else: print(f\"Failed to crawl {result.url}: {result.errormessage}\") Review: - Purpose: Enables streaming to process results as soon as they\u2019re available. - Dispatcher: Uses MemoryAdaptiveDispatcher for concurrency and memory management. - Stream: Enabled (stream=True), allowing real-time processing during crawling. - Best Use Case: When you need to act on results immediately, such as for real-time analytics or progressive data storage. 4.3 Semaphore-based Crawling async def crawlwithsemaphore(urls): browserconfig = BrowserConfig(headless=True, verbose=False) runconfig = CrawlerRunConfig(cachemode=CacheMode.BYPASS) dispatcher = SemaphoreDispatcher( semaphorecount=5, ratelimiter=RateLimiter( basedelay=(0.5, 1.0), maxdelay=10.0 ), monitor=CrawlerMonitor( maxvisiblerows=15, displaymode=DisplayMode.DETAILED ) ) async with AsyncWebCrawler(config=browserconfig) as crawler: results = await crawler.arunmany( urls, config=runconfig, dispatcher=dispatcher ) return results Review: - Purpose: Uses SemaphoreDispatcher to limit concurrency with a fixed number of slots. - Dispatcher: Configured with a semaphore to control parallel crawling tasks. - Rate Limiter: Prevents servers from being overwhelmed by pacing requests. - Best Use Case: When you want precise control over the number of concurrent requests, independent of system memory. 4.4 Robots.txt Consideration import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): urls = [ \"https://example1.com\", \"https://example2.com\", \"https://example3.com\" ] config = CrawlerRunConfig( cachemode=CacheMode.ENABLED, checkrobotstxt=True, Will respect robots.txt for each URL semaphorecount=3 Max concurrent requests ) async with AsyncWebCrawler() as crawler: async for result in crawler.arunmany(urls, config=config): if result.success: print(f\"Successfully crawled {result.url}\") elif result.statuscode == 403 and \"robots.txt\" in result.errormessage: print(f\"Skipped {result.url} - blocked by robots.txt\") else: print(f\"Failed to crawl {result.url}: {result.errormessage}\") if name == \"main\": asyncio.run(main()) Review: - Purpose: Ensures compliance with robots.txt rules for ethical and legal web crawling. - Configuration: Set checkrobotstxt=True to validate each URL against robots.txt before crawling. - Dispatcher: Handles requests with concurrency limits (semaphorecount=3). - Best Use Case: When crawling websites that strictly enforce robots.txt policies or for responsible crawling practices. 5. Dispatch Results Each crawl result includes dispatch information: @dataclass class DispatchResult: taskid: str memoryusage: float peakmemory: float starttime: datetime endtime: datetime errormessage: str = \"\" Access via result.dispatchresult: for result in results: if result.success: dr = result.dispatchresult print(f\"URL: {result.url}\") print(f\"Memory: {dr.memoryusage:.1f}MB\") print(f\"Duration: {dr.endtime - dr.starttime}\") 6. Summary 1. Two Dispatcher Types : MemoryAdaptiveDispatcher (default): Dynamic concurrency based on memory SemaphoreDispatcher: Fixed concurrency limit 2. Optional Components : RateLimiter: Smart request pacing and backoff CrawlerMonitor: Real-time progress visualization 3. Key Benefits : Automatic memory management Built-in rate limiting Live progress monitoring Flexible concurrency control Choose the dispatcher that best fits your needs: MemoryAdaptiveDispatcher : For large crawls or limited resources SemaphoreDispatcher : For simple, fixed-concurrency scenarios Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/crawl-dispatcher/\n\nTitle: Crawl Dispatcher - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Crawl Dispatcher Crawl Dispatcher We\u2019re excited to announce a Crawl Dispatcher module that can handle thousands of crawling tasks simultaneously. By efficiently managing system resources (memory, CPU, network), this dispatcher ensures high-performance data extraction at scale. It also provides real-time monitoring of each crawler\u2019s status, memory usage, and overall progress. Stay tuned\u2014this feature is coming soon in an upcoming release of Crawl4AI! For the latest news, keep an eye on our changelogs and follow @unclecode on X. Below is a sample of how the dispatcher\u2019s performance monitor might look in action: !Crawl Dispatcher Performance Monitor We can\u2019t wait to bring you this streamlined, scalable approach to multi-URL crawling\u2014watch this space for updates! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/identity-based-crawling/\n\nTitle: Identity Based Crawling - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Preserve Your Identity with Crawl4AI 1. Managed Browsers: Your Digital Identity Solution 3. Using Managed Browsers in Crawl4AI 4. Magic Mode: Simplified Automation 5. Comparing Managed Browsers vs. Magic Mode 6. Summary Preserve Your Identity with Crawl4AI Crawl4AI empowers you to navigate and interact with the web using your authentic digital identity , ensuring you\u2019re recognized as a human and not mistaken for a bot. This tutorial covers: 1. Managed Browsers \u2013 The recommended approach for persistent profiles and identity-based crawling. 2. Magic Mode \u2013 A simplified fallback solution for quick automation without persistent identity. 1. Managed Browsers: Your Digital Identity Solution Managed Browsers let developers create and use persistent browser profiles. These profiles store local storage, cookies, and other session data, letting you browse as your real self \u2014complete with logins, preferences, and cookies. Key Benefits Authentic Browsing Experience : Retain session data and browser fingerprints as though you\u2019re a normal user. Effortless Configuration : Once you log in or solve CAPTCHAs in your chosen data directory, you can re-run crawls without repeating those steps. Empowered Data Access : If you can see the data in your own browser, you can automate its retrieval with your genuine identity. Below is a partial update to your Managed Browsers tutorial, specifically the section about creating a user-data directory using Playwright\u2019s Chromium binary rather than a system-wide Chrome/Edge. We\u2019ll show how to locate that binary and launch it with a --user-data-dir argument to set up your profile. You can then point BrowserConfig.userdatadir to that folder for subsequent crawls. Creating a User Data Directory (Command-Line Approach via Playwright) If you installed Crawl4AI (which installs Playwright under the hood), you already have a Playwright-managed Chromium on your system. Follow these steps to launch that Chromium from your command line, specifying a custom data directory: 1. Find the Playwright Chromium binary: - On most systems, installed browsers go under a /.cache/ms-playwright/ folder or similar path. - To see an overview of installed browsers, run: python -m playwright install --dry-run or playwright install --dry-run (depending on your environment). This shows where Playwright keeps Chromium. For instance, you might see a path like: /.cache/ms-playwright/chromium-1234/chrome-linux/chrome on Linux, or a corresponding folder on macOS/Windows. 2. Launch the Playwright Chromium binary with a custom user-data directory: Linux example /.cache/ms-playwright/chromium-1234/chrome-linux/chrome \\ --user-data-dir=/home/<you>/mychromeprofile macOS example (Playwright\u2019s internal binary) /Library/Caches/ms-playwright/chromium-1234/chrome-mac/Chromium.app/Contents/MacOS/Chromium \\ --user-data-dir=/Users/<you>/mychromeprofile Windows example (PowerShell/cmd) \"C:\\Users\\<you>\\AppData\\Local\\ms-playwright\\chromium-1234\\chrome-win\\chrome.exe\" ^ --user-data-dir=\"C:\\Users\\<you>\\mychromeprofile\" Replace the path with the actual subfolder indicated in your ms-playwright cache structure. - This opens a fresh Chromium with your new or existing data folder. - Log into any sites or configure your browser the way you want. - Close when done\u2014your profile data is saved in that folder. 3. Use that folder in BrowserConfig.userdatadir : from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig browserconfig = BrowserConfig( headless=True, usemanagedbrowser=True, userdatadir=\"/home/<you>/mychromeprofile\", browsertype=\"chromium\" ) Next time you run your code, it reuses that folder\u2014preserving your session data, cookies, local storage, etc. 3. Using Managed Browsers in Crawl4AI Once you have a data directory with your session data, pass it to BrowserConfig : import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig async def main(): 1) Reference your persistent data directory browserconfig = BrowserConfig( headless=True, 'True' for automated runs verbose=True, usemanagedbrowser=True, Enables persistent browser strategy browsertype=\"chromium\", userdatadir=\"/path/to/my-chrome-profile\" ) 2) Standard crawl config crawlconfig = CrawlerRunConfig( waitfor=\"css:.logged-in-content\" ) async with AsyncWebCrawler(config=browserconfig) as crawler: result = await crawler.arun(url=\"https://example.com/private\", config=crawlconfig) if result.success: print(\"Successfully accessed private data with your identity!\") else: print(\"Error:\", result.errormessage) if name == \"main\": asyncio.run(main()) Workflow 1. Login externally (via CLI or your normal Chrome with --user-data-dir=...). 2. Close that browser. 3. Use the same folder in userdatadir= in Crawl4AI. 4. Crawl \u2013 The site sees your identity as if you\u2019re the same user who just logged in. 4. Magic Mode: Simplified Automation If you don\u2019t need a persistent profile or identity-based approach, Magic Mode offers a quick way to simulate human-like browsing without storing long-term data. from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com\", config=CrawlerRunConfig( magic=True, Simplifies a lot of interaction removeoverlayelements=True, pagetimeout=60000 ) ) Magic Mode : Simulates a user-like experience Randomizes user agent & navigator Randomizes interactions & timings Masks automation signals Attempts pop-up handling But it\u2019s no substitute for true user-based sessions if you want a fully legitimate identity-based solution. 5. Comparing Managed Browsers vs. Magic Mode Feature | Managed Browsers | Magic Mode ---|---|--- Session Persistence | Full localStorage/cookies retained in userdatadir | No persistent data (fresh each run) Genuine Identity | Real user profile with full rights & preferences | Emulated user-like patterns, but no actual identity Complex Sites | Best for login-gated sites or heavy config | Simple tasks, minimal login or config needed Setup | External creation of userdatadir, then use in Crawl4AI | Single-line approach (magic=True) Reliability | Extremely consistent (same data across runs) | Good for smaller tasks, can be less stable 6. Summary Create your user-data directory by launching Chrome/Chromium externally with --user-data-dir=/some/path. Log in or configure sites as needed, then close the browser. Reference that folder in BrowserConfig(userdatadir=\"...\") + usemanagedbrowser=True. Enjoy persistent sessions that reflect your real identity. If you only need quick, ephemeral automation, Magic Mode might suffice. Recommended : Always prefer a Managed Browser for robust, identity-based crawling and simpler interactions with complex sites. Use Magic Mode for quick tasks or prototypes where persistent data is unnecessary. With these approaches, you preserve your authentic browsing environment, ensuring the site sees you exactly as a normal user\u2014no repeated logins or wasted time. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/advanced/ssl-certificate/\n\nTitle: SSL Certificate - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies SSLCertificate Reference 1. Overview 2. Construction & Fetching 3. Common Properties 4. Export Methods 5. Example Usage in Crawl4AI 6. Notes & Best Practices SSLCertificate Reference The SSLCertificate class encapsulates an SSL certificate\u2019s data and allows exporting it in various formats (PEM, DER, JSON, or text). It\u2019s used within Crawl4AI whenever you set fetchsslcertificate=True in your CrawlerRunConfig. 1. Overview Location : crawl4ai/sslcertificate.py class SSLCertificate: \"\"\" Represents an SSL certificate with methods to export in various formats. Main Methods: - fromurl(url, timeout=10) - fromfile(filepath) - frombinary(binarydata) - tojson(filepath=None) - topem(filepath=None) - toder(filepath=None) ... Common Properties: - issuer - subject - validfrom - validuntil - fingerprint \"\"\" Typical Use Case 1. You enable certificate fetching in your crawl by: CrawlerRunConfig(fetchsslcertificate=True, ...) 2. After arun(), if result.sslcertificate is present, it\u2019s an instance of SSLCertificate. 3. You can read basic properties (issuer, subject, validity) or export them in multiple formats. 2. Construction & Fetching 2.1 fromurl(url, timeout=10) Manually load an SSL certificate from a given URL (port 443). Typically used internally, but you can call it directly if you want: cert = SSLCertificate.fromurl(\"https://example.com\") if cert: print(\"Fingerprint:\", cert.fingerprint) 2.2 fromfile(filepath) Load from a file containing certificate data in ASN.1 or DER. Rarely needed unless you have local cert files: cert = SSLCertificate.fromfile(\"/path/to/cert.der\") 2.3 frombinary(binarydata) Initialize from raw binary. E.g., if you captured it from a socket or another source: cert = SSLCertificate.frombinary(rawbytes) 3. Common Properties After obtaining a SSLCertificate instance (e.g. result.sslcertificate from a crawl), you can read: 1. issuer (dict) - E.g. {\"CN\": \"My Root CA\", \"O\": \"...\"} 2. subject (dict) - E.g. {\"CN\": \"example.com\", \"O\": \"ExampleOrg\"} 3. validfrom (str) - NotBefore date/time. Often in ASN.1/UTC format. 4. validuntil (str) - NotAfter date/time. 5. fingerprint (str) - The SHA-256 digest (lowercase hex). - E.g. \"d14d2e...\" 4. Export Methods Once you have a SSLCertificate object, you can export or inspect it: 4.1 tojson(filepath=None) \u2192 Optional[str] Returns a JSON string containing the parsed certificate fields. If filepath is provided, saves it to disk instead, returning None. Usage : jsondata = cert.tojson() returns JSON string cert.tojson(\"certificate.json\") writes file, returns None 4.2 topem(filepath=None) \u2192 Optional[str] Returns a PEM-encoded string (common for web servers). If filepath is provided, saves it to disk instead. pemstr = cert.topem() in-memory PEM string cert.topem(\"/path/to/cert.pem\") saved to file 4.3 toder(filepath=None) \u2192 Optional[bytes] Returns the original DER (binary ASN.1) bytes. If filepath is specified, writes the bytes there instead. derbytes = cert.toder() cert.toder(\"certificate.der\") 4.4 (Optional) exportastext() If you see a method like exportastext(), it typically returns an OpenSSL-style textual representation. Not always needed, but can help for debugging or manual inspection. 5. Example Usage in Crawl4AI Below is a minimal sample showing how the crawler obtains an SSL cert from a site, then reads or exports it. The code snippet: import asyncio import os from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): tmpdir = \"tmp\" os.makedirs(tmpdir, existok=True) config = CrawlerRunConfig( fetchsslcertificate=True, cachemode=CacheMode.BYPASS ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com\", config=config) if result.success and result.sslcertificate: cert = result.sslcertificate 1. Basic Info print(\"Issuer CN:\", cert.issuer.get(\"CN\", \"\")) print(\"Valid until:\", cert.validuntil) print(\"Fingerprint:\", cert.fingerprint) 2. Export cert.tojson(os.path.join(tmpdir, \"certificate.json\")) cert.topem(os.path.join(tmpdir, \"certificate.pem\")) cert.toder(os.path.join(tmpdir, \"certificate.der\")) if name == \"main\": asyncio.run(main()) 6. Notes & Best Practices 1. Timeout : SSLCertificate.fromurl internally uses a default 10s socket connect and wraps SSL. 2. Binary Form : The certificate is loaded in ASN.1 (DER) form, then re-parsed by OpenSSL.crypto. 3. Validation : This does not validate the certificate chain or trust store. It only fetches and parses. 4. Integration : Within Crawl4AI, you typically just set fetchsslcertificate=True in CrawlerRunConfig; the final result\u2019s sslcertificate is automatically built. 5. Export : If you need to store or analyze a cert, the tojson and topem are quite universal. Summary SSLCertificate is a convenience class for capturing and exporting the TLS certificate from your crawled site(s). Common usage is in the CrawlResult.sslcertificate field, accessible after setting fetchsslcertificate=True. Offers quick access to essential certificate details (issuer, subject, fingerprint) and is easy to export (PEM, DER, JSON) for further analysis or server usage. Use it whenever you need insight into a site\u2019s certificate or require some form of cryptographic or compliance check. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/extraction/no-llm-strategies/\n\nTitle: LLM-Free Strategies - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Extracting JSON (No LLM) 1. Intro to Schema-Based Extraction 2. Simple Example: Crypto Prices 3. Advanced Schema & Nested Structures 4. Why \u201cNo LLM\u201d Is Often Better 5. Base Element Attributes & Additional Fields 6. Putting It All Together: Larger Example 7. Tips & Best Practices 8. Schema Generation Utility 9. Conclusion Extracting JSON (No LLM) One of Crawl4AI\u2019s most powerful features is extracting structured JSON from websites without relying on large language models. By defining a schema with CSS or XPath selectors, you can extract data instantly\u2014even from complex or nested HTML structures\u2014without the cost, latency, or environmental impact of an LLM. Why avoid LLM for basic extractions? 1. Faster & Cheaper: No API calls or GPU overhead. 2. Lower Carbon Footprint : LLM inference can be energy-intensive. A well-defined schema is practically carbon-free. 3. Precise & Repeatable: CSS/XPath selectors do exactly what you specify. LLM outputs can vary or hallucinate. 4. Scales Readily : For thousands of pages, schema-based extraction runs quickly and in parallel. Below, we\u2019ll explore how to craft these schemas and use them with JsonCssExtractionStrategy (or JsonXPathExtractionStrategy if you prefer XPath). We\u2019ll also highlight advanced features like nested fields and base element attributes. 1. Intro to Schema-Based Extraction A schema defines: 1. A base selector that identifies each \u201ccontainer\u201d element on the page (e.g., a product row, a blog post card). 2. Fields describing which CSS/XPath selectors to use for each piece of data you want to capture (text, attribute, HTML block, etc.). 3. Nested or list types for repeated or hierarchical structures. For example, if you have a list of products, each one might have a name, price, reviews, and \u201crelated products.\u201d This approach is faster and more reliable than an LLM for consistent, structured pages. 2. Simple Example: Crypto Prices Let\u2019s begin with a simple schema-based extraction using the JsonCssExtractionStrategy. Below is a snippet that extracts cryptocurrency prices from a site (similar to the legacy Coinbase example). Notice we don\u2019t call any LLM: import json import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def extractcryptoprices(): 1. Define a simple extraction schema schema = { \"name\": \"Crypto Prices\", \"baseSelector\": \"div.crypto-row\", Repeated elements \"fields\": [ { \"name\": \"coinname\", \"selector\": \"h2.coin-name\", \"type\": \"text\" }, { \"name\": \"price\", \"selector\": \"span.coin-price\", \"type\": \"text\" } ] } 2. Create the extraction strategy extractionstrategy = JsonCssExtractionStrategy(schema, verbose=True) 3. Set up your crawler config (if needed) config = CrawlerRunConfig( e.g., pass jscode or waitfor if the page is dynamic waitfor=\"css:.crypto-row:nth-child(20)\" cachemode = CacheMode.BYPASS, extractionstrategy=extractionstrategy, ) async with AsyncWebCrawler(verbose=True) as crawler: 4. Run the crawl and extraction result = await crawler.arun( url=\"https://example.com/crypto-prices\", config=config ) if not result.success: print(\"Crawl failed:\", result.errormessage) return 5. Parse the extracted JSON data = json.loads(result.extractedcontent) print(f\"Extracted {len(data)} coin entries\") print(json.dumps(data[0], indent=2) if data else \"No data found\") asyncio.run(extractcryptoprices()) Highlights : baseSelector : Tells us where each \u201citem\u201d (crypto row) is. fields : Two fields (coinname, price) using simple CSS selectors. Each field defines a type (e.g., text, attribute, html, regex, etc.). No LLM is needed, and the performance is near-instant for hundreds or thousands of items. XPath Example withraw:// HTML Below is a short example demonstrating XPath extraction plus the raw:// scheme. We\u2019ll pass a dummy HTML directly (no network request) and define the extraction strategy in CrawlerRunConfig. import json import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import JsonXPathExtractionStrategy async def extractcryptopricesxpath(): 1. Minimal dummy HTML with some repeating rows dummyhtml = \"\"\" <html> <body> <div class='crypto-row'> <h2 class='coin-name'>Bitcoin</h2> <span class='coin-price'>$28,000</span> </div> <div class='crypto-row'> <h2 class='coin-name'>Ethereum</h2> <span class='coin-price'>$1,800</span> </div> </body> </html> \"\"\" 2. Define the JSON schema (XPath version) schema = { \"name\": \"Crypto Prices via XPath\", \"baseSelector\": \"//div[@class='crypto-row']\", \"fields\": [ { \"name\": \"coinname\", \"selector\": \".//h2[@class='coin-name']\", \"type\": \"text\" }, { \"name\": \"price\", \"selector\": \".//span[@class='coin-price']\", \"type\": \"text\" } ] } 3. Place the strategy in the CrawlerRunConfig config = CrawlerRunConfig( extractionstrategy=JsonXPathExtractionStrategy(schema, verbose=True) ) 4. Use raw:// scheme to pass dummyhtml directly rawurl = f\"raw://{dummyhtml}\" async with AsyncWebCrawler(verbose=True) as crawler: result = await crawler.arun( url=rawurl, config=config ) if not result.success: print(\"Crawl failed:\", result.errormessage) return data = json.loads(result.extractedcontent) print(f\"Extracted {len(data)} coin rows\") if data: print(\"First item:\", data[0]) asyncio.run(extractcryptopricesxpath()) Key Points : 1. JsonXPathExtractionStrategy is used instead of JsonCssExtractionStrategy. 2. baseSelector and each field\u2019s \"selector\" use XPath instead of CSS. 3. raw:// lets us pass dummyhtml with no real network request\u2014handy for local testing. 4. Everything (including the extraction strategy) is in CrawlerRunConfig. That\u2019s how you keep the config self-contained, illustrate XPath usage, and demonstrate the raw scheme for direct HTML input\u2014all while avoiding the old approach of passing extractionstrategy directly to arun(). 3. Advanced Schema & Nested Structures Real sites often have nested or repeated data\u2014like categories containing products, which themselves have a list of reviews or features. For that, we can define nested or list (and even nestedlist) fields. Sample E-Commerce HTML We have a sample e-commerce HTML file on GitHub (example): https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sampleecommerce.html This snippet includes categories, products, features, reviews, and related items. Let\u2019s see how to define a schema that fully captures that structure without LLM. schema = { \"name\": \"E-commerce Product Catalog\", \"baseSelector\": \"div.category\", (1) We can define optional baseFields if we want to extract attributes from the category container \"baseFields\": [ {\"name\": \"datacatid\", \"type\": \"attribute\", \"attribute\": \"data-cat-id\"}, ], \"fields\": [ { \"name\": \"categoryname\", \"selector\": \"h2.category-name\", \"type\": \"text\" }, { \"name\": \"products\", \"selector\": \"div.product\", \"type\": \"nestedlist\", repeated sub-objects \"fields\": [ { \"name\": \"name\", \"selector\": \"h3.product-name\", \"type\": \"text\" }, { \"name\": \"price\", \"selector\": \"p.product-price\", \"type\": \"text\" }, { \"name\": \"details\", \"selector\": \"div.product-details\", \"type\": \"nested\", single sub-object \"fields\": [ { \"name\": \"brand\", \"selector\": \"span.brand\", \"type\": \"text\" }, { \"name\": \"model\", \"selector\": \"span.model\", \"type\": \"text\" } ] }, { \"name\": \"features\", \"selector\": \"ul.product-features li\", \"type\": \"list\", \"fields\": [ {\"name\": \"feature\", \"type\": \"text\"} ] }, { \"name\": \"reviews\", \"selector\": \"div.review\", \"type\": \"nestedlist\", \"fields\": [ { \"name\": \"reviewer\", \"selector\": \"span.reviewer\", \"type\": \"text\" }, { \"name\": \"rating\", \"selector\": \"span.rating\", \"type\": \"text\" }, { \"name\": \"comment\", \"selector\": \"p.review-text\", \"type\": \"text\" } ] }, { \"name\": \"relatedproducts\", \"selector\": \"ul.related-products li\", \"type\": \"list\", \"fields\": [ { \"name\": \"name\", \"selector\": \"span.related-name\", \"type\": \"text\" }, { \"name\": \"price\", \"selector\": \"span.related-price\", \"type\": \"text\" } ] } ] } ] } Key Takeaways: Nested vs. List : type: \"nested\" means a single sub-object (like details). type: \"list\" means multiple items that are simple dictionaries or single text fields. type: \"nestedlist\" means repeated complex objects (like products or reviews). Base Fields : We can extract attributes from the container element via \"baseFields\". For instance, \"datacatid\" might be data-cat-id=\"elect123\". Transforms : We can also define a transform if we want to lower/upper case, strip whitespace, or even run a custom function. Running the Extraction import json import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.extractionstrategy import JsonCssExtractionStrategy ecommerceschema = { ... the advanced schema from above ... } async def extractecommercedata(): strategy = JsonCssExtractionStrategy(ecommerceschema, verbose=True) config = CrawlerRunConfig() async with AsyncWebCrawler(verbose=True) as crawler: result = await crawler.arun( url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sampleecommerce.html\", extractionstrategy=strategy, config=config ) if not result.success: print(\"Crawl failed:\", result.errormessage) return Parse the JSON output data = json.loads(result.extractedcontent) print(json.dumps(data, indent=2) if data else \"No data found.\") asyncio.run(extractecommercedata()) If all goes well, you get a structured JSON array with each \u201ccategory,\u201d containing an array of products. Each product includes details, features, reviews, etc. All of that without an LLM. 4. Why \u201cNo LLM\u201d Is Often Better 1. Zero Hallucination : Schema-based extraction doesn\u2019t guess text. It either finds it or not. 2. Guaranteed Structure : The same schema yields consistent JSON across many pages, so your downstream pipeline can rely on stable keys. 3. Speed : LLM-based extraction can be 10\u20131000x slower for large-scale crawling. 4. Scalable : Adding or updating a field is a matter of adjusting the schema, not re-tuning a model. When might you consider an LLM? Possibly if the site is extremely unstructured or you want AI summarization. But always try a schema approach first for repeated or consistent data patterns. 5. Base Element Attributes & Additional Fields It\u2019s easy to extract attributes (like href, src, or data-xxx) from your base or nested elements using: { \"name\": \"href\", \"type\": \"attribute\", \"attribute\": \"href\", \"default\": null } You can define them in baseFields (extracted from the main container element) or in each field\u2019s sub-lists. This is especially helpful if you need an item\u2019s link or ID stored in the parent <div>. 6. Putting It All Together: Larger Example Consider a blog site. We have a schema that extracts the URL from each post card (via baseFields with an \"attribute\": \"href\"), plus the title, date, summary, and author: schema = { \"name\": \"Blog Posts\", \"baseSelector\": \"a.blog-post-card\", \"baseFields\": [ {\"name\": \"posturl\", \"type\": \"attribute\", \"attribute\": \"href\"} ], \"fields\": [ {\"name\": \"title\", \"selector\": \"h2.post-title\", \"type\": \"text\", \"default\": \"No Title\"}, {\"name\": \"date\", \"selector\": \"time.post-date\", \"type\": \"text\", \"default\": \"\"}, {\"name\": \"summary\", \"selector\": \"p.post-summary\", \"type\": \"text\", \"default\": \"\"}, {\"name\": \"author\", \"selector\": \"span.post-author\", \"type\": \"text\", \"default\": \"\"} ] } Then run with JsonCssExtractionStrategy(schema) to get an array of blog post objects, each with \"posturl\", \"title\", \"date\", \"summary\", \"author\". 7. Tips & Best Practices 1. Inspect the DOM in Chrome DevTools or Firefox\u2019s Inspector to find stable selectors. 2. Start Simple : Verify you can extract a single field. Then add complexity like nested objects or lists. 3. Test your schema on partial HTML or a test page before a big crawl. 4. Combine with JS Execution if the site loads content dynamically. You can pass jscode or waitfor in CrawlerRunConfig. 5. Look at Logs when verbose=True: if your selectors are off or your schema is malformed, it\u2019ll often show warnings. 6. Use baseFields if you need attributes from the container element (e.g., href, data-id), especially for the \u201cparent\u201d item. 7. Performance : For large pages, make sure your selectors are as narrow as possible. 8. Schema Generation Utility While manually crafting schemas is powerful and precise, Crawl4AI now offers a convenient utility to automatically generate extraction schemas using LLM. This is particularly useful when: 1. You're dealing with a new website structure and want a quick starting point 2. You need to extract complex nested data structures 3. You want to avoid the learning curve of CSS/XPath selector syntax Using the Schema Generator The schema generator is available as a static method on both JsonCssExtractionStrategy and JsonXPathExtractionStrategy. You can choose between OpenAI's GPT-4 or the open-source Ollama for schema generation: from crawl4ai.extractionstrategy import JsonCssExtractionStrategy, JsonXPathExtractionStrategy Sample HTML with product information html = \"\"\" <div class=\"product-card\"> <h2 class=\"title\">Gaming Laptop</h2> <div class=\"price\">$999.99</div> <div class=\"specs\"> <ul> <li>16GB RAM</li> <li>1TB SSD</li> </ul> </div> </div> \"\"\" Option 1: Using OpenAI (requires API token) cssschema = JsonCssExtractionStrategy.generateschema( html, schematype=\"css\", This is the default llmprovider=\"openai/gpt-4o\", Default provider apitoken=\"your-openai-token\" Required for OpenAI ) Option 2: Using Ollama (open source, no token needed) xpathschema = JsonXPathExtractionStrategy.generateschema( html, schematype=\"xpath\", llmprovider=\"ollama/llama3.3\", Open source alternative apitoken=None Not needed for Ollama ) Use the generated schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(cssschema) LLM Provider Options 1. OpenAI GPT-4 (openai/gpt4o) 2. Default provider 3. Requires an API token 4. Generally provides more accurate schemas 5. Set via environment variable: OPENAIAPIKEY 6. Ollama (ollama/llama3.3) 7. Open source alternative 8. No API token required 9. Self-hosted option 10. Good for development and testing Benefits of Schema Generation 1. One-Time Cost : While schema generation uses LLM, it's a one-time cost. The generated schema can be reused for unlimited extractions without further LLM calls. 2. Smart Pattern Recognition : The LLM analyzes the HTML structure and identifies common patterns, often producing more robust selectors than manual attempts. 3. Automatic Nesting : Complex nested structures are automatically detected and properly represented in the schema. 4. Learning Tool : The generated schemas serve as excellent examples for learning how to write your own schemas. Best Practices 1. Review Generated Schemas : While the generator is smart, always review and test the generated schema before using it in production. 2. Provide Representative HTML : The better your sample HTML represents the overall structure, the more accurate the generated schema will be. 3. Consider Both CSS and XPath : Try both schema types and choose the one that works best for your specific case. 4. Cache Generated Schemas : Since generation uses LLM, save successful schemas for reuse. 5. API Token Security : Never hardcode API tokens. Use environment variables or secure configuration management. 6. Choose Provider Wisely : 7. Use OpenAI for production-quality schemas 8. Use Ollama for development, testing, or when you need a self-hosted solution That's it for Extracting JSON (No LLM)! You've seen how schema-based approaches (either CSS or XPath) can handle everything from simple lists to deeply nested product catalogs\u2014instantly, with minimal overhead. Enjoy building robust scrapers that produce consistent, structured JSON for your data pipelines! 9. Conclusion With JsonCssExtractionStrategy (or JsonXPathExtractionStrategy), you can build powerful, LLM-free pipelines that: Scrape any consistent site for structured data. Support nested objects, repeating lists, or advanced transformations. Scale to thousands of pages quickly and reliably. Next Steps : Combine your extracted JSON with advanced filtering or summarization in a second pass if needed. For dynamic pages, combine strategies with jscode or infinite scroll hooking to ensure all content is loaded. Remember : For repeated, structured data, you don\u2019t need to pay for or wait on an LLM. A well-crafted schema plus CSS or XPath gets you the data faster, cleaner, and cheaper\u2014the real power of Crawl4AI. Last Updated : 2025-01-01 That\u2019s it for Extracting JSON (No LLM)! You\u2019ve seen how schema-based approaches (either CSS or XPath) can handle everything from simple lists to deeply nested product catalogs\u2014instantly, with minimal overhead. Enjoy building robust scrapers that produce consistent, structured JSON for your data pipelines! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/extraction/llm-strategies/\n\nTitle: LLM Strategies - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Extracting JSON (LLM) 1. Why Use an LLM? 2. Provider-Agnostic via LightLLM 3. How LLM Extraction Works 4. Key Parameters 5. Putting It in CrawlerRunConfig 6. Chunking Details 7. Input Format 8. Token Usage & Show Usage 9. Example: Building a Knowledge Graph 10. Best Practices & Caveats 11. Conclusion Extracting JSON (LLM) In some cases, you need to extract complex or unstructured information from a webpage that a simple CSS/XPath schema cannot easily parse. Or you want AI -driven insights, classification, or summarization. For these scenarios, Crawl4AI provides an LLM-based extraction strategy that: 1. Works with any large language model supported by LightLLM (Ollama, OpenAI, Claude, and more). 2. Automatically splits content into chunks (if desired) to handle token limits, then combines results. 3. Lets you define a schema (like a Pydantic model) or a simpler \u201cblock\u201d extraction approach. Important : LLM-based extraction can be slower and costlier than schema-based approaches. If your page data is highly structured, consider using JsonCssExtractionStrategy or JsonXPathExtractionStrategy first. But if you need AI to interpret or reorganize content, read on! 1. Why Use an LLM? Complex Reasoning : If the site\u2019s data is unstructured, scattered, or full of natural language context. Semantic Extraction : Summaries, knowledge graphs, or relational data that require comprehension. Flexible : You can pass instructions to the model to do more advanced transformations or classification. 2. Provider-Agnostic via LightLLM Crawl4AI uses a \u201cprovider string\u201d (e.g., \"openai/gpt-4o\", \"ollama/llama2.0\", \"aws/titan\") to identify your LLM. Any model that LightLLM supports is fair game. You just provide: provider : The <provider>/<modelname> identifier (e.g., \"openai/gpt-4\", \"ollama/llama2\", \"huggingface/google-flan\", etc.). apitoken : If needed (for OpenAI, HuggingFace, etc.); local models or Ollama might not require it. apibase (optional): If your provider has a custom endpoint. This means you aren\u2019t locked into a single LLM vendor. Switch or experiment easily. 3. How LLM Extraction Works 3.1 Flow 1. Chunking (optional): The HTML or markdown is split into smaller segments if it\u2019s very long (based on chunktokenthreshold, overlap, etc.). 2. Prompt Construction : For each chunk, the library forms a prompt that includes your instruction (and possibly schema or examples). 3. LLM Inference : Each chunk is sent to the model in parallel or sequentially (depending on your concurrency). 4. Combining : The results from each chunk are merged and parsed into JSON. 3.2 extractiontype \"schema\" : The model tries to return JSON conforming to your Pydantic-based schema. \"block\" : The model returns freeform text, or smaller JSON structures, which the library collects. For structured data, \"schema\" is recommended. You provide schema=YourPydanticModel.modeljsonschema(). 4. Key Parameters Below is an overview of important LLM extraction parameters. All are typically set inside LLMExtractionStrategy(...). You then put that strategy in your CrawlerRunConfig(..., extractionstrategy=...). 1. provider (str): e.g., \"openai/gpt-4\", \"ollama/llama2\". 2. apitoken (str): The API key or token for that model. May not be needed for local models. 3. schema (dict): A JSON schema describing the fields you want. Usually generated by YourModel.modeljsonschema(). 4. extractiontype (str): \"schema\" or \"block\". 5. instruction (str): Prompt text telling the LLM what you want extracted. E.g., \u201cExtract these fields as a JSON array.\u201d 6. chunktokenthreshold (int): Maximum tokens per chunk. If your content is huge, you can break it up for the LLM. 7. overlaprate (float): Overlap ratio between adjacent chunks. E.g., 0.1 means 10% of each chunk is repeated to preserve context continuity. 8. applychunking (bool): Set True to chunk automatically. If you want a single pass, set False. 9. inputformat (str): Determines which crawler result is passed to the LLM. Options include: - \"markdown\": The raw markdown (default). - \"fitmarkdown\": The filtered \u201cfit\u201d markdown if you used a content filter. - \"html\": The cleaned or raw HTML. 10. extraargs (dict): Additional LLM parameters like temperature, maxtokens, topp, etc. 11. showusage() : A method you can call to print out usage info (token usage per chunk, total cost if known). Example : extractionstrategy = LLMExtractionStrategy( provider=\"openai/gpt-4\", apitoken=\"YOUROPENAIKEY\", schema=MyModel.modeljsonschema(), extractiontype=\"schema\", instruction=\"Extract a list of items from the text with 'name' and 'price' fields.\", chunktokenthreshold=1200, overlaprate=0.1, applychunking=True, inputformat=\"html\", extraargs={\"temperature\": 0.1, \"maxtokens\": 1000}, verbose=True ) 5. Putting It in CrawlerRunConfig Important : In Crawl4AI, all strategy definitions should go inside the CrawlerRunConfig, not directly as a param in arun(). Here\u2019s a full example: import os import asyncio import json from pydantic import BaseModel, Field from typing import List from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import LLMExtractionStrategy class Product(BaseModel): name: str price: str async def main(): 1. Define the LLM extraction strategy llmstrategy = LLMExtractionStrategy( provider=\"openai/gpt-4o-mini\", e.g. \"ollama/llama2\" apitoken=os.getenv('OPENAIAPIKEY'), schema=Product.schemajson(), Or use modeljsonschema() extractiontype=\"schema\", instruction=\"Extract all product objects with 'name' and 'price' from the content.\", chunktokenthreshold=1000, overlaprate=0.0, applychunking=True, inputformat=\"markdown\", or \"html\", \"fitmarkdown\" extraargs={\"temperature\": 0.0, \"maxtokens\": 800} ) 2. Build the crawler config crawlconfig = CrawlerRunConfig( extractionstrategy=llmstrategy, cachemode=CacheMode.BYPASS ) 3. Create a browser config if needed browsercfg = BrowserConfig(headless=True) async with AsyncWebCrawler(config=browsercfg) as crawler: 4. Let's say we want to crawl a single page result = await crawler.arun( url=\"https://example.com/products\", config=crawlconfig ) if result.success: 5. The extracted content is presumably JSON data = json.loads(result.extractedcontent) print(\"Extracted items:\", data) 6. Show usage stats llmstrategy.showusage() prints token usage else: print(\"Error:\", result.errormessage) if name == \"main\": asyncio.run(main()) 6. Chunking Details 6.1 chunktokenthreshold If your page is large, you might exceed your LLM\u2019s context window. chunktokenthreshold sets the approximate max tokens per chunk. The library calculates word\u2192token ratio using wordtokenrate (often 0.75 by default). If chunking is enabled (applychunking=True), the text is split into segments. 6.2 overlaprate To keep context continuous across chunks, we can overlap them. E.g., overlaprate=0.1 means each subsequent chunk includes 10% of the previous chunk\u2019s text. This is helpful if your needed info might straddle chunk boundaries. 6.3 Performance & Parallelism By chunking, you can potentially process multiple chunks in parallel (depending on your concurrency settings and the LLM provider). This reduces total time if the site is huge or has many sections. 7. Input Format By default, LLMExtractionStrategy uses inputformat=\"markdown\", meaning the crawler\u2019s final markdown is fed to the LLM. You can change to: html : The cleaned HTML or raw HTML (depending on your crawler config) goes into the LLM. fitmarkdown : If you used, for instance, PruningContentFilter, the \u201cfit\u201d version of the markdown is used. This can drastically reduce tokens if you trust the filter. markdown : Standard markdown output from the crawler\u2019s markdowngenerator. This setting is crucial: if the LLM instructions rely on HTML tags, pick \"html\". If you prefer a text-based approach, pick \"markdown\". LLMExtractionStrategy( ... inputformat=\"html\", Instead of \"markdown\" or \"fitmarkdown\" ) 8. Token Usage & Show Usage To keep track of tokens and cost, each chunk is processed with an LLM call. We record usage in: usages (list): token usage per chunk or call. totalusage : sum of all chunk calls. showusage() : prints a usage report (if the provider returns usage data). llmstrategy = LLMExtractionStrategy(...) ... llmstrategy.showusage() e.g. \u201cTotal usage: 1241 tokens across 2 chunk calls\u201d If your model provider doesn\u2019t return usage info, these fields might be partial or empty. 9. Example: Building a Knowledge Graph Below is a snippet combining LLMExtractionStrategy with a Pydantic schema for a knowledge graph. Notice how we pass an instruction telling the model what to parse. import os import json import asyncio from typing import List from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import LLMExtractionStrategy class Entity(BaseModel): name: str description: str class Relationship(BaseModel): entity1: Entity entity2: Entity description: str relationtype: str class KnowledgeGraph(BaseModel): entities: List[Entity] relationships: List[Relationship] async def main(): LLM extraction strategy llmstrat = LLMExtractionStrategy( provider=\"openai/gpt-4\", apitoken=os.getenv('OPENAIAPIKEY'), schema=KnowledgeGraph.schemajson(), extractiontype=\"schema\", instruction=\"Extract entities and relationships from the content. Return valid JSON.\", chunktokenthreshold=1400, applychunking=True, inputformat=\"html\", extraargs={\"temperature\": 0.1, \"maxtokens\": 1500} ) crawlconfig = CrawlerRunConfig( extractionstrategy=llmstrat, cachemode=CacheMode.BYPASS ) async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler: Example page url = \"https://www.nbcnews.com/business\" result = await crawler.arun(url=url, config=crawlconfig) if result.success: with open(\"kbresult.json\", \"w\", encoding=\"utf-8\") as f: f.write(result.extractedcontent) llmstrat.showusage() else: print(\"Crawl failed:\", result.errormessage) if name == \"main\": asyncio.run(main()) Key Observations : extractiontype=\"schema\" ensures we get JSON fitting our KnowledgeGraph. inputformat=\"html\" means we feed HTML to the model. instruction guides the model to output a structured knowledge graph. 10. Best Practices & Caveats 1. Cost & Latency: LLM calls can be slow or expensive. Consider chunking or smaller coverage if you only need partial data. 2. Model Token Limits : If your page + instruction exceed the context window, chunking is essential. 3. Instruction Engineering : Well-crafted instructions can drastically improve output reliability. 4. Schema Strictness : \"schema\" extraction tries to parse the model output as JSON. If the model returns invalid JSON, partial extraction might happen, or you might get an error. 5. Parallel vs. Serial : The library can process multiple chunks in parallel, but you must watch out for rate limits on certain providers. 6. Check Output : Sometimes, an LLM might omit fields or produce extraneous text. You may want to post-validate with Pydantic or do additional cleanup. 11. Conclusion LLM-based extraction in Crawl4AI is provider-agnostic , letting you choose from hundreds of models via LightLLM. It\u2019s perfect for semantically complex tasks or generating advanced structures like knowledge graphs. However, it\u2019s slower and potentially costlier than schema-based approaches. Keep these tips in mind: Put your LLM strategy inCrawlerRunConfig. Use inputformat to pick which form (markdown, HTML, fitmarkdown) the LLM sees. Tweak chunktokenthreshold , overlaprate , and applychunking to handle large content efficiently. Monitor token usage with showusage(). If your site\u2019s data is consistent or repetitive, consider JsonCssExtractionStrategy first for speed and simplicity. But if you need an AI-driven approach, LLMExtractionStrategy offers a flexible, multi-provider solution for extracting structured JSON from any website. Next Steps : 1. Experiment with Different Providers - Try switching the provider (e.g., \"ollama/llama2\", \"openai/gpt-4o\", etc.) to see differences in speed, accuracy, or cost. - Pass different extraargs like temperature, topp, and maxtokens to fine-tune your results. 2. Performance Tuning - If pages are large, tweak chunktokenthreshold, overlaprate, or applychunking to optimize throughput. - Check the usage logs with showusage() to keep an eye on token consumption and identify potential bottlenecks. 3. Validate Outputs - If using extractiontype=\"schema\", parse the LLM\u2019s JSON with a Pydantic model for a final validation step. - Log or handle any parse errors gracefully, especially if the model occasionally returns malformed JSON. 4. Explore Hooks & Automation - Integrate LLM extraction with hooks for complex pre/post-processing. - Use a multi-step pipeline: crawl, filter, LLM-extract, then store or index results for further analysis. Last Updated : 2025-01-01 That\u2019s it for Extracting JSON (LLM) \u2014now you can harness AI to parse, classify, or reorganize data on the web. Happy crawling! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/extraction/clustring-strategies/\n\nTitle: Clustering Strategies - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Cosine Strategy How It Works Basic Usage Configuration Options Use Cases Advanced Features Best Practices Error Handling Cosine Strategy The Cosine Strategy in Crawl4AI uses similarity-based clustering to identify and extract relevant content sections from web pages. This strategy is particularly useful when you need to find and extract content based on semantic similarity rather than structural patterns. How It Works The Cosine Strategy: 1. Breaks down page content into meaningful chunks 2. Converts text into vector representations 3. Calculates similarity between chunks 4. Clusters similar content together 5. Ranks and filters content based on relevance Basic Usage from crawl4ai.extractionstrategy import CosineStrategy strategy = CosineStrategy( semanticfilter=\"product reviews\", Target content type wordcountthreshold=10, Minimum words per cluster simthreshold=0.3 Similarity threshold ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com/reviews\", extractionstrategy=strategy ) content = result.extractedcontent Configuration Options Core Parameters CosineStrategy( Content Filtering semanticfilter: str = None, Keywords/topic for content filtering wordcountthreshold: int = 10, Minimum words per cluster simthreshold: float = 0.3, Similarity threshold (0.0 to 1.0) Clustering Parameters maxdist: float = 0.2, Maximum distance for clustering linkagemethod: str = 'ward', Clustering linkage method topk: int = 3, Number of top categories to extract Model Configuration modelname: str = 'sentence-transformers/all-MiniLM-L6-v2', Embedding model verbose: bool = False Enable logging ) Parameter Details 1. semanticfilter - Sets the target topic or content type - Use keywords relevant to your desired content - Example: \"technical specifications\", \"user reviews\", \"pricing information\" 2. simthreshold - Controls how similar content must be to be grouped together - Higher values (e.g., 0.8) mean stricter matching - Lower values (e.g., 0.3) allow more variation Strict matching strategy = CosineStrategy(simthreshold=0.8) Loose matching strategy = CosineStrategy(simthreshold=0.3) 3. wordcountthreshold - Filters out short content blocks - Helps eliminate noise and irrelevant content Only consider substantial paragraphs strategy = CosineStrategy(wordcountthreshold=50) 4. topk - Number of top content clusters to return - Higher values return more diverse content Get top 5 most relevant content clusters strategy = CosineStrategy(topk=5) Use Cases 1. Article Content Extraction strategy = CosineStrategy( semanticfilter=\"main article content\", wordcountthreshold=100, Longer blocks for articles topk=1 Usually want single main content ) result = await crawler.arun( url=\"https://example.com/blog/post\", extractionstrategy=strategy ) 2. Product Review Analysis strategy = CosineStrategy( semanticfilter=\"customer reviews and ratings\", wordcountthreshold=20, Reviews can be shorter topk=10, Get multiple reviews simthreshold=0.4 Allow variety in review content ) 3. Technical Documentation strategy = CosineStrategy( semanticfilter=\"technical specifications documentation\", wordcountthreshold=30, simthreshold=0.6, Stricter matching for technical content maxdist=0.3 Allow related technical sections ) Advanced Features Custom Clustering strategy = CosineStrategy( linkagemethod='complete', Alternative clustering method maxdist=0.4, Larger clusters modelname='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' Multilingual support ) Content Filtering Pipeline strategy = CosineStrategy( semanticfilter=\"pricing plans features\", wordcountthreshold=15, simthreshold=0.5, topk=3 ) async def extractpricingfeatures(url: str): async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=url, extractionstrategy=strategy ) if result.success: content = json.loads(result.extractedcontent) return { 'pricingfeatures': content, 'clusters': len(content), 'similarityscores': [item['score'] for item in content] } Best Practices 1. Adjust Thresholds Iteratively - Start with default values - Adjust based on results - Monitor clustering quality 2. Choose Appropriate Word Count Thresholds - Higher for articles (100+) - Lower for reviews/comments (20+) - Medium for product descriptions (50+) 3. Optimize Performance strategy = CosineStrategy( wordcountthreshold=10, Filter early topk=5, Limit results verbose=True Monitor performance ) 4. Handle Different Content Types For mixed content pages strategy = CosineStrategy( semanticfilter=\"product features\", simthreshold=0.4, More flexible matching maxdist=0.3, Larger clusters topk=3 Multiple relevant sections ) Error Handling try: result = await crawler.arun( url=\"https://example.com\", extractionstrategy=strategy ) if result.success: content = json.loads(result.extractedcontent) if not content: print(\"No relevant content found\") else: print(f\"Extraction failed: {result.errormessage}\") except Exception as e: print(f\"Error during extraction: {str(e)}\") The Cosine Strategy is particularly effective when: - Content structure is inconsistent - You need semantic understanding - You want to find similar content blocks - Structure-based extraction (CSS/XPath) isn't reliable It works well with other strategies and can be used as a pre-processing step for LLM-based extraction. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/extraction/chunking/\n\nTitle: Chunking - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Chunking Strategies Why Use Chunking? Methods of Chunking Combining Chunking with Cosine Similarity Chunking Strategies Chunking strategies are critical for dividing large texts into manageable parts, enabling effective content processing and extraction. These strategies are foundational in cosine similarity-based extraction techniques, which allow users to retrieve only the most relevant chunks of content for a given query. Additionally, they facilitate direct integration into RAG (Retrieval-Augmented Generation) systems for structured and scalable workflows. Why Use Chunking? 1. Cosine Similarity and Query Relevance : Prepares chunks for semantic similarity analysis. 2. RAG System Integration : Seamlessly processes and stores chunks for retrieval. 3. Structured Processing : Allows for diverse segmentation methods, such as sentence-based, topic-based, or windowed approaches. Methods of Chunking 1. Regex-Based Chunking Splits text based on regular expression patterns, useful for coarse segmentation. Code Example : class RegexChunking: def init(self, patterns=None): self.patterns = patterns or [r'\\n\\n'] Default pattern for paragraphs def chunk(self, text): paragraphs = [text] for pattern in self.patterns: paragraphs = [seg for p in paragraphs for seg in re.split(pattern, p)] return paragraphs Example Usage text = \"\"\"This is the first paragraph. This is the second paragraph.\"\"\" chunker = RegexChunking() print(chunker.chunk(text)) 2. Sentence-Based Chunking Divides text into sentences using NLP tools, ideal for extracting meaningful statements. Code Example : from nltk.tokenize import senttokenize class NlpSentenceChunking: def chunk(self, text): sentences = senttokenize(text) return [sentence.strip() for sentence in sentences] Example Usage text = \"This is sentence one. This is sentence two.\" chunker = NlpSentenceChunking() print(chunker.chunk(text)) 3. Topic-Based Segmentation Uses algorithms like TextTiling to create topic-coherent chunks. Code Example : from nltk.tokenize import TextTilingTokenizer class TopicSegmentationChunking: def init(self): self.tokenizer = TextTilingTokenizer() def chunk(self, text): return self.tokenizer.tokenize(text) Example Usage text = \"\"\"This is an introduction. This is a detailed discussion on the topic.\"\"\" chunker = TopicSegmentationChunking() print(chunker.chunk(text)) 4. Fixed-Length Word Chunking Segments text into chunks of a fixed word count. Code Example : class FixedLengthWordChunking: def init(self, chunksize=100): self.chunksize = chunksize def chunk(self, text): words = text.split() return [' '.join(words[i:i + self.chunksize]) for i in range(0, len(words), self.chunksize)] Example Usage text = \"This is a long text with many words to be chunked into fixed sizes.\" chunker = FixedLengthWordChunking(chunksize=5) print(chunker.chunk(text)) 5. Sliding Window Chunking Generates overlapping chunks for better contextual coherence. Code Example : class SlidingWindowChunking: def init(self, windowsize=100, step=50): self.windowsize = windowsize self.step = step def chunk(self, text): words = text.split() chunks = [] for i in range(0, len(words) - self.windowsize + 1, self.step): chunks.append(' '.join(words[i:i + self.windowsize])) return chunks Example Usage text = \"This is a long text to demonstrate sliding window chunking.\" chunker = SlidingWindowChunking(windowsize=5, step=2) print(chunker.chunk(text)) Combining Chunking with Cosine Similarity To enhance the relevance of extracted content, chunking strategies can be paired with cosine similarity techniques. Here\u2019s an example workflow: Code Example : from sklearn.featureextraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosinesimilarity class CosineSimilarityExtractor: def init(self, query): self.query = query self.vectorizer = TfidfVectorizer() def findrelevantchunks(self, chunks): vectors = self.vectorizer.fittransform([self.query] + chunks) similarities = cosinesimilarity(vectors[0:1], vectors[1:]).flatten() return [(chunks[i], similarities[i]) for i in range(len(chunks))] Example Workflow text = \"\"\"This is a sample document. It has multiple sentences. We are testing chunking and similarity.\"\"\" chunker = SlidingWindowChunking(windowsize=5, step=3) chunks = chunker.chunk(text) query = \"testing chunking\" extractor = CosineSimilarityExtractor(query) relevantchunks = extractor.findrelevantchunks(chunks) print(relevantchunks) Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/api/async-webcrawler/\n\nTitle: AsyncWebCrawler - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies AsyncWebCrawler 1. Constructor Overview 2. Lifecycle: Start/Close or Context Manager 3. Primary Method: arun() 4. Batch Processing: arunmany() 5. CrawlResult Output 6. Quick Example 7. Best Practices & Migration Notes 8. Summary AsyncWebCrawler The AsyncWebCrawler is the core class for asynchronous web crawling in Crawl4AI. You typically create it once , optionally customize it with a BrowserConfig (e.g., headless, user agent), then run multiple arun() calls with different CrawlerRunConfig objects. Recommended usage : 1. Create a BrowserConfig for global browser settings. 2. Instantiate AsyncWebCrawler(config=browserconfig). 3. Use the crawler in an async context manager (async with) or manage start/close manually. 4. Call arun(url, config=crawlerrunconfig) for each page you want. 1. Constructor Overview class AsyncWebCrawler: def init( self, crawlerstrategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, alwaysbypasscache: bool = False, deprecated alwaysbypasscache: Optional[bool] = None, also deprecated basedirectory: str = ..., threadsafe: bool = False, kwargs, ): \"\"\" Create an AsyncWebCrawler instance. Args: crawlerstrategy: (Advanced) Provide a custom crawler strategy if needed. config: A BrowserConfig object specifying how the browser is set up. alwaysbypasscache: (Deprecated) Use CrawlerRunConfig.cachemode instead. basedirectory: Folder for storing caches/logs (if relevant). threadsafe: If True, attempts some concurrency safeguards. Usually False. kwargs: Additional legacy or debugging parameters. \"\"\" ) Typical Initialization python from crawl4ai import AsyncWebCrawler, BrowserConfig browsercfg = BrowserConfig( browsertype=\"chromium\", headless=True, verbose=True ) crawler = AsyncWebCrawler(config=browsercfg) Notes : Legacy parameters like alwaysbypasscache remain for backward compatibility, but prefer to set caching in CrawlerRunConfig. 2. Lifecycle: Start/Close or Context Manager 2.1 Context Manager (Recommended) async with AsyncWebCrawler(config=browsercfg) as crawler: result = await crawler.arun(\"https://example.com\") The crawler automatically starts/closes resources When the async with block ends, the crawler cleans up (closes the browser, etc.). 2.2 Manual Start & Close crawler = AsyncWebCrawler(config=browsercfg) await crawler.start() result1 = await crawler.arun(\"https://example.com\") result2 = await crawler.arun(\"https://another.com\") await crawler.close() Use this style if you have a long-running application or need full control of the crawler\u2019s lifecycle. 3. Primary Method: arun() async def arun( self, url: str, config: Optional[CrawlerRunConfig] = None, Legacy parameters for backward compatibility... ) -> CrawlResult: ... 3.1 New Approach You pass a CrawlerRunConfig object that sets up everything about a crawl\u2014content filtering, caching, session reuse, JS code, screenshots, etc. import asyncio from crawl4ai import CrawlerRunConfig, CacheMode runcfg = CrawlerRunConfig( cachemode=CacheMode.BYPASS, cssselector=\"main.article\", wordcountthreshold=10, screenshot=True ) async with AsyncWebCrawler(config=browsercfg) as crawler: result = await crawler.arun(\"https://example.com/news\", config=runcfg) print(\"Crawled HTML length:\", len(result.cleanedhtml)) if result.screenshot: print(\"Screenshot base64 length:\", len(result.screenshot)) 3.2 Legacy Parameters Still Accepted For backward compatibility, arun() can still accept direct arguments like cssselector=..., wordcountthreshold=..., etc., but we strongly advise migrating them into a CrawlerRunConfig. 4. Batch Processing: arunmany() async def arunmany( self, urls: List[str], config: Optional[CrawlerRunConfig] = None, Legacy parameters maintained for backwards compatibility... ) -> List[CrawlResult]: \"\"\" Process multiple URLs with intelligent rate limiting and resource monitoring. \"\"\" 4.1 Resource-Aware Crawling The arunmany() method now uses an intelligent dispatcher that: Monitors system memory usage Implements adaptive rate limiting Provides detailed progress monitoring Manages concurrent crawls efficiently 4.2 Example Usage from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, RateLimitConfig from crawl4ai.dispatcher import DisplayMode Configure browser browsercfg = BrowserConfig(headless=True) Configure crawler with rate limiting runcfg = CrawlerRunConfig( Enable rate limiting enableratelimiting=True, ratelimitconfig=RateLimitConfig( basedelay=(1.0, 2.0), Random delay between 1-2 seconds maxdelay=30.0, Maximum delay after rate limit hits maxretries=2, Number of retries before giving up ratelimitcodes=[429, 503] Status codes that trigger rate limiting ), Resource monitoring memorythresholdpercent=70.0, Pause if memory exceeds this checkinterval=0.5, How often to check resources maxsessionpermit=3, Maximum concurrent crawls displaymode=DisplayMode.DETAILED.value Show detailed progress ) urls = [ \"https://example.com/page1\", \"https://example.com/page2\", \"https://example.com/page3\" ] async with AsyncWebCrawler(config=browsercfg) as crawler: results = await crawler.arunmany(urls, config=runcfg) for result in results: print(f\"URL: {result.url}, Success: {result.success}\") 4.3 Key Features 1. Rate Limiting Automatic delay between requests Exponential backoff on rate limit detection Domain-specific rate limiting Configurable retry strategy 2. Resource Monitoring Memory usage tracking Adaptive concurrency based on system load Automatic pausing when resources are constrained 3. Progress Monitoring Detailed or aggregated progress display Real-time status updates Memory usage statistics 4. Error Handling Graceful handling of rate limits Automatic retries with backoff Detailed error reporting 5. CrawlResult Output Each arun() returns a CrawlResult containing: url: Final URL (if redirected). html: Original HTML. cleanedhtml: Sanitized HTML. markdownv2 (or future markdown): Markdown outputs (raw, fit, etc.). extractedcontent: If an extraction strategy was used (JSON for CSS/LLM strategies). screenshot, pdf: If screenshots/PDF requested. media, links: Information about discovered images/links. success, errormessage: Status info. For details, see CrawlResult doc. 6. Quick Example Below is an example hooking it all together: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy import json async def main(): 1. Browser config browsercfg = BrowserConfig( browsertype=\"firefox\", headless=False, verbose=True ) 2. Run config schema = { \"name\": \"Articles\", \"baseSelector\": \"article.post\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\" }, { \"name\": \"url\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\" } ] } runcfg = CrawlerRunConfig( cachemode=CacheMode.BYPASS, extractionstrategy=JsonCssExtractionStrategy(schema), wordcountthreshold=15, removeoverlayelements=True, waitfor=\"css:.post\" Wait for posts to appear ) async with AsyncWebCrawler(config=browsercfg) as crawler: result = await crawler.arun( url=\"https://example.com/blog\", config=runcfg ) if result.success: print(\"Cleaned HTML length:\", len(result.cleanedhtml)) if result.extractedcontent: articles = json.loads(result.extractedcontent) print(\"Extracted articles:\", articles[:2]) else: print(\"Error:\", result.errormessage) asyncio.run(main()) Explanation : We define a BrowserConfig with Firefox, no headless, and verbose=True. We define a CrawlerRunConfig that bypasses cache , uses a CSS extraction schema, has a wordcountthreshold=15, etc. We pass them to AsyncWebCrawler(config=...) and arun(url=..., config=...). 7. Best Practices & Migration Notes 1. Use BrowserConfig for global settings about the browser\u2019s environment. 2. Use CrawlerRunConfig for per-crawl logic (caching, content filtering, extraction strategies, wait conditions). 3. Avoid legacy parameters like cssselector or wordcountthreshold directly in arun(). Instead: runcfg = CrawlerRunConfig(cssselector=\".main-content\", wordcountthreshold=20) result = await crawler.arun(url=\"...\", config=runcfg) 4. Context Manager usage is simplest unless you want a persistent crawler across many calls. 8. Summary AsyncWebCrawler is your entry point to asynchronous crawling: Constructor accepts BrowserConfig (or defaults). arun(url, config=CrawlerRunConfig) is the main method for single-page crawls. arunmany(urls, config=CrawlerRunConfig) handles concurrency across multiple URLs. For advanced lifecycle control, use start() and close() explicitly. Migration : If you used AsyncWebCrawler(browsertype=\"chromium\", cssselector=\"...\"), move browser settings to BrowserConfig(...) and content/crawl logic to CrawlerRunConfig(...). This modular approach ensures your code is clean , scalable , and easy to maintain. For any advanced or rarely used parameters, see the BrowserConfig docs. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/api/arun/\n\nTitle: arun() - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies arun() Parameter Guide (New Approach) 1. Core Usage 2. Cache Control 3. Content Processing & Selection 4. Page Navigation & Timing 5. Session Management 6. Screenshot, PDF & Media Options 7. Extraction Strategy 8. Comprehensive Example 9. Best Practices 10. Conclusion arun() Parameter Guide (New Approach) In Crawl4AI\u2019s latest configuration model, nearly all parameters that once went directly to arun() are now part of CrawlerRunConfig. When calling arun(), you provide: await crawler.arun( url=\"https://example.com\", config=myrunconfig ) Below is an organized look at the parameters that can go inside CrawlerRunConfig, divided by their functional areas. For Browser settings (e.g., headless, browsertype), see BrowserConfig. 1. Core Usage from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): runconfig = CrawlerRunConfig( verbose=True, Detailed logging cachemode=CacheMode.ENABLED, Use normal read/write cache checkrobotstxt=True, Respect robots.txt rules ... other parameters ) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com\", config=runconfig ) Check if blocked by robots.txt if not result.success and result.statuscode == 403: print(f\"Error: {result.errormessage}\") Key Fields : - verbose=True logs each crawl step. - cachemode decides how to read/write the local crawl cache. 2. Cache Control cachemode (default: CacheMode.ENABLED) Use a built-in enum from CacheMode: ENABLED: Normal caching\u2014reads if available, writes if missing. DISABLED: No caching\u2014always refetch pages. READONLY: Reads from cache only; no new writes. WRITEONLY: Writes to cache but doesn\u2019t read existing data. BYPASS: Skips reading cache for this crawl (though it might still write if set up that way). runconfig = CrawlerRunConfig( cachemode=CacheMode.BYPASS ) Additional flags : bypasscache=True acts like CacheMode.BYPASS. disablecache=True acts like CacheMode.DISABLED. nocacheread=True acts like CacheMode.WRITEONLY. nocachewrite=True acts like CacheMode.READONLY. 3. Content Processing & Selection 3.1 Text Processing runconfig = CrawlerRunConfig( wordcountthreshold=10, Ignore text blocks <10 words onlytext=False, If True, tries to remove non-text elements keepdataattributes=False Keep or discard data- attributes ) 3.2 Content Selection runconfig = CrawlerRunConfig( cssselector=\".main-content\", Focus on .main-content region only excludedtags=[\"form\", \"nav\"], Remove entire tag blocks removeforms=True, Specifically strip <form> elements removeoverlayelements=True, Attempt to remove modals/popups ) 3.3 Link Handling runconfig = CrawlerRunConfig( excludeexternallinks=True, Remove external links from final content excludesocialmedialinks=True, Remove links to known social sites excludedomains=[\"ads.example.com\"], Exclude links to these domains excludesocialmediadomains=[\"facebook.com\",\"twitter.com\"], Extend the default list ) 3.4 Media Filtering runconfig = CrawlerRunConfig( excludeexternalimages=True Strip images from other domains ) 4. Page Navigation & Timing 4.1 Basic Browser Flow runconfig = CrawlerRunConfig( waitfor=\"css:.dynamic-content\", Wait for .dynamic-content delaybeforereturnhtml=2.0, Wait 2s before capturing final HTML pagetimeout=60000, Navigation & script timeout (ms) ) Key Fields : waitfor: \"css:selector\" or \"js:() => boolean\" e.g. js:() => document.querySelectorAll('.item').length > 10. meandelay & maxrange: define random delays for arunmany() calls. semaphorecount: concurrency limit when crawling multiple URLs. 4.2 JavaScript Execution runconfig = CrawlerRunConfig( jscode=[ \"window.scrollTo(0, document.body.scrollHeight);\", \"document.querySelector('.load-more')?.click();\" ], jsonly=False ) jscode can be a single string or a list of strings. jsonly=True means \u201cI\u2019m continuing in the same session with new JS steps, no new full navigation.\u201d 4.3 Anti-Bot runconfig = CrawlerRunConfig( magic=True, simulateuser=True, overridenavigator=True ) magic=True tries multiple stealth features. - simulateuser=True mimics mouse movements or random delays. - overridenavigator=True fakes some navigator properties (like user agent checks). 5. Session Management sessionid : runconfig = CrawlerRunConfig( sessionid=\"mysession123\" ) If re-used in subsequent arun() calls, the same tab/page context is continued (helpful for multi-step tasks or stateful browsing). 6. Screenshot, PDF & Media Options runconfig = CrawlerRunConfig( screenshot=True, Grab a screenshot as base64 screenshotwaitfor=1.0, Wait 1s before capturing pdf=True, Also produce a PDF imagedescriptionminwordthreshold=5, If analyzing alt text imagescorethreshold=3, Filter out low-score images ) Where they appear : - result.screenshot \u2192 Base64 screenshot string. - result.pdf \u2192 Byte array with PDF data. 7. Extraction Strategy For advanced data extraction (CSS/LLM-based), set extractionstrategy: runconfig = CrawlerRunConfig( extractionstrategy=mycssorllmstrategy ) The extracted data will appear in result.extractedcontent. 8. Comprehensive Example Below is a snippet combining many parameters: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extractionstrategy import JsonCssExtractionStrategy async def main(): Example schema schema = { \"name\": \"Articles\", \"baseSelector\": \"article.post\", \"fields\": [ {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}, {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"} ] } runconfig = CrawlerRunConfig( Core verbose=True, cachemode=CacheMode.ENABLED, checkrobotstxt=True, Respect robots.txt rules Content wordcountthreshold=10, cssselector=\"main.content\", excludedtags=[\"nav\", \"footer\"], excludeexternallinks=True, Page & JS jscode=\"document.querySelector('.show-more')?.click();\", waitfor=\"css:.loaded-block\", pagetimeout=30000, Extraction extractionstrategy=JsonCssExtractionStrategy(schema), Session sessionid=\"persistentsession\", Media screenshot=True, pdf=True, Anti-bot simulateuser=True, magic=True, ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(\"https://example.com/posts\", config=runconfig) if result.success: print(\"HTML length:\", len(result.cleanedhtml)) print(\"Extraction JSON:\", result.extractedcontent) if result.screenshot: print(\"Screenshot length:\", len(result.screenshot)) if result.pdf: print(\"PDF bytes length:\", len(result.pdf)) else: print(\"Error:\", result.errormessage) if name == \"main\": asyncio.run(main()) What we covered : 1. Crawling the main content region, ignoring external links. 2. Running JavaScript to click \u201c.show-more\u201d. 3. Waiting for \u201c.loaded-block\u201d to appear. 4. Generating a screenshot & PDF of the final page. 5. Extracting repeated \u201carticle.post\u201d elements with a CSS-based extraction strategy. 9. Best Practices 1. UseBrowserConfig for global browser settings (headless, user agent). 2. UseCrawlerRunConfig to handle the specific crawl needs: content filtering, caching, JS, screenshot, extraction, etc. 3. Keep your parameters consistent in run configs\u2014especially if you\u2019re part of a large codebase with multiple crawls. 4. Limit large concurrency (semaphorecount) if the site or your system can\u2019t handle it. 5. For dynamic pages, set jscode or scanfullpage so you load all content. 10. Conclusion All parameters that used to be direct arguments to arun() now belong in CrawlerRunConfig. This approach: Makes code clearer and more maintainable. Minimizes confusion about which arguments affect global vs. per-crawl behavior. Allows you to create reusable config objects for different pages or tasks. For a full reference, check out the CrawlerRunConfig Docs. Happy crawling with your structured, flexible config approach! Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/api/arun_many/\n\nTitle: arun_many() - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies arunmany(...) Reference Function Signature Differences from arun() Dispatcher Reference Common Pitfalls Conclusion arunmany(...) Reference > Note : This function is very similar to arun() but focused on concurrent or batch crawling. If you\u2019re unfamiliar with arun() usage, please read that doc first, then review this for differences. Function Signature async def arunmany( urls: Union[List[str], List[Any]], config: Optional[CrawlerRunConfig] = None, dispatcher: Optional[BaseDispatcher] = None, ... ) -> Union[List[CrawlResult], AsyncGenerator[CrawlResult, None]]: \"\"\" Crawl multiple URLs concurrently or in batches. :param urls: A list of URLs (or tasks) to crawl. :param config: (Optional) A default CrawlerRunConfig applying to each crawl. :param dispatcher: (Optional) A concurrency controller (e.g. MemoryAdaptiveDispatcher). ... :return: Either a list of CrawlResult objects, or an async generator if streaming is enabled. \"\"\" Differences from arun() 1. Multiple URLs : Instead of crawling a single URL, you pass a list of them (strings or tasks). The function returns either a list of CrawlResult or an async generator if streaming is enabled. 2. Concurrency & Dispatchers: dispatcher param allows advanced concurrency control. If omitted, a default dispatcher (like MemoryAdaptiveDispatcher) is used internally. Dispatchers handle concurrency, rate limiting, and memory-based adaptive throttling (see Multi-URL Crawling). 3. Streaming Support : Enable streaming by setting stream=True in your CrawlerRunConfig. When streaming, use async for to process results as they become available. Ideal for processing large numbers of URLs without waiting for all to complete. 4. Parallel Execution: arunmany() can run multiple requests concurrently under the hood. Each CrawlResult might also include a dispatchresult with concurrency details (like memory usage, start/end times). Basic Example (Batch Mode) Minimal usage: The default dispatcher will be used results = await crawler.arunmany( urls=[\"https://site1.com\", \"https://site2.com\"], config=CrawlerRunConfig(stream=False) Default behavior ) for res in results: if res.success: print(res.url, \"crawled OK!\") else: print(\"Failed:\", res.url, \"-\", res.errormessage) Streaming Example config = CrawlerRunConfig( stream=True, Enable streaming mode cachemode=CacheMode.BYPASS ) Process results as they complete async for result in await crawler.arunmany( urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"], config=config ): if result.success: print(f\"Just completed: {result.url}\") Process each result immediately processresult(result) With a Custom Dispatcher dispatcher = MemoryAdaptiveDispatcher( memorythresholdpercent=70.0, maxsessionpermit=10 ) results = await crawler.arunmany( urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"], config=myrunconfig, dispatcher=dispatcher ) Key Points : - Each URL is processed by the same or separate sessions, depending on the dispatcher\u2019s strategy. - dispatchresult in each CrawlResult (if using concurrency) can hold memory and timing info. - If you need to handle authentication or session IDs, pass them in each individual task or within your run config. Return Value Either a list of CrawlResult objects, or an async generator if streaming is enabled. You can iterate to check result.success or read each item\u2019s extractedcontent, markdown, or dispatchresult. Dispatcher Reference MemoryAdaptiveDispatcher : Dynamically manages concurrency based on system memory usage. SemaphoreDispatcher : Fixed concurrency limit, simpler but less adaptive. For advanced usage or custom settings, see Multi-URL Crawling with Dispatchers. Common Pitfalls 1. Large Lists : If you pass thousands of URLs, be mindful of memory or rate-limits. A dispatcher can help. 2. Session Reuse : If you need specialized logins or persistent contexts, ensure your dispatcher or tasks handle sessions accordingly. 3. Error Handling : Each CrawlResult might fail for different reasons\u2014always check result.success or the errormessage before proceeding. Conclusion Use arunmany() when you want to crawl multiple URLs simultaneously or in controlled parallel tasks. If you need advanced concurrency features (like memory-based adaptive throttling or complex rate-limiting), provide a dispatcher. Each result is a standard CrawlResult, possibly augmented with concurrency stats (dispatchresult) for deeper inspection. For more details on concurrency logic and dispatchers, see the Advanced Multi-URL Crawling docs. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/api/parameters/\n\nTitle: Browser & Crawler Config - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies 1. BrowserConfig \u2013 Controlling the Browser 1.1 Parameter Highlights 2. CrawlerRunConfig \u2013 Controlling Each Crawl 2.1 Parameter Highlights 2.2 Helper Methods 2.3 Example Usage 3. Putting It All Together 1. BrowserConfig \u2013 Controlling the Browser BrowserConfig focuses on how the browser is launched and behaves. This includes headless mode, proxies, user agents, and other environment tweaks. from crawl4ai import AsyncWebCrawler, BrowserConfig browsercfg = BrowserConfig( browsertype=\"chromium\", headless=True, viewportwidth=1280, viewportheight=720, proxy=\"http://user:pass@proxy:8080\", useragent=\"Mozilla/5.0 (X11; Linux x8664) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36\", ) 1.1 Parameter Highlights Parameter | Type / Default | What It Does ---|---|--- browsertype | \"chromium\", \"firefox\", \"webkit\"(default:\"chromium\") | Which browser engine to use. \"chromium\" is typical for many sites, \"firefox\" or \"webkit\" for specialized tests. headless | bool (default: True) | Headless means no visible UI. False is handy for debugging. viewportwidth | int (default: 1080) | Initial page width (in px). Useful for testing responsive layouts. viewportheight | int (default: 600) | Initial page height (in px). proxy | str (default: None) | Single-proxy URL if you want all traffic to go through it, e.g. \"http://user:pass@proxy:8080\". proxyconfig | dict (default: None) | For advanced or multi-proxy needs, specify details like {\"server\": \"...\", \"username\": \"...\", ...}. usepersistentcontext | bool (default: False) | If True, uses a persistent browser context (keep cookies, sessions across runs). Also sets usemanagedbrowser=True. userdatadir | str or None (default: None) | Directory to store user data (profiles, cookies). Must be set if you want permanent sessions. ignorehttpserrors | bool (default: True) | If True, continues despite invalid certificates (common in dev/staging). javascriptenabled | bool (default: True) | Disable if you want no JS overhead, or if only static content is needed. cookies | list (default: []) | Pre-set cookies, each a dict like {\"name\": \"session\", \"value\": \"...\", \"url\": \"...\"}. headers | dict (default: {}) | Extra HTTP headers for every request, e.g. {\"Accept-Language\": \"en-US\"}. useragent | str (default: Chrome-based UA) | Your custom or random user agent. useragentmode=\"random\" can shuffle it. lightmode | bool (default: False) | Disables some background features for performance gains. textmode | bool (default: False) | If True, tries to disable images/other heavy content for speed. usemanagedbrowser | bool (default: False) | For advanced \u201cmanaged\u201d interactions (debugging, CDP usage). Typically set automatically if persistent context is on. extraargs | list (default: []) | Additional flags for the underlying browser process, e.g. [\"--disable-extensions\"]. Tips : - Set headless=False to visually debug how pages load or how interactions proceed. - If you need authentication storage or repeated sessions, consider usepersistentcontext=True and specify userdatadir. - For large pages, you might need a bigger viewportwidth and viewportheight to handle dynamic content. 2. CrawlerRunConfig \u2013 Controlling Each Crawl While BrowserConfig sets up the environment , CrawlerRunConfig details how each crawl operation should behave: caching, content filtering, link or domain blocking, timeouts, JavaScript code, etc. from crawl4ai import AsyncWebCrawler, CrawlerRunConfig runcfg = CrawlerRunConfig( waitfor=\"css:.main-content\", wordcountthreshold=15, excludedtags=[\"nav\", \"footer\"], excludeexternallinks=True, stream=True, Enable streaming for arunmany() ) 2.1 Parameter Highlights We group them by category. A) Content Processing Parameter | Type / Default | What It Does ---|---|--- wordcountthreshold | int (default: 200) | Skips text blocks below X words. Helps ignore trivial sections. extractionstrategy | ExtractionStrategy (default: None) | If set, extracts structured data (CSS-based, LLM-based, etc.). markdowngenerator | MarkdownGenerationStrategy (None) | If you want specialized markdown output (citations, filtering, chunking, etc.). contentfilter | RelevantContentFilter (None) | Filters out irrelevant text blocks. E.g., PruningContentFilter or BM25ContentFilter. cssselector | str (None) | Retains only the part of the page matching this selector. excludedtags | list (None) | Removes entire tags (e.g. [\"script\", \"style\"]). excludedselector | str (None) | Like cssselector but to exclude. E.g. \"#ads, .tracker\". onlytext | bool (False) | If True, tries to extract text-only content. prettiify | bool (False) | If True, beautifies final HTML (slower, purely cosmetic). keepdataattributes | bool (False) | If True, preserve data- attributes in cleaned HTML. removeforms | bool (False) | If True, remove all <form> elements. B) Caching & Session Parameter | Type / Default | What It Does ---|---|--- cachemode | CacheMode or None | Controls how caching is handled (ENABLED, BYPASS, DISABLED, etc.). If None, typically defaults to ENABLED. sessionid | str or None | Assign a unique ID to reuse a single browser session across multiple arun() calls. bypasscache | bool (False) | If True, acts like CacheMode.BYPASS. disablecache | bool (False) | If True, acts like CacheMode.DISABLED. nocacheread | bool (False) | If True, acts like CacheMode.WRITEONLY (writes cache but never reads). nocachewrite | bool (False) | If True, acts like CacheMode.READONLY (reads cache but never writes). Use these for controlling whether you read or write from a local content cache. Handy for large batch crawls or repeated site visits. C) Page Navigation & Timing Parameter | Type / Default | What It Does ---|---|--- waituntil | str (domcontentloaded) | Condition for navigation to \u201ccomplete\u201d. Often \"networkidle\" or \"domcontentloaded\". pagetimeout | int (60000 ms) | Timeout for page navigation or JS steps. Increase for slow sites. waitfor | str or None | Wait for a CSS (\"css:selector\") or JS (\"js:() => bool\") condition before content extraction. waitforimages | bool (False) | Wait for images to load before finishing. Slows down if you only want text. delaybeforereturnhtml | float (0.1) | Additional pause (seconds) before final HTML is captured. Good for last-second updates. checkrobotstxt | bool (False) | Whether to check and respect robots.txt rules before crawling. If True, caches robots.txt for efficiency. meandelay and maxrange | float (0.1, 0.3) | If you call arunmany(), these define random delay intervals between crawls, helping avoid detection or rate limits. semaphorecount | int (5) | Max concurrency for arunmany(). Increase if you have resources for parallel crawls. D) Page Interaction Parameter | Type / Default | What It Does ---|---|--- jscode | str or list[str] (None) | JavaScript to run after load. E.g. \"document.querySelector('button')?.click();\". jsonly | bool (False) | If True, indicates we\u2019re reusing an existing session and only applying JS. No full reload. ignorebodyvisibility | bool (True) | Skip checking if <body> is visible. Usually best to keep True. scanfullpage | bool (False) | If True, auto-scroll the page to load dynamic content (infinite scroll). scrolldelay | float (0.2) | Delay between scroll steps if scanfullpage=True. processiframes | bool (False) | Inlines iframe content for single-page extraction. removeoverlayelements | bool (False) | Removes potential modals/popups blocking the main content. simulateuser | bool (False) | Simulate user interactions (mouse movements) to avoid bot detection. overridenavigator | bool (False) | Override navigator properties in JS for stealth. magic | bool (False) | Automatic handling of popups/consent banners. Experimental. adjustviewporttocontent | bool (False) | Resizes viewport to match page content height. If your page is a single-page app with repeated JS updates, set jsonly=True in subsequent calls, plus a sessionid for reusing the same tab. E) Media Handling Parameter | Type / Default | What It Does ---|---|--- screenshot | bool (False) | Capture a screenshot (base64) in result.screenshot. screenshotwaitfor | float or None | Extra wait time before the screenshot. screenshotheightthreshold | int (20000) | If the page is taller than this, alternate screenshot strategies are used. pdf | bool (False) | If True, returns a PDF in result.pdf. imagedescriptionminwordthreshold | int (50) | Minimum words for an image\u2019s alt text or description to be considered valid. imagescorethreshold | int (3) | Filter out low-scoring images. The crawler scores images by relevance (size, context, etc.). excludeexternalimages | bool (False) | Exclude images from other domains. F) Link/Domain Handling Parameter | Type / Default | What It Does ---|---|--- excludesocialmediadomains | list (e.g. Facebook/Twitter) | A default list can be extended. Any link to these domains is removed from final output. excludeexternallinks | bool (False) | Removes all links pointing outside the current domain. excludesocialmedialinks | bool (False) | Strips links specifically to social sites (like Facebook or Twitter). excludedomains | list ([]) | Provide a custom list of domains to exclude (like [\"ads.com\", \"trackers.io\"]). Use these for link-level content filtering (often to keep crawls \u201cinternal\u201d or to remove spammy domains). G) Rate Limiting & Resource Management Parameter | Type / Default | What It Does ---|---|--- enableratelimiting | bool (default: False) | Enable intelligent rate limiting for multiple URLs ratelimitconfig | RateLimitConfig (default: None) | Configuration for rate limiting behavior The RateLimitConfig class has these fields: Field | Type / Default | What It Does ---|---|--- basedelay | Tuple[float, float] (1.0, 3.0) | Random delay range between requests to the same domain maxdelay | float (60.0) | Maximum delay after rate limit detection maxretries | int (3) | Number of retries before giving up on rate-limited requests ratelimitcodes | List[int] ([429, 503]) | HTTP status codes that trigger rate limiting behavior Parameter | Type / Default | What It Does ---|---|--- memorythresholdpercent | float (70.0) | Maximum memory usage before pausing new crawls checkinterval | float (1.0) | How often to check system resources (in seconds) maxsessionpermit | int (20) | Maximum number of concurrent crawl sessions displaymode | str (None, \"DETAILED\", \"AGGREGATED\") | How to display progress information H) Debug & Logging Parameter | Type / Default | What It Does ---|---|--- verbose | bool (True) | Prints logs detailing each step of crawling, interactions, or errors. logconsole | bool (False) | Logs the page\u2019s JavaScript console output if you want deeper JS debugging. 2.2 Helper Methods Both BrowserConfig and CrawlerRunConfig provide a clone() method to create modified copies: Create a base configuration baseconfig = CrawlerRunConfig( cachemode=CacheMode.ENABLED, wordcountthreshold=200 ) Create variations using clone() streamconfig = baseconfig.clone(stream=True) nocacheconfig = baseconfig.clone( cachemode=CacheMode.BYPASS, stream=True ) The clone() method is particularly useful when you need slightly different configurations for different use cases, without modifying the original config. 2.3 Example Usage import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, RateLimitConfig async def main(): Configure the browser browsercfg = BrowserConfig( headless=False, viewportwidth=1280, viewportheight=720, proxy=\"http://user:pass@myproxy:8080\", textmode=True ) Configure the run runcfg = CrawlerRunConfig( cachemode=CacheMode.BYPASS, sessionid=\"mysession\", cssselector=\"main.article\", excludedtags=[\"script\", \"style\"], excludeexternallinks=True, waitfor=\"css:.article-loaded\", screenshot=True, enableratelimiting=True, ratelimitconfig=RateLimitConfig( basedelay=(1.0, 3.0), maxdelay=60.0, maxretries=3, ratelimitcodes=[429, 503] ), memorythresholdpercent=70.0, checkinterval=1.0, maxsessionpermit=20, displaymode=\"DETAILED\", stream=True ) async with AsyncWebCrawler(config=browsercfg) as crawler: result = await crawler.arun( url=\"https://example.com/news\", config=runcfg ) if result.success: print(\"Final cleanedhtml length:\", len(result.cleanedhtml)) if result.screenshot: print(\"Screenshot captured (base64, length):\", len(result.screenshot)) else: print(\"Crawl failed:\", result.errormessage) if name == \"main\": asyncio.run(main()) 2.4 Compliance & Ethics | Parameter | Type / Default | What It Does | |-----------------------|-------------------------|----------------------------------------------------------------------------------------------------------------------| | checkrobotstxt| bool (False) | When True, checks and respects robots.txt rules before crawling. Uses efficient caching with SQLite backend. | | useragent | str (None) | User agent string to identify your crawler. Used for robots.txt checking when enabled. | python runconfig = CrawlerRunConfig( checkrobotstxt=True, Enable robots.txt compliance useragent=\"MyBot/1.0\" Identify your crawler ) 3. Putting It All Together Use BrowserConfig for global browser settings: engine, headless, proxy, user agent. Use CrawlerRunConfig for each crawl\u2019s context : how to filter content, handle caching, wait for dynamic elements, or run JS. Pass both configs to AsyncWebCrawler (the BrowserConfig) and then to arun() (the CrawlerRunConfig). Create a modified copy with the clone() method streamcfg = runcfg.clone( stream=True, cachemode=CacheMode.BYPASS ) Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/api/crawl-result/\n\nTitle: CrawlResult - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies CrawlResult Reference 1. Basic Crawl Info 2. Raw / Cleaned Content 3. Markdown Fields 4. Media & Links 5. Additional Fields 6. dispatchresult (optional) 7. Example: Accessing Everything 8. Key Points & Future CrawlResult Reference The CrawlResult class encapsulates everything returned after a single crawl operation. It provides the raw or processed content , details on links and media, plus optional metadata (like screenshots, PDFs, or extracted JSON). Location : crawl4ai/crawler/models.py (for reference) class CrawlResult(BaseModel): url: str html: str success: bool cleanedhtml: Optional[str] = None media: Dict[str, List[Dict]] = {} links: Dict[str, List[Dict]] = {} downloadedfiles: Optional[List[str]] = None screenshot: Optional[str] = None pdf : Optional[bytes] = None markdown: Optional[Union[str, MarkdownGenerationResult]] = None markdownv2: Optional[MarkdownGenerationResult] = None fitmarkdown: Optional[str] = None fithtml: Optional[str] = None extractedcontent: Optional[str] = None metadata: Optional[dict] = None errormessage: Optional[str] = None sessionid: Optional[str] = None responseheaders: Optional[dict] = None statuscode: Optional[int] = None sslcertificate: Optional[SSLCertificate] = None dispatchresult: Optional[DispatchResult] = None ... Below is a field-by-field explanation and possible usage patterns. 1. Basic Crawl Info 1.1 url (str) What : The final crawled URL (after any redirects). Usage : print(result.url) e.g., \"https://example.com/\" 1.2 success (bool) What : True if the crawl pipeline ended without major errors; False otherwise. Usage : if not result.success: print(f\"Crawl failed: {result.errormessage}\") 1.3 statuscode (Optional[int]) What : The page\u2019s HTTP status code (e.g., 200, 404). Usage : if result.statuscode == 404: print(\"Page not found!\") 1.4 errormessage (Optional[str]) What : If success=False, a textual description of the failure. Usage : if not result.success: print(\"Error:\", result.errormessage) 1.5 sessionid (Optional[str]) What : The ID used for reusing a browser context across multiple calls. Usage : If you used sessionid=\"loginsession\" in CrawlerRunConfig, see it here: print(\"Session:\", result.sessionid) 1.6 responseheaders (Optional[dict]) What : Final HTTP response headers. Usage : if result.responseheaders: print(\"Server:\", result.responseheaders.get(\"Server\", \"Unknown\")) 1.7 sslcertificate (Optional[SSLCertificate]) What : If fetchsslcertificate=True in your CrawlerRunConfig, result.sslcertificate contains a SSLCertificate object describing the site\u2019s certificate. You can export the cert in multiple formats (PEM/DER/JSON) or access its properties like issuer, subject, validfrom, validuntil, etc. Usage : if result.sslcertificate: print(\"Issuer:\", result.sslcertificate.issuer) 2. Raw / Cleaned Content 2.1 html (str) What : The original unmodified HTML from the final page load. Usage : Possibly large print(len(result.html)) 2.2 cleanedhtml (Optional[str]) What : A sanitized HTML version\u2014scripts, styles, or excluded tags are removed based on your CrawlerRunConfig. Usage : print(result.cleanedhtml[:500]) Show a snippet 2.3 fithtml (Optional[str]) What : If a content filter or heuristic (e.g., Pruning/BM25) modifies the HTML, the \u201cfit\u201d or post-filter version. When : This is only present if your markdowngenerator or contentfilter produces it. Usage : if result.fithtml: print(\"High-value HTML content:\", result.fithtml[:300]) 3. Markdown Fields 3.1 The Markdown Generation Approach Crawl4AI can convert HTML\u2192Markdown, optionally including: Raw markdown Links as citations (with a references section) Fit markdown if a content filter is used (like Pruning or BM25) 3.2 markdownv2 (Optional[MarkdownGenerationResult]) What : The structured object holding multiple markdown variants. Soon to be consolidated into markdown. MarkdownGenerationResult includes: - rawmarkdown (str) : The full HTML\u2192Markdown conversion. - markdownwithcitations (str) : Same markdown, but with link references as academic-style citations. - referencesmarkdown (str) : The reference list or footnotes at the end. - fitmarkdown (Optional[str]) : If content filtering (Pruning/BM25) was applied, the filtered \u201cfit\u201d text. - fithtml (Optional[str]) : The HTML that led to fitmarkdown. Usage : if result.markdownv2: mdres = result.markdownv2 print(\"Raw MD:\", mdres.rawmarkdown[:300]) print(\"Citations MD:\", mdres.markdownwithcitations[:300]) print(\"References:\", mdres.referencesmarkdown) if mdres.fitmarkdown: print(\"Pruned text:\", mdres.fitmarkdown[:300]) 3.3 markdown (Optional[Union[str, MarkdownGenerationResult]]) What : In future versions, markdown will fully replace markdownv2. Right now, it might be a str or a MarkdownGenerationResult. Usage : Soon, you might see: if isinstance(result.markdown, MarkdownGenerationResult): print(result.markdown.rawmarkdown[:200]) else: print(result.markdown) 3.4 fitmarkdown (Optional[str]) What : A direct reference to the final filtered markdown (legacy approach). When : This is set if a filter or content strategy explicitly writes there. Usually overshadowed by markdownv2.fitmarkdown. Usage : print(result.fitmarkdown) Legacy field, prefer result.markdownv2.fitmarkdown Important : \u201cFit\u201d content (in fitmarkdown/fithtml) only exists if you used a filter (like PruningContentFilter or BM25ContentFilter) within a MarkdownGenerationStrategy. 4. Media & Links 4.1 media (Dict[str, List[Dict]]) What : Contains info about discovered images, videos, or audio. Typically keys: \"images\", \"videos\", \"audios\". Common Fields in each item: src (str) : Media URL alt or title (str) : Descriptive text score (float) : Relevance score if the crawler\u2019s heuristic found it \u201cimportant\u201d desc or description (Optional[str]) : Additional context extracted from surrounding text Usage : images = result.media.get(\"images\", []) for img in images: if img.get(\"score\", 0) > 5: print(\"High-value image:\", img[\"src\"]) 4.2 links (Dict[str, List[Dict]]) What : Holds internal and external link data. Usually two keys: \"internal\" and \"external\". Common Fields : href (str) : The link target text (str) : Link text title (str) : Title attribute context (str) : Surrounding text snippet domain (str) : If external, the domain Usage : for link in result.links[\"internal\"]: print(f\"Internal link to {link['href']} with text {link['text']}\") 5. Additional Fields 5.1 extractedcontent (Optional[str]) What : If you used extractionstrategy (CSS, LLM, etc.), the structured output (JSON). Usage : if result.extractedcontent: data = json.loads(result.extractedcontent) print(data) 5.2 downloadedfiles (Optional[List[str]]) What : If acceptdownloads=True in your BrowserConfig + downloadspath, lists local file paths for downloaded items. Usage : if result.downloadedfiles: for filepath in result.downloadedfiles: print(\"Downloaded:\", filepath) 5.3 screenshot (Optional[str]) What : Base64-encoded screenshot if screenshot=True in CrawlerRunConfig. Usage : import base64 if result.screenshot: with open(\"page.png\", \"wb\") as f: f.write(base64.b64decode(result.screenshot)) 5.4 pdf (Optional[bytes]) What : Raw PDF bytes if pdf=True in CrawlerRunConfig. Usage : if result.pdf: with open(\"page.pdf\", \"wb\") as f: f.write(result.pdf) 5.5 metadata (Optional[dict]) What : Page-level metadata if discovered (title, description, OG data, etc.). Usage : if result.metadata: print(\"Title:\", result.metadata.get(\"title\")) print(\"Author:\", result.metadata.get(\"author\")) 6. dispatchresult (optional) A DispatchResult object providing additional concurrency and resource usage information when crawling URLs in parallel (e.g., via arunmany() with custom dispatchers). It contains: taskid : A unique identifier for the parallel task. memoryusage (float): The memory (in MB) used at the time of completion. peakmemory (float): The peak memory usage (in MB) recorded during the task\u2019s execution. starttime / endtime (datetime): Time range for this crawling task. errormessage (str): Any dispatcher- or concurrency-related error encountered. Example usage: for result in results: if result.success and result.dispatchresult: dr = result.dispatchresult print(f\"URL: {result.url}, Task ID: {dr.taskid}\") print(f\"Memory: {dr.memoryusage:.1f} MB (Peak: {dr.peakmemory:.1f} MB)\") print(f\"Duration: {dr.endtime - dr.starttime}\") > Note : This field is typically populated when using arunmany(...) alongside a dispatcher (e.g., MemoryAdaptiveDispatcher or SemaphoreDispatcher). If no concurrency or dispatcher is used, dispatchresult may remain None. 7. Example: Accessing Everything async def handleresult(result: CrawlResult): if not result.success: print(\"Crawl error:\", result.errormessage) return Basic info print(\"Crawled URL:\", result.url) print(\"Status code:\", result.statuscode) HTML print(\"Original HTML size:\", len(result.html)) print(\"Cleaned HTML size:\", len(result.cleanedhtml or \"\")) Markdown output if result.markdownv2: print(\"Raw Markdown:\", result.markdownv2.rawmarkdown[:300]) print(\"Citations Markdown:\", result.markdownv2.markdownwithcitations[:300]) if result.markdownv2.fitmarkdown: print(\"Fit Markdown:\", result.markdownv2.fitmarkdown[:200]) else: print(\"Raw Markdown (legacy):\", result.markdown[:200] if result.markdown else \"N/A\") Media & Links if \"images\" in result.media: print(\"Image count:\", len(result.media[\"images\"])) if \"internal\" in result.links: print(\"Internal link count:\", len(result.links[\"internal\"])) Extraction strategy result if result.extractedcontent: print(\"Structured data:\", result.extractedcontent) Screenshot/PDF if result.screenshot: print(\"Screenshot length:\", len(result.screenshot)) if result.pdf: print(\"PDF bytes length:\", len(result.pdf)) 8. Key Points & Future 1. markdownv2 vs markdown - Right now, markdownv2 is the more robust container (MarkdownGenerationResult), providing rawmarkdown , markdownwithcitations , references, plus possible fitmarkdown. - In future versions, everything will unify under markdown. If you rely on advanced features (citations, fit content), check markdownv2. 2. Fit Content - fitmarkdown and fithtml appear only if you used a content filter (like PruningContentFilter or BM25ContentFilter) inside your MarkdownGenerationStrategy or set them directly. - If no filter is used, they remain None. 3. References & Citations - If you enable link citations in your DefaultMarkdownGenerator (options={\"citations\": True}), you\u2019ll see markdownwithcitations plus a referencesmarkdown block. This helps large language models or academic-like referencing. 4. Links & Media - links[\"internal\"] and links[\"external\"] group discovered anchors by domain. - media[\"images\"] / [\"videos\"] / [\"audios\"] store extracted media elements with optional scoring or context. 5. Error Cases - If success=False, check errormessage (e.g., timeouts, invalid URLs). - statuscode might be None if we failed before an HTTP response. Use CrawlResult to glean all final outputs and feed them into your data pipelines, AI models, or archives. With the synergy of a properly configured BrowserConfig and CrawlerRunConfig , the crawler can produce robust, structured results here in CrawlResult. Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/api/strategies/\n\nTitle: Strategies - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies Extraction & Chunking Strategies API Extraction Strategies Chunking Strategies Usage Examples Best Practices Extraction & Chunking Strategies API This documentation covers the API reference for extraction and chunking strategies in Crawl4AI. Extraction Strategies All extraction strategies inherit from the base ExtractionStrategy class and implement two key methods: - extract(url: str, html: str) -> List[Dict[str, Any]] - run(url: str, sections: List[str]) -> List[Dict[str, Any]] LLMExtractionStrategy Used for extracting structured data using Language Models. LLMExtractionStrategy( Required Parameters provider: str = DEFAULTPROVIDER, LLM provider (e.g., \"ollama/llama2\") apitoken: Optional[str] = None, API token Extraction Configuration instruction: str = None, Custom extraction instruction schema: Dict = None, Pydantic model schema for structured data extractiontype: str = \"block\", \"block\" or \"schema\" Chunking Parameters chunktokenthreshold: int = 4000, Maximum tokens per chunk overlaprate: float = 0.1, Overlap between chunks wordtokenrate: float = 0.75, Word to token conversion rate applychunking: bool = True, Enable/disable chunking API Configuration baseurl: str = None, Base URL for API extraargs: Dict = {}, Additional provider arguments verbose: bool = False Enable verbose logging ) CosineStrategy Used for content similarity-based extraction and clustering. CosineStrategy( Content Filtering semanticfilter: str = None, Topic/keyword filter wordcountthreshold: int = 10, Minimum words per cluster simthreshold: float = 0.3, Similarity threshold Clustering Parameters maxdist: float = 0.2, Maximum cluster distance linkagemethod: str = 'ward', Clustering method topk: int = 3, Top clusters to return Model Configuration modelname: str = 'sentence-transformers/all-MiniLM-L6-v2', Embedding model verbose: bool = False Enable verbose logging ) JsonCssExtractionStrategy Used for CSS selector-based structured data extraction. JsonCssExtractionStrategy( schema: Dict[str, Any], Extraction schema verbose: bool = False Enable verbose logging ) Schema Structure schema = { \"name\": str, Schema name \"baseSelector\": str, Base CSS selector \"fields\": [ List of fields to extract { \"name\": str, Field name \"selector\": str, CSS selector \"type\": str, Field type: \"text\", \"attribute\", \"html\", \"regex\" \"attribute\": str, For type=\"attribute\" \"pattern\": str, For type=\"regex\" \"transform\": str, Optional: \"lowercase\", \"uppercase\", \"strip\" \"default\": Any Default value if extraction fails } ] } Chunking Strategies All chunking strategies inherit from ChunkingStrategy and implement the chunk(text: str) -> list method. RegexChunking Splits text based on regex patterns. RegexChunking( patterns: List[str] = None Regex patterns for splitting Default: [r'\\n\\n'] ) SlidingWindowChunking Creates overlapping chunks with a sliding window approach. SlidingWindowChunking( windowsize: int = 100, Window size in words step: int = 50 Step size between windows ) OverlappingWindowChunking Creates chunks with specified overlap. OverlappingWindowChunking( windowsize: int = 1000, Chunk size in words overlap: int = 100 Overlap size in words ) Usage Examples LLM Extraction from pydantic import BaseModel from crawl4ai.extractionstrategy import LLMExtractionStrategy Define schema class Article(BaseModel): title: str content: str author: str Create strategy strategy = LLMExtractionStrategy( provider=\"ollama/llama2\", schema=Article.schema(), instruction=\"Extract article details\" ) Use with crawler result = await crawler.arun( url=\"https://example.com/article\", extractionstrategy=strategy ) Access extracted data data = json.loads(result.extractedcontent) CSS Extraction from crawl4ai.extractionstrategy import JsonCssExtractionStrategy Define schema schema = { \"name\": \"Product List\", \"baseSelector\": \".product-card\", \"fields\": [ { \"name\": \"title\", \"selector\": \"h2.title\", \"type\": \"text\" }, { \"name\": \"price\", \"selector\": \".price\", \"type\": \"text\", \"transform\": \"strip\" }, { \"name\": \"image\", \"selector\": \"img\", \"type\": \"attribute\", \"attribute\": \"src\" } ] } Create and use strategy strategy = JsonCssExtractionStrategy(schema) result = await crawler.arun( url=\"https://example.com/products\", extractionstrategy=strategy ) Content Chunking from crawl4ai.chunkingstrategy import OverlappingWindowChunking Create chunking strategy chunker = OverlappingWindowChunking( windowsize=500, 500 words per chunk overlap=50 50 words overlap ) Use with extraction strategy strategy = LLMExtractionStrategy( provider=\"ollama/llama2\", chunkingstrategy=chunker ) result = await crawler.arun( url=\"https://example.com/long-article\", extractionstrategy=strategy ) Best Practices 1. Choose the Right Strategy - Use LLMExtractionStrategy for complex, unstructured content - Use JsonCssExtractionStrategy for well-structured HTML - Use CosineStrategy for content similarity and clustering 2. Optimize Chunking For long documents strategy = LLMExtractionStrategy( chunktokenthreshold=2000, Smaller chunks overlaprate=0.1 10% overlap ) 3. Handle Errors try: result = await crawler.arun( url=\"https://example.com\", extractionstrategy=strategy ) if result.success: content = json.loads(result.extractedcontent) except Exception as e: print(f\"Extraction failed: {e}\") 4. Monitor Performance strategy = CosineStrategy( verbose=True, Enable logging wordcountthreshold=20, Filter short content topk=5 Limit results ) Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/#crawl4ai-open-source-llm-friendly-web-crawler-scraper\n\nTitle: Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Quick Start What Does Crawl4AI Do? Documentation Structure How You Can Support Quick Links \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper ![unclecode%2Fcrawl4ai | Trendshift ](https://trendshift.io/repositories/11716) ![GitHub Stars ](https://github.com/unclecode/crawl4ai/stargazers) ![GitHub Forks ](https://github.com/unclecode/crawl4ai/network/members) ![PyPI version ](https://badge.fury.io/py/crawl4ai) ![Python Version ](https://pypi.org/project/crawl4ai/) ![Downloads ](https://pepy.tech/project/crawl4ai) ![License ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE) Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. > Note : If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") Print the extracted content print(result.markdown) Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown : Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control : Hooks, proxies, stealth modes, session re-use\u2014fine-grained control. 4. High Performance : Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source : No forced API keys, no paywalls\u2014everyone can access their data. Core Philosophies : - Democratize Data : Free to use, transparent, and highly configurable. - LLM Friendly : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we\u2019ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you\u2019ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests : Whether it\u2019s a small fix, a big feature, or better docs\u2014contributions are always welcome. Join Discord : Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word : Mention Crawl4AI in your blog posts, talks, or on social media. Our mission : to empower everyone\u2014students, researchers, entrepreneurs, data scientists\u2014to access, parse, and shape the world\u2019s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let\u2019s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! \u2014 Unclecode, Founder & Maintainer of Crawl4AI Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/#quick-start\n\nTitle: Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Quick Start What Does Crawl4AI Do? Documentation Structure How You Can Support Quick Links \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper ![unclecode%2Fcrawl4ai | Trendshift ](https://trendshift.io/repositories/11716) ![GitHub Stars ](https://github.com/unclecode/crawl4ai/stargazers) ![GitHub Forks ](https://github.com/unclecode/crawl4ai/network/members) ![PyPI version ](https://badge.fury.io/py/crawl4ai) ![Python Version ](https://pypi.org/project/crawl4ai/) ![Downloads ](https://pepy.tech/project/crawl4ai) ![License ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE) Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. > Note : If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") Print the extracted content print(result.markdown) Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown : Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control : Hooks, proxies, stealth modes, session re-use\u2014fine-grained control. 4. High Performance : Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source : No forced API keys, no paywalls\u2014everyone can access their data. Core Philosophies : - Democratize Data : Free to use, transparent, and highly configurable. - LLM Friendly : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we\u2019ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you\u2019ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests : Whether it\u2019s a small fix, a big feature, or better docs\u2014contributions are always welcome. Join Discord : Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word : Mention Crawl4AI in your blog posts, talks, or on social media. Our mission : to empower everyone\u2014students, researchers, entrepreneurs, data scientists\u2014to access, parse, and shape the world\u2019s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let\u2019s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! \u2014 Unclecode, Founder & Maintainer of Crawl4AI Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/#what-does-crawl4ai-do\n\nTitle: Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Quick Start What Does Crawl4AI Do? Documentation Structure How You Can Support Quick Links \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper ![unclecode%2Fcrawl4ai | Trendshift ](https://trendshift.io/repositories/11716) ![GitHub Stars ](https://github.com/unclecode/crawl4ai/stargazers) ![GitHub Forks ](https://github.com/unclecode/crawl4ai/network/members) ![PyPI version ](https://badge.fury.io/py/crawl4ai) ![Python Version ](https://pypi.org/project/crawl4ai/) ![Downloads ](https://pepy.tech/project/crawl4ai) ![License ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE) Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. > Note : If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") Print the extracted content print(result.markdown) Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown : Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control : Hooks, proxies, stealth modes, session re-use\u2014fine-grained control. 4. High Performance : Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source : No forced API keys, no paywalls\u2014everyone can access their data. Core Philosophies : - Democratize Data : Free to use, transparent, and highly configurable. - LLM Friendly : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we\u2019ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you\u2019ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests : Whether it\u2019s a small fix, a big feature, or better docs\u2014contributions are always welcome. Join Discord : Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word : Mention Crawl4AI in your blog posts, talks, or on social media. Our mission : to empower everyone\u2014students, researchers, entrepreneurs, data scientists\u2014to access, parse, and shape the world\u2019s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let\u2019s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! \u2014 Unclecode, Founder & Maintainer of Crawl4AI Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/#documentation-structure\n\nTitle: Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Quick Start What Does Crawl4AI Do? Documentation Structure How You Can Support Quick Links \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper ![unclecode%2Fcrawl4ai | Trendshift ](https://trendshift.io/repositories/11716) ![GitHub Stars ](https://github.com/unclecode/crawl4ai/stargazers) ![GitHub Forks ](https://github.com/unclecode/crawl4ai/network/members) ![PyPI version ](https://badge.fury.io/py/crawl4ai) ![Python Version ](https://pypi.org/project/crawl4ai/) ![Downloads ](https://pepy.tech/project/crawl4ai) ![License ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE) Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. > Note : If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") Print the extracted content print(result.markdown) Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown : Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control : Hooks, proxies, stealth modes, session re-use\u2014fine-grained control. 4. High Performance : Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source : No forced API keys, no paywalls\u2014everyone can access their data. Core Philosophies : - Democratize Data : Free to use, transparent, and highly configurable. - LLM Friendly : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we\u2019ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you\u2019ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests : Whether it\u2019s a small fix, a big feature, or better docs\u2014contributions are always welcome. Join Discord : Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word : Mention Crawl4AI in your blog posts, talks, or on social media. Our mission : to empower everyone\u2014students, researchers, entrepreneurs, data scientists\u2014to access, parse, and shape the world\u2019s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let\u2019s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! \u2014 Unclecode, Founder & Maintainer of Crawl4AI Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/#how-you-can-support\n\nTitle: Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Quick Start What Does Crawl4AI Do? Documentation Structure How You Can Support Quick Links \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper ![unclecode%2Fcrawl4ai | Trendshift ](https://trendshift.io/repositories/11716) ![GitHub Stars ](https://github.com/unclecode/crawl4ai/stargazers) ![GitHub Forks ](https://github.com/unclecode/crawl4ai/network/members) ![PyPI version ](https://badge.fury.io/py/crawl4ai) ![Python Version ](https://pypi.org/project/crawl4ai/) ![Downloads ](https://pepy.tech/project/crawl4ai) ![License ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE) Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. > Note : If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") Print the extracted content print(result.markdown) Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown : Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control : Hooks, proxies, stealth modes, session re-use\u2014fine-grained control. 4. High Performance : Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source : No forced API keys, no paywalls\u2014everyone can access their data. Core Philosophies : - Democratize Data : Free to use, transparent, and highly configurable. - LLM Friendly : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we\u2019ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you\u2019ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests : Whether it\u2019s a small fix, a big feature, or better docs\u2014contributions are always welcome. Join Discord : Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word : Mention Crawl4AI in your blog posts, talks, or on social media. Our mission : to empower everyone\u2014students, researchers, entrepreneurs, data scientists\u2014to access, parse, and shape the world\u2019s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let\u2019s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! \u2014 Unclecode, Founder & Maintainer of Crawl4AI Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n\nURL: https://docs.crawl4ai.com/#quick-links\n\nTitle: Home - Crawl4AI Documentation (v0.4.3bx)\n\nCrawl4AI Documentation (v0.4.3bx) Home Quick Start Search Home Setup & Installation Installation Docker Deployment Quick Start Blog & Changelog Blog Home Changelog Core Simple Crawling Crawler Result Browser & Crawler Config Markdown Generation Fit Markdown Page Interaction Content Selection Cache Modes Local Files & Raw HTML Link & Media Advanced Overview File Downloading Lazy Loading Hooks & Auth Proxy & Security Session Management Multi-URL Crawling Crawl Dispatcher Identity Based Crawling SSL Certificate Extraction LLM-Free Strategies LLM Strategies Clustering Strategies Chunking API Reference AsyncWebCrawler arun() arunmany() Browser & Crawler Config CrawlResult Strategies \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper Quick Start What Does Crawl4AI Do? Documentation Structure How You Can Support Quick Links \ud83d\ude80\ud83e\udd16 Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper ![unclecode%2Fcrawl4ai | Trendshift ](https://trendshift.io/repositories/11716) ![GitHub Stars ](https://github.com/unclecode/crawl4ai/stargazers) ![GitHub Forks ](https://github.com/unclecode/crawl4ai/network/members) ![PyPI version ](https://badge.fury.io/py/crawl4ai) ![Python Version ](https://pypi.org/project/crawl4ai/) ![Downloads ](https://pepy.tech/project/crawl4ai) ![License ](https://github.com/unclecode/crawl4ai/blob/main/LICENSE) Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for large language models, AI agents, and data pipelines. Fully open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease. > Note : If you're looking for the old documentation, you can access it here. Quick Start Here's a quick example to show you how easy it is to use Crawl4AI with its asynchronous capabilities: import asyncio from crawl4ai import AsyncWebCrawler async def main(): Create an instance of AsyncWebCrawler async with AsyncWebCrawler() as crawler: Run the crawler on a URL result = await crawler.arun(url=\"https://crawl4ai.com\") Print the extracted content print(result.markdown) Run the async main function asyncio.run(main()) What Does Crawl4AI Do? Crawl4AI is a feature-rich crawler and scraper that aims to: 1. Generate Clean Markdown : Perfect for RAG pipelines or direct ingestion into LLMs. 2. Structured Extraction : Parse repeated patterns with CSS, XPath, or LLM-based extraction. 3. Advanced Browser Control : Hooks, proxies, stealth modes, session re-use\u2014fine-grained control. 4. High Performance : Parallel crawling, chunk-based extraction, real-time use cases. 5. Open Source : No forced API keys, no paywalls\u2014everyone can access their data. Core Philosophies : - Democratize Data : Free to use, transparent, and highly configurable. - LLM Friendly : Minimally processed, well-structured text, images, and metadata, so AI models can easily consume it. Documentation Structure To help you get started, we\u2019ve organized our docs into clear sections: Setup & Installation Basic instructions to install Crawl4AI via pip or Docker. Quick Start A hands-on introduction showing how to do your first crawl, generate Markdown, and do a simple extraction. Core Deeper guides on single-page crawling, advanced browser/crawler parameters, content filtering, and caching. Advanced Explore link & media handling, lazy loading, hooking & authentication, proxies, session management, and more. Extraction Detailed references for no-LLM (CSS, XPath) vs. LLM-based strategies, chunking, and clustering approaches. API Reference Find the technical specifics of each class and method, including AsyncWebCrawler, arun(), and CrawlResult. Throughout these sections, you\u2019ll find code samples you can copy-paste into your environment. If something is missing or unclear, raise an issue or PR. How You Can Support Star & Fork: If you find Crawl4AI helpful, star the repo on GitHub or fork it to add your own features. File Issues : Encounter a bug or missing feature? Let us know by filing an issue, so we can improve. Pull Requests : Whether it\u2019s a small fix, a big feature, or better docs\u2014contributions are always welcome. Join Discord : Come chat about web scraping, crawling tips, or AI workflows with the community. Spread the Word : Mention Crawl4AI in your blog posts, talks, or on social media. Our mission : to empower everyone\u2014students, researchers, entrepreneurs, data scientists\u2014to access, parse, and shape the world\u2019s data with speed, cost-efficiency, and creative freedom. Quick Links GitHub Repo Installation Guide Quick Start API Reference Changelog Thank you for joining me on this journey. Let\u2019s keep building an open, democratic approach to data extraction and AI together. Happy Crawling! \u2014 Unclecode, Founder & Maintainer of Crawl4AI Site built with MkDocs and Terminal for MkDocs. Search xClose Type to start searching\n\n---\n\n", "content_type": "text"}