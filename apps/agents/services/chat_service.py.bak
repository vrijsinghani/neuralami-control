from apps.common.utils import get_llm, format_message
from apps.agents.utils import get_tool_classes
from langchain.agents import AgentExecutor
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import Tool, StructuredTool
from apps.agents.chat.history import DjangoCacheMessageHistory
from channels.db import database_sync_to_async
from langchain_core.messages import SystemMessage, HumanMessage
import json
import logging

logger = logging.getLogger(__name__)

class ChatService:
    def __init__(self, agent, model_name, client_data, callback_handler, session_id=None):
        self.agent = agent
        self.model_name = model_name
        self.client_data = client_data
        self.callback_handler = callback_handler
        self.llm = None
        self.token_counter = None
        self.agent_executor = None
        self.processing = False
        self.tool_cache = {}  # Cache for tool results
        self.session_id = session_id or f"{agent.id}_{client_data['client_id'] if client_data else 'no_client'}"

    async def initialize(self):
        """Initialize the chat service with LLM and agent"""
        try:
            # Get LLM and token counter from utils
            self.llm, self.token_counter = get_llm(
                model_name=self.model_name,
                temperature=0.7,
                streaming=True
            )

            # Load tools
            tools = await self._load_tools()
            
            # Initialize memory
            message_history = DjangoCacheMessageHistory(
                session_id=self.session_id,
                ttl=3600
            )

            memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                chat_memory=message_history,
                output_key="output",
                input_key="input"
            )

            # Get tool names and descriptions
            tool_names = [tool.name for tool in tools]
            tool_descriptions = [f"{tool.name}: {tool.description}" for tool in tools]

            # Create base prompt template
            prompt = ChatPromptTemplate.from_messages([
                ("system", """
{system_prompt}

Available Tools:
{tools}

IMPORTANT:
1. Always analyze tool outputs before responding
2. Provide clear, structured responses
3. Handle errors gracefully
4. Keep responses focused and concise

Response Format:
1. To use a tool:
{{"action": "tool_name", "action_input": {{"param": "value"}}}}

2. To provide an answer:
{{"action": "Final Answer", "action_input": "your response"}}

Remember:
- Analyze tool outputs before responding
- Don't return raw tool data
- Structure responses clearly
- Handle errors gracefully
"""),
                ("human", "{input}"),
                ("ai", "{agent_scratchpad}"),
                ("system", "Previous conversation:\n{chat_history}")
            ])

            # Create the agent
            from langchain.agents import create_structured_chat_agent
            agent = create_structured_chat_agent(
                llm=self.llm,
                tools=tools,
                prompt=prompt.partial(
                    system_prompt=await self._create_agent_prompt(),
                    tools="\n".join(tool_descriptions),
                    tool_names=", ".join(tool_names)
                )
            )
            
            # Create agent executor
            self.agent_executor = AgentExecutor(
                agent=agent,
                tools=tools,
                memory=memory,
                verbose=True,
                max_iterations=5,
                max_execution_time=60,
                early_stopping_method="force",
                handle_parsing_errors=True,
                return_intermediate_steps=False,
                output_key="output",
                input_key="input"
            )
            
            return self.agent_executor

        except Exception as e:
            logger.error(f"Error initializing chat service: {str(e)}", exc_info=True)
            raise

    async def process_message(self, message: str) -> str:
        """Process a message using the agent"""
        if not self.agent_executor or self.processing:
            return None

        self.processing = True
        try:
            input_data = {
                "input": message,
                "chat_history": self.agent_executor.memory.chat_memory.messages if self.agent_executor.memory else []
            }
            
            logger.debug(f"Processing input: {input_data}")
            
            async for chunk in self.agent_executor.astream(input_data):
                logger.debug(f"Received chunk: {chunk}")
                
                try:
                    if isinstance(chunk, dict):
                        # Handle tool execution
                        if 'steps' in chunk:
                            for step in chunk['steps']:
                                if hasattr(step, 'action'):
                                    # Format tool name nicely
                                    tool_name = step.action.tool.replace('_', ' ').title()
                                    
                                    # Send tool execution message
                                    tool_message = {
                                        "type": "tool_execution",
                                        "content": {
                                            "title": f"Using {tool_name}",
                                            "details": {
                                                "input": step.action.tool_input,
                                                "output": step.observation if hasattr(step, 'observation') else None
                                            },
                                            "collapsed": True  # Indicate initial state
                                        }
                                    }
                                    
                                    await self.callback_handler.on_llm_new_token(
                                        json.dumps(tool_message)
                                    )

                        # Handle final output
                        elif "output" in chunk:
                            await self.callback_handler.on_llm_new_token(
                                json.dumps({
                                    "type": "final_answer",
                                    "content": chunk["output"]
                                })
                            )

                except Exception as chunk_error:
                    logger.error(f"Error processing chunk: {str(chunk_error)}")
                    await self.callback_handler.on_llm_new_token(
                        json.dumps({
                            "type": "error",
                            "message": str(chunk_error)
                        })
                    )

        finally:
            self.processing = False
            return None

    @database_sync_to_async
    def _load_tools(self):
        """Load and initialize agent tools asynchronously"""
        try:
            tools = []
            seen_tools = set()
            
            for tool_model in self.agent.tools.all():
                try:
                    tool_key = f"{tool_model.tool_class}_{tool_model.tool_subclass}"
                    if tool_key in seen_tools:
                        continue
                    seen_tools.add(tool_key)

                    tool_classes = get_tool_classes(tool_model.tool_class)
                    tool_class = next((cls for cls in tool_classes 
                                   if cls.__name__ == tool_model.tool_subclass), None)
                    
                    if tool_class:
                        logger.info(f"Initializing tool: {tool_class.__name__}")
                        tool_instance = tool_class()
                        
                        # Wrap tool with caching
                        original_run = tool_instance._run
                        async def cached_run(*args, **kwargs):
                            cache_key = f"{tool_instance.__class__.__name__}:{json.dumps(kwargs, sort_keys=True)}"
                            if cache_key in self.tool_cache:
                                logger.info(f"Cache hit for tool: {cache_key}")
                                return self.tool_cache[cache_key]
                            
                            result = await original_run(*args, **kwargs)
                            self.tool_cache[cache_key] = result
                            return result
                            
                        tool_instance._run = cached_run
                        
                        # Create structured or basic tool
                        if hasattr(tool_instance, 'args_schema'):
                            tool = StructuredTool.from_function(
                                func=tool_instance._run,
                                name=tool_model.name.lower().replace(" ", "_"),
                                description=self._create_tool_description(tool_instance, tool_model),
                                args_schema=tool_instance.args_schema,
                                coroutine=tool_instance.arun if hasattr(tool_instance, 'arun') else None,
                                return_direct=False
                            )
                        else:
                            tool = Tool(
                                name=tool_model.name.lower().replace(" ", "_"),
                                description=self._create_tool_description(tool_instance, tool_model),
                                func=tool_instance._run,
                                coroutine=tool_instance.arun if hasattr(tool_instance, 'arun') else None
                            )
                        
                        tools.append(tool)
                        logger.info(f"Successfully loaded tool: {tool_model.name}")
                        
                except Exception as e:
                    logger.error(f"Error loading tool {tool_model.name}: {str(e)}")
                    
            return tools
            
        except Exception as e:
            logger.error(f"Error loading tools: {str(e)}")
            return []

    def _create_tool_description(self, tool_instance, tool_model):
        """Create a detailed description for the tool"""
        try:
            base_description = tool_instance.description or tool_model.description
            schema = tool_instance.args_schema

            if schema:
                field_descriptions = []
                for field_name, field in schema.model_fields.items():
                    field_type = str(field.annotation).replace('typing.', '')
                    if hasattr(field.annotation, '__name__'):
                        field_type = field.annotation.__name__
                    
                    field_desc = field.description or ''
                    default = field.default
                    if default is Ellipsis:
                        default = "Required"
                    elif default is None:
                        default = "Optional"
                    
                    field_descriptions.append(
                        f"- {field_name} ({field_type}): {field_desc} Default: {default}"
                    )

                tool_description = f"""{base_description}

Parameters:
{chr(10).join(field_descriptions)}

Example:
{{"action": "{tool_model.name.lower().replace(' ', '_')}", 
  "action_input": {{
    "client_id": 123,
    "start_date": "2024-01-01",
    "end_date": "2024-01-31",
    "metrics": "newUsers",
    "dimensions": "date"
  }}
}}"""
                
                return tool_description
            
            return base_description

        except Exception as e:
            logger.error(f"Error creating tool description: {str(e)}")
            return base_description or "Tool description unavailable"

    @database_sync_to_async
    def _create_agent_prompt(self):
        """Create the system prompt for the agent"""
        # Build context section
        context_parts = []
        if self.agent.name:
            context_parts.append(f"You are {self.agent.name}")
        if self.agent.role:
            context_parts.append(f"Role: {self.agent.role}")
        if self.client_data:
            context_parts.extend([
                "Current Context:",
                f"- Client ID: {self.client_data.get('client_id', 'N/A')}",
                f"- Current Date: {self.client_data.get('current_date', 'N/A')}"
            ])
        
        context = "\n".join(context_parts)

        # Add custom system prompt if provided
        if self.agent.use_system_prompt:
            context += f"\n\n{self.agent.use_system_prompt}"

        return f"""{context}

CORE INSTRUCTIONS:
1. Response Format:
   - Tool Usage: {{"action": "tool_name", "action_input": {{...}}}}
   - Final Answer: {{"action": "Final Answer", "action_input": "response"}}

2. When Using Tools:
   - Analyze outputs before responding
   - Provide structured summaries
   - Include relevant insights
   - Handle errors gracefully

3. General Guidelines:
   - Keep responses clear and concise
   - Use appropriate formatting
   - Focus on user's needs
   - Maintain context awareness"""