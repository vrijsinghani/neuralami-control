# Generated by Django 5.1.2 on 2025-01-07 22:16

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('common', '0005_alter_llmtestharnessmodel_options'),
    ]

    operations = [
        migrations.AlterModelOptions(
            name='llmconfiguration',
            options={'ordering': ['-is_active', '-created_at'], 'verbose_name': 'LLM Configuration', 'verbose_name_plural': 'LLM Configurations'},
        ),
        migrations.RemoveIndex(
            model_name='llmconfiguration',
            name='common_llmc_provide_ef1ca0_idx',
        ),
        migrations.RemoveIndex(
            model_name='llmconfiguration',
            name='common_llmc_model_f_6efd5a_idx',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='api_base_url',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='cost_per_1k_completion_tokens',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='cost_per_1k_prompt_tokens',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='default_temperature',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='max_retries',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='max_tokens',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='model_family',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='provider_config',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='retry_delay',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='stream_buffer_size',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='stream_chunk_size',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='stream_timeout',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='supports_streaming',
        ),
        migrations.RemoveField(
            model_name='llmconfiguration',
            name='timeout',
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='api_key_secondary',
            field=models.CharField(blank=True, help_text='Secondary/backup API key', max_length=255),
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='enable_model_cache',
            field=models.BooleanField(default=True, help_text='Enable caching of available models'),
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='enable_response_cache',
            field=models.BooleanField(default=True, help_text='Enable caching of LLM responses'),
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='model_cache_ttl',
            field=models.IntegerField(default=3600, help_text='Time to live for cached model information in seconds'),
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='model_parameters',
            field=models.JSONField(default=dict, help_text='Default parameters for model (temperature, top_p, etc.)'),
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='provider_settings',
            field=models.JSONField(default=dict, help_text='Provider-specific settings and overrides'),
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='response_cache_ttl',
            field=models.IntegerField(default=3600, help_text='Time to live for cached responses in seconds'),
        ),
        migrations.AddField(
            model_name='llmconfiguration',
            name='streaming_config',
            field=models.JSONField(default=dict, help_text='Configuration for streaming responses'),
        ),
        migrations.AlterField(
            model_name='llmconfiguration',
            name='api_key',
            field=models.CharField(blank=True, help_text='Primary API key for the provider', max_length=255),
        ),
        migrations.AlterField(
            model_name='llmconfiguration',
            name='default_model',
            field=models.CharField(max_length=100),
        ),
        migrations.AlterField(
            model_name='llmconfiguration',
            name='is_active',
            field=models.BooleanField(default=False),
        ),
        migrations.AlterField(
            model_name='llmconfiguration',
            name='organization_id',
            field=models.CharField(blank=True, help_text='Organization ID for providers that require it', max_length=255),
        ),
        migrations.AlterField(
            model_name='llmconfiguration',
            name='provider_type',
            field=models.CharField(choices=[('litellm', 'LiteLLM'), ('openai', 'OpenAI'), ('anthropic', 'Anthropic'), ('gemini', 'Google Gemini'), ('openrouter', 'OpenRouter'), ('ollama', 'Ollama')], default='gemini', max_length=20),
        ),
        migrations.AlterField(
            model_name='llmconfiguration',
            name='tokens_per_minute',
            field=models.IntegerField(default=40000),
        ),
    ]
